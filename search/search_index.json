{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyEGRO: Python for Efficient Global Robust Optimization \u00b6 Overview \u00b6 PyEGRO is a Python library designed for solving complex engineering problems with efficient global robust optimization. It provides tools for initial design sampling, metamodeling, sensitivity analysis, uncertainty quantification and robust optimization. Key Modules \u00b6 1. Design of Experiments \u00b6 Advanced sampling methods: Latin Hypercube Sampling (LHS) Sobol Sequence Halton Sequence Design variables and environmental variables (deterministic and stochastic). Multi-dimensional Input. Customizable sampling criteria for enhanced precision. 2. Metamodeling \u00b6 Supports Gaussian Process Regression (GPR) or Kriging and Multi-Fidelity using Co-Kriging metamodel Using Polynomial Chaos Expansion (PCE) in uncertainty quntification and support in robust optimization loop Real-time progress visualization. 3. EGO for Metamodeling \u00b6 Provides multiple acquisition functions: Expected Improvement (EI) and Boosting the Exploration term (\ud835\udf01-EI) Probability of Improvement (PI) Lower Confidence Bound (LCB) Exploration Enhanced EI (E3I) [ 1 ] Expected Improvement for Global Fit (EIGF) [ 2 ] Distance-Enhanced Gradient (CRI3) [ 3 ] Comprehensive training configurations for surrogate models. Built-in visualization and performance tracking. 4. Robust Optimization \u00b6 Support techniques: Monte Carlo Simulation (MCS) and Polynomial Chaos Expansion (PCE) Able to use Artificial Neural Network (ANN) in robust optimization loop call Two-Stage approach. Hyperparameter optimization using Optuna . Multi-objective Pareto solutions. Both direct function and surrogate-based evaluations supported. 5. Global Sensitivity Analysis \u00b6 Sobol indices for quantifying sensitivity. Seamless analysis with true or surrogate functions. Built-in visualization tools for insightful results. 6. Uncertainty Quantification \u00b6 Monte Carlo Simulation methods for uncertainty propagation: Moment estimation. Distribution analysis (Kernel density function, KDF). Visualization of Mean and Confidence bounds on surrogate model (1D and 2D) Data logging in .csv for post-processing \ud83d\udc64 About the Author \u00b6 Thanasak Wanglomklang PhD Student in Mechanical Engineering Email : thanasak.wanglomklang@ec-lyon.fr Phone : +33 06 51 22 45 69 Location : Lyon, France Personal Website : twanglom.github.io Hi, I\u2019m Thanasak, a PhD student in Mechanical Engineering at \u00c9cole Centrale de Lyon, supervised by S\u00e9bastien Besset and Fr\u00e9d\u00e9ric Gillot . My research focuses on optimization under uncertainty, and computational vibro-acoustics, with applications in aircraft cabin shape optimization for noise reduction. I\u2019m developing PyEGRO, a tool to address these challenges in my research. In future, aiming to support uncertainty-aware engineering design. \ud83d\udc65 Community and Contributions \u00b6 We welcome contributions to PyEGRO ! Visit our GitHub repository to raise issues, submit pull requests, or explore the source code. References \u00b6 1. Berk, J., Nguyen, V., Gupta, S., Rana, S., & Venkatesh, S. (2019). Exploration enhanced expected improvement for Bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part II 18 (pp. 621-637). Springer International Publishing. 2. Lam, C. Q. (2008). Sequential adaptive designs in computer experiments for response surface model fit (Doctoral dissertation, The Ohio State University). 3. Shimoyama, K., & Kawai, S. (2019). A kriging-based dynamic adaptive sampling method for uncertainty quantification. Transactions of the Japan Society for Aeronautical and Space Sciences, 62 (3), 137-150. License \u00b6 ---","title":"Home"},{"location":"#pyegro-python-for-efficient-global-robust-optimization","text":"","title":"PyEGRO: Python for Efficient Global Robust Optimization"},{"location":"#overview","text":"PyEGRO is a Python library designed for solving complex engineering problems with efficient global robust optimization. It provides tools for initial design sampling, metamodeling, sensitivity analysis, uncertainty quantification and robust optimization.","title":"Overview"},{"location":"#key-modules","text":"","title":"Key Modules"},{"location":"#1-design-of-experiments","text":"Advanced sampling methods: Latin Hypercube Sampling (LHS) Sobol Sequence Halton Sequence Design variables and environmental variables (deterministic and stochastic). Multi-dimensional Input. Customizable sampling criteria for enhanced precision.","title":"1. Design of Experiments"},{"location":"#2-metamodeling","text":"Supports Gaussian Process Regression (GPR) or Kriging and Multi-Fidelity using Co-Kriging metamodel Using Polynomial Chaos Expansion (PCE) in uncertainty quntification and support in robust optimization loop Real-time progress visualization.","title":"2. Metamodeling"},{"location":"#3-ego-for-metamodeling","text":"Provides multiple acquisition functions: Expected Improvement (EI) and Boosting the Exploration term (\ud835\udf01-EI) Probability of Improvement (PI) Lower Confidence Bound (LCB) Exploration Enhanced EI (E3I) [ 1 ] Expected Improvement for Global Fit (EIGF) [ 2 ] Distance-Enhanced Gradient (CRI3) [ 3 ] Comprehensive training configurations for surrogate models. Built-in visualization and performance tracking.","title":"3. EGO for Metamodeling"},{"location":"#4-robust-optimization","text":"Support techniques: Monte Carlo Simulation (MCS) and Polynomial Chaos Expansion (PCE) Able to use Artificial Neural Network (ANN) in robust optimization loop call Two-Stage approach. Hyperparameter optimization using Optuna . Multi-objective Pareto solutions. Both direct function and surrogate-based evaluations supported.","title":"4. Robust Optimization"},{"location":"#5-global-sensitivity-analysis","text":"Sobol indices for quantifying sensitivity. Seamless analysis with true or surrogate functions. Built-in visualization tools for insightful results.","title":"5. Global Sensitivity Analysis"},{"location":"#6-uncertainty-quantification","text":"Monte Carlo Simulation methods for uncertainty propagation: Moment estimation. Distribution analysis (Kernel density function, KDF). Visualization of Mean and Confidence bounds on surrogate model (1D and 2D) Data logging in .csv for post-processing","title":"6. Uncertainty Quantification"},{"location":"#about-the-author","text":"Thanasak Wanglomklang PhD Student in Mechanical Engineering Email : thanasak.wanglomklang@ec-lyon.fr Phone : +33 06 51 22 45 69 Location : Lyon, France Personal Website : twanglom.github.io Hi, I\u2019m Thanasak, a PhD student in Mechanical Engineering at \u00c9cole Centrale de Lyon, supervised by S\u00e9bastien Besset and Fr\u00e9d\u00e9ric Gillot . My research focuses on optimization under uncertainty, and computational vibro-acoustics, with applications in aircraft cabin shape optimization for noise reduction. I\u2019m developing PyEGRO, a tool to address these challenges in my research. In future, aiming to support uncertainty-aware engineering design.","title":"\ud83d\udc64 About the Author"},{"location":"#community-and-contributions","text":"We welcome contributions to PyEGRO ! Visit our GitHub repository to raise issues, submit pull requests, or explore the source code.","title":"\ud83d\udc65 Community and Contributions"},{"location":"#references","text":"1. Berk, J., Nguyen, V., Gupta, S., Rana, S., & Venkatesh, S. (2019). Exploration enhanced expected improvement for Bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part II 18 (pp. 621-637). Springer International Publishing. 2. Lam, C. Q. (2008). Sequential adaptive designs in computer experiments for response surface model fit (Doctoral dissertation, The Ohio State University). 3. Shimoyama, K., & Kawai, S. (2019). A kriging-based dynamic adaptive sampling method for uncertainty quantification. Transactions of the Japan Society for Aeronautical and Space Sciences, 62 (3), 137-150.","title":"References"},{"location":"#license","text":"---","title":"License"},{"location":"api-reference/doe/doe_api_reference/","text":"PyEGRO DOE Module API Reference \u00b6 Overview \u00b6 The Design of Experiments (DOE) module in PyEGRO provides tools for generating initial design samples with support for multiple sampling methods and variable types. This reference details the classes, methods, and parameters available in the module. Classes \u00b6 InitialDesign \u00b6 The main class for creating and managing design samples. Constructor \u00b6 InitialDesign ( output_dir = 'DATA_PREPARATION' , show_progress = True , show_result = True , sampling_method = 'lhs' , sampling_criterion = 'maximin' , results_filename = 'training_data' ) Parameters \u00b6 output_dir (str, optional): Directory for saving files. Default: 'DATA_PREPARATION' show_progress (bool, optional): Whether to show progress bars during execution. Default: True show_result (bool, optional): Whether to show result summary after execution. Default: True sampling_method (str, optional): Sampling method to use ('lhs', 'sobol', 'halton', or 'random'). Default: 'lhs' sampling_criterion (str, optional): Criterion for LHS (only used when sampling_method is 'lhs'). Default: 'maximin' results_filename (str, optional): Base filename for saving results CSV. Default: 'training_data' Methods \u00b6 add_design_variable \u00b6 Add a design variable to the experiment. add_design_variable ( name , range_bounds , cov = None , std = None , delta = None , distribution = 'normal' , description = '' ) Parameters \u00b6 name (str): Name of the variable range_bounds (List[float]): List containing [min, max] bounds cov (float, optional): Coefficient of variation std (float, optional): Standard deviation delta (float, optional): -+ Uniform bounds distribution (str, optional): Distribution type ('normal' or 'lognormal'). Default: 'normal' description (str, optional): Optional description of the variable. Default: '' Returns \u00b6 None add_env_variable \u00b6 Add an environmental variable to the experiment. add_env_variable ( name , distribution , description = '' , mean = None , cov = None , std = None , low = None , high = None ) Parameters \u00b6 name (str): Name of the variable distribution (str): Distribution type (see Supported Distributions) description (str, optional): Optional description of the variable. Default: '' mean (float, optional): Mean value (for normal, lognormal, etc.). Default: None cov (float, optional): Coefficient of variation (alternative to std). Default: None std (float, optional): Standard deviation (alternative to cov). Default: None low (float, optional): Lower bound (for uniform, triangular, etc.). Default: None high (float, optional): Upper bound (for uniform, triangular, etc.). Default: None Returns \u00b6 None save \u00b6 Save design configuration to a JSON file. save ( filename = 'data_info' ) Parameters \u00b6 filename (str, optional): Base name for the JSON file (without extension). Default: 'data_info' Returns \u00b6 None load \u00b6 Load design configuration from a previously saved JSON file. load ( filename ) Parameters \u00b6 filename (str): Base name of the JSON file to load (without extension) Returns \u00b6 None run \u00b6 Run the sampling process and evaluate the objective function. run ( objective_function , num_samples , save_results = True ) Parameters \u00b6 objective_function (Callable): Function to evaluate samples num_samples (int): Number of samples to generate save_results (bool, optional): Whether to save results to file. Default: True Returns \u00b6 pandas.DataFrame containing samples and objective values print_summary \u00b6 Print summary statistics of the sampling results. print_summary ( df ) Parameters \u00b6 df (pandas.DataFrame): DataFrame containing the sampling results Returns \u00b6 None AdaptiveDistributionSampler \u00b6 A class for generating samples from different distributions. Class Attributes \u00b6 SAMPLING_METHODS (list): List of supported sampling methods: ['random', 'lhs', 'sobol', 'halton'] Methods \u00b6 generate_samples \u00b6 Generate samples for a given distribution type. generate_samples ( distribution , mean = None , cov = None , std = None , lower = None , upper = None , size = 1000 , ** kwargs ) Parameters \u00b6 distribution (str): Distribution type (see Supported Distributions) mean (float, optional): Mean value. Default: None cov (float, optional): Coefficient of variation. Default: None std (float, optional): Standard deviation (alternative to cov). Default: None lower (float, optional): Lower bound. Default: None upper (float, optional): Upper bound. Default: None size (int, optional): Number of samples to generate. Default: 1000 **kwargs (dict): Additional parameters for specific distributions Returns \u00b6 numpy.ndarray of samples generate_design_samples \u00b6 Generate samples for design variables using specified method. generate_design_samples ( design_vars , num_samples , method = 'lhs' , criterion = 'maximin' ) Parameters \u00b6 design_vars (List[Variable]): List of design variables num_samples (int): Number of samples to generate method (str, optional): Sampling method. Default: 'lhs' criterion (str, optional): Criterion for LHS. Default: 'maximin' Returns \u00b6 numpy.ndarray of samples scaled to variable bounds generate_env_samples \u00b6 Generate samples for an environmental variable. generate_env_samples ( var , num_samples ) Parameters \u00b6 var (Variable or dict): Environmental variable to sample num_samples (int): Number of samples to generate Returns \u00b6 numpy.ndarray of samples generate_all_samples \u00b6 Generate samples for all variables. generate_all_samples ( variables , num_samples , method = 'lhs' , criterion = None ) Parameters \u00b6 variables (List[Variable]): List of Variable objects num_samples (int): Number of samples to generate method (str, optional): Sampling method. Default: 'lhs' criterion (str, optional): Criterion for LHS. Default: None Returns \u00b6 numpy.ndarray of samples calculate_input_bounds \u00b6 Calculate input bounds for all variables. calculate_input_bounds ( variables ) Parameters \u00b6 variables (List[Variable]): List of variables Returns \u00b6 List[List[float]] containing [min, max] bounds for each variable Variable \u00b6 A dataclass representing a variable in the design. Attributes \u00b6 name (str): Name of the variable vars_type (str): Variable type ('design_vars' or 'env_vars') distribution (str): Distribution type (see Supported Distributions) description (str): Optional description range_bounds (List[float], optional): List containing [min, max] bounds (for design variables) low (float, optional): Lower bound (for various distributions) high (float, optional): Upper bound (for various distributions) mean (float, optional): Mean value (for various distributions) cov (float, optional): Coefficient of variation std (float, optional): Standard deviation (alternative to cov) Supported Distributions \u00b6 The following distributions are supported for environment variables: Uniform : Constant probability within range Required parameters: low , high Normal : Gaussian distribution Required parameters: mean plus either cov or std Optional parameters: low , high (for truncation) Lognormal : Natural logarithm follows normal distribution Required parameters: mean plus either cov or std Optional parameters: low , high (for truncation) Module Functions \u00b6 get_version \u00b6 Return the current version of the module. get_version () Returns \u00b6 str - Version string get_citation \u00b6 Return citation information for the module. get_citation () Returns \u00b6 str - Citation text print_usage \u00b6 Print detailed usage instructions for the module. print_usage () Returns \u00b6 None Examples \u00b6 Basic Example \u00b6 import numpy as np from PyEGRO.doe import InitialDesign # Define objective function def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] return - (( X1 - 2 ) ** 2 + ( X2 - 3 ) ** 2 ) # Create initial design design = InitialDesign ( output_dir = 'my_experiment' , sampling_method = 'lhs' , sampling_criterion = 'maximin' ) # Add variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) # Run sampling results = design . run ( objective_function = objective_func_2D , num_samples = 50 ) With Multiple Distribution Types \u00b6 # Create design design = InitialDesign ( sampling_method = 'sobol' ) # Add design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) # Add environmental variables with different distributions design . add_env_variable ( name = 'normal_var' , distribution = 'normal' , mean = 5 , std = 2.0 # Using std instead of cov ) design . add_env_variable ( name = 'uniform_var' , distribution = 'uniform' , low = 0.0 , high = 1.0 ) design . add_env_variable ( name = 'lognormal_var' , distribution = 'lognormal' , mean = 10 , std = 2.0 ) # Define objective function that uses all variables def objective_func ( x ): design_var = x [:, 0 ] normal_var = x [:, 1 ] uniform_var = x [:, 2 ] lognormal_var = x [:, 3 ] return design_var ** 2 + 10 * normal_var + 5 * uniform_var + lognormal_var # Run sampling results = design . run ( objective_function = objective_func , num_samples = 100 ) Output File Formats \u00b6 data_info.json \u00b6 Contains configuration information including: - Variable definitions (name, type, distribution, bounds, etc.) - Sampling configuration (method, criterion) - Metadata (creation time, variable counts) training_data.csv \u00b6 Contains the sample data: - Columns for each variable (design and environmental) - Final column 'y' with objective function values","title":"Design of Experiment"},{"location":"api-reference/doe/doe_api_reference/#pyegro-doe-module-api-reference","text":"","title":"PyEGRO DOE Module API Reference"},{"location":"api-reference/doe/doe_api_reference/#overview","text":"The Design of Experiments (DOE) module in PyEGRO provides tools for generating initial design samples with support for multiple sampling methods and variable types. This reference details the classes, methods, and parameters available in the module.","title":"Overview"},{"location":"api-reference/doe/doe_api_reference/#classes","text":"","title":"Classes"},{"location":"api-reference/doe/doe_api_reference/#initialdesign","text":"The main class for creating and managing design samples.","title":"InitialDesign"},{"location":"api-reference/doe/doe_api_reference/#adaptivedistributionsampler","text":"A class for generating samples from different distributions.","title":"AdaptiveDistributionSampler"},{"location":"api-reference/doe/doe_api_reference/#variable","text":"A dataclass representing a variable in the design.","title":"Variable"},{"location":"api-reference/doe/doe_api_reference/#supported-distributions","text":"The following distributions are supported for environment variables: Uniform : Constant probability within range Required parameters: low , high Normal : Gaussian distribution Required parameters: mean plus either cov or std Optional parameters: low , high (for truncation) Lognormal : Natural logarithm follows normal distribution Required parameters: mean plus either cov or std Optional parameters: low , high (for truncation)","title":"Supported Distributions"},{"location":"api-reference/doe/doe_api_reference/#module-functions","text":"","title":"Module Functions"},{"location":"api-reference/doe/doe_api_reference/#get_version","text":"Return the current version of the module. get_version ()","title":"get_version"},{"location":"api-reference/doe/doe_api_reference/#get_citation","text":"Return citation information for the module. get_citation ()","title":"get_citation"},{"location":"api-reference/doe/doe_api_reference/#print_usage","text":"Print detailed usage instructions for the module. print_usage ()","title":"print_usage"},{"location":"api-reference/doe/doe_api_reference/#examples","text":"","title":"Examples"},{"location":"api-reference/doe/doe_api_reference/#basic-example","text":"import numpy as np from PyEGRO.doe import InitialDesign # Define objective function def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] return - (( X1 - 2 ) ** 2 + ( X2 - 3 ) ** 2 ) # Create initial design design = InitialDesign ( output_dir = 'my_experiment' , sampling_method = 'lhs' , sampling_criterion = 'maximin' ) # Add variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) # Run sampling results = design . run ( objective_function = objective_func_2D , num_samples = 50 )","title":"Basic Example"},{"location":"api-reference/doe/doe_api_reference/#with-multiple-distribution-types","text":"# Create design design = InitialDesign ( sampling_method = 'sobol' ) # Add design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) # Add environmental variables with different distributions design . add_env_variable ( name = 'normal_var' , distribution = 'normal' , mean = 5 , std = 2.0 # Using std instead of cov ) design . add_env_variable ( name = 'uniform_var' , distribution = 'uniform' , low = 0.0 , high = 1.0 ) design . add_env_variable ( name = 'lognormal_var' , distribution = 'lognormal' , mean = 10 , std = 2.0 ) # Define objective function that uses all variables def objective_func ( x ): design_var = x [:, 0 ] normal_var = x [:, 1 ] uniform_var = x [:, 2 ] lognormal_var = x [:, 3 ] return design_var ** 2 + 10 * normal_var + 5 * uniform_var + lognormal_var # Run sampling results = design . run ( objective_function = objective_func , num_samples = 100 )","title":"With Multiple Distribution Types"},{"location":"api-reference/doe/doe_api_reference/#output-file-formats","text":"","title":"Output File Formats"},{"location":"api-reference/doe/doe_api_reference/#data_infojson","text":"Contains configuration information including: - Variable definitions (name, type, distribution, bounds, etc.) - Sampling configuration (method, criterion) - Metadata (creation time, variable counts)","title":"data_info.json"},{"location":"api-reference/doe/doe_api_reference/#training_datacsv","text":"Contains the sample data: - Columns for each variable (design and environmental) - Final column 'y' with objective function values","title":"training_data.csv"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/","text":"PyEGRO Co-Kriging Module API Reference \u00b6 This document provides detailed API reference for the Co-Kriging (multi-fidelity) module in the PyEGRO package. Table of Contents \u00b6 MetaTrainingCoKriging Class CoKrigingModel Class CoKrigingKernel Class DeviceAgnosticCoKriging Class Visualization Functions MetaTrainingCoKriging Class \u00b6 The MetaTrainingCoKriging class provides a high-level interface for training and managing Co-Kriging models that combine low and high-fidelity data sources. Constructor \u00b6 MetaTrainingCoKriging ( num_iterations = 1000 , prefer_gpu = True , show_progress = True , show_hardware_info = True , show_model_info = True , output_dir = 'RESULT_MODEL_COKRIGING' , kernel = 'matern25' , learning_rate = 0.01 , patience = 50 ) Parameters \u00b6 num_iterations (int, optional): Number of training iterations. Default: 1000 prefer_gpu (bool, optional): Whether to use GPU if available. Default: True show_progress (bool, optional): Whether to show detailed progress. Default: True show_hardware_info (bool, optional): Whether to show system hardware info. Default: True show_model_info (bool, optional): Whether to show model architecture info. Default: True output_dir (str, optional): Directory for saving results. Default: 'RESULT_MODEL_COKRIGING' kernel (str, optional): Kernel to use for base GP model. Options: 'matern25' , 'matern15' , 'matern05' , 'rbf' . Default: 'matern25' learning_rate (float, optional): Learning rate for optimizer. Default: 0.01 patience (int, optional): Number of iterations to wait for improvement before early stopping. Default: 50 Methods \u00b6 train \u00b6 Train the Co-Kriging model with low and high-fidelity data. train ( X_low , y_low , X_high , y_high , X_test = None , y_test = None , feature_names = None ) Parameters \u00b6 X_low (numpy.ndarray or pandas.DataFrame): Low-fidelity input features y_low (numpy.ndarray, pandas.DataFrame, or pandas.Series): Low-fidelity target values X_high (numpy.ndarray or pandas.DataFrame): High-fidelity input features y_high (numpy.ndarray, pandas.DataFrame, or pandas.Series): High-fidelity target values X_test (numpy.ndarray or pandas.DataFrame, optional): Test features. Default: None y_test (numpy.ndarray, pandas.DataFrame, or pandas.Series, optional): Test targets. Default: None feature_names (list of str, optional): List of feature names. Default: None Returns \u00b6 model (CoKrigingModel): Trained model scaler_X (StandardScaler): Feature scaler scaler_y (StandardScaler): Target scaler predict \u00b6 Make predictions with the trained Co-Kriging model. predict ( X , fidelity = 'high' ) Parameters \u00b6 X (numpy.ndarray or pandas.DataFrame): Input features for prediction fidelity (str, optional): 'high' or 'low' to specify which fidelity level to predict. Default: 'high' Returns \u00b6 mean (numpy.ndarray): Mean predictions std (numpy.ndarray): Standard deviations of predictions load_model \u00b6 Load a trained model from disk. load_model ( model_path = None ) Parameters \u00b6 model_path (str, optional): Path to the saved model file. If None, uses the default path. Default: None Returns \u00b6 model (CoKrigingModel): Loaded model print_hyperparameters \u00b6 Print the learned hyperparameters of the model. print_hyperparameters () CoKrigingModel Class \u00b6 The CoKrigingModel class implements a Gaussian Process model for Co-Kriging/multi-fidelity modeling using the Kennedy & O'Hagan (2000) approach. Constructor \u00b6 CoKrigingModel ( train_x , train_y , likelihood , kernel = 'matern15' ) Parameters \u00b6 train_x (torch.Tensor): Training input data (with fidelity indicator as last column) train_y (torch.Tensor): Training target data likelihood (gpytorch.likelihoods.Likelihood): GP likelihood function kernel (str, optional): Kernel type. Options: 'matern25' , 'matern15' , 'matern05' , 'rbf' . Default: 'matern15' Methods \u00b6 forward \u00b6 Computes the mean and covariance of the GP posterior. forward ( x ) Parameters \u00b6 x (torch.Tensor): Input data (with fidelity indicator as last column) Returns \u00b6 gpytorch.distributions.MultivariateNormal : Distribution with predicted mean and covariance CoKrigingKernel Class \u00b6 The CoKrigingKernel class implements a custom kernel for Co-Kriging that handles multi-fidelity data following the auto-regressive structure of the Kennedy & O'Hagan model. Constructor \u00b6 CoKrigingKernel ( base_kernel , num_dims , active_dims = None ) Parameters \u00b6 base_kernel (gpytorch.kernels.Kernel): Base kernel to use for both low-fidelity and difference components num_dims (int): Number of dimensions including the fidelity indicator active_dims (tuple, optional): Dimensions that the kernel operates on. Default: None Properties \u00b6 rho \u00b6 Returns the scaling parameter \u03c1 that controls the correlation between fidelity levels. rho Methods \u00b6 forward \u00b6 Compute the covariance matrix between inputs x1 and x2 based on the Kennedy & O'Hagan multi-fidelity formulation. forward ( x1 , x2 , diag = False , ** params ) Parameters \u00b6 x1 (torch.Tensor): First input x2 (torch.Tensor): Second input diag (bool, optional): Whether to return just the diagonal of the covariance matrix. Default: False **params : Additional parameters for the base kernels Returns \u00b6 torch.Tensor : Covariance matrix DeviceAgnosticCoKriging Class \u00b6 The DeviceAgnosticCoKriging class provides a device-agnostic handler for Co-Kriging models. Constructor \u00b6 DeviceAgnosticCoKriging ( prefer_gpu = False ) Parameters \u00b6 prefer_gpu (bool, optional): Whether to use GPU if available. Default: False Methods \u00b6 load_model \u00b6 Load model using state dict approach. load_model ( model_dir = 'RESULT_MODEL_COKRIGING' ) Parameters \u00b6 model_dir (str, optional): Directory containing the model files. Default: 'RESULT_MODEL_COKRIGING' Returns \u00b6 bool : True if model was loaded successfully, False otherwise predict \u00b6 Make predictions with the loaded Co-Kriging model. predict ( X , fidelity = 'high' , batch_size = 1000 ) Parameters \u00b6 X (numpy.ndarray): Input features (n_samples, n_features) fidelity (str, optional): 'high' or 'low' to specify which fidelity level to predict. Default: 'high' batch_size (int, optional): Batch size for processing large datasets. Default: 1000 Returns \u00b6 Tuple of (mean_predictions, std_predictions) as numpy arrays Visualization Functions \u00b6 visualize_cokriging \u00b6 Create comprehensive visualizations for Co-Kriging model performance. visualize_cokriging ( meta , X_low , y_low , X_high , y_high , X_test = None , y_test = None , variable_names = None , bounds = None , savefig = False , output_dir = None ) Parameters \u00b6 meta (MetaTrainingCoKriging): Trained MetaTrainingCoKriging instance X_low (numpy.ndarray or pandas.DataFrame): Low-fidelity inputs y_low (numpy.ndarray, pandas.DataFrame, or pandas.Series): Low-fidelity targets X_high (numpy.ndarray or pandas.DataFrame): High-fidelity inputs y_high (numpy.ndarray, pandas.DataFrame, or pandas.Series): High-fidelity targets X_test (numpy.ndarray or pandas.DataFrame, optional): Test inputs. Default: None y_test (numpy.ndarray, pandas.DataFrame, or pandas.Series, optional): Test targets. Default: None variable_names (list of str, optional): Names of input variables. Default: None bounds (numpy.ndarray, optional): Bounds of input variables for sampling. Default: None savefig (bool, optional): Whether to save figures to disk. Default: False output_dir (str, optional): Directory to save figures. Default: None , uses meta.output_dir Returns \u00b6 figures (dict): Dictionary of figure handles","title":"Cokriging"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#pyegro-co-kriging-module-api-reference","text":"This document provides detailed API reference for the Co-Kriging (multi-fidelity) module in the PyEGRO package.","title":"PyEGRO Co-Kriging Module API Reference"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#table-of-contents","text":"MetaTrainingCoKriging Class CoKrigingModel Class CoKrigingKernel Class DeviceAgnosticCoKriging Class Visualization Functions","title":"Table of Contents"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#metatrainingcokriging-class","text":"The MetaTrainingCoKriging class provides a high-level interface for training and managing Co-Kriging models that combine low and high-fidelity data sources.","title":"MetaTrainingCoKriging Class"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#constructor","text":"MetaTrainingCoKriging ( num_iterations = 1000 , prefer_gpu = True , show_progress = True , show_hardware_info = True , show_model_info = True , output_dir = 'RESULT_MODEL_COKRIGING' , kernel = 'matern25' , learning_rate = 0.01 , patience = 50 )","title":"Constructor"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#methods","text":"","title":"Methods"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#cokrigingmodel-class","text":"The CoKrigingModel class implements a Gaussian Process model for Co-Kriging/multi-fidelity modeling using the Kennedy & O'Hagan (2000) approach.","title":"CoKrigingModel Class"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#constructor_1","text":"CoKrigingModel ( train_x , train_y , likelihood , kernel = 'matern15' )","title":"Constructor"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#methods_1","text":"","title":"Methods"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#cokrigingkernel-class","text":"The CoKrigingKernel class implements a custom kernel for Co-Kriging that handles multi-fidelity data following the auto-regressive structure of the Kennedy & O'Hagan model.","title":"CoKrigingKernel Class"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#constructor_2","text":"CoKrigingKernel ( base_kernel , num_dims , active_dims = None )","title":"Constructor"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#properties","text":"","title":"Properties"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#methods_2","text":"","title":"Methods"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#deviceagnosticcokriging-class","text":"The DeviceAgnosticCoKriging class provides a device-agnostic handler for Co-Kriging models.","title":"DeviceAgnosticCoKriging Class"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#constructor_3","text":"DeviceAgnosticCoKriging ( prefer_gpu = False )","title":"Constructor"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#methods_3","text":"","title":"Methods"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#visualization-functions","text":"","title":"Visualization Functions"},{"location":"api-reference/meta/cokriging/cokriging_api_reference/#visualize_cokriging","text":"Create comprehensive visualizations for Co-Kriging model performance. visualize_cokriging ( meta , X_low , y_low , X_high , y_high , X_test = None , y_test = None , variable_names = None , bounds = None , savefig = False , output_dir = None )","title":"visualize_cokriging"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/","text":"PyEGRO.meta.egocokriging API Reference \u00b6 This document provides detailed API documentation for the PyEGRO.meta.egocokriging module, a framework for multi-fidelity surrogate modeling and optimization using Co-Kriging. Table of Contents \u00b6 Main Optimizer Multi-Fidelity Model Configuration Acquisition Functions Utilities Visualization Main Optimizer \u00b6 EfficientGlobalOptimization \u00b6 Main class for Efficient Global Optimization with multi-fidelity support. from PyEGRO.meta.egocokriging import EfficientGlobalOptimization Constructor \u00b6 EfficientGlobalOptimization ( objective_func , bounds : np . ndarray , variable_names : list , config : TrainingConfig = None , initial_data : pd . DataFrame = None , low_fidelity_func = None ) Parameters: - objective_func (callable): High-fidelity objective function to be optimized - bounds (np.ndarray): Bounds for each variable, shape (n_dimensions, 2) - variable_names (list): Names of input variables - config (TrainingConfig, optional): Configuration for the optimization process - initial_data (pd.DataFrame, optional): Initial data points with optional 'fidelity' column - low_fidelity_func (callable, optional): Low-fidelity objective function (faster but less accurate) Methods \u00b6 run() \u00b6 Run the optimization process with multi-fidelity support. history = optimizer . run () Returns: - dict : History of the optimization process containing iterations, performance metrics, rho values, and chosen points Multi-Fidelity Model \u00b6 CoKrigingKernel \u00b6 Co-Kriging kernel for multi-fidelity Gaussian process modeling. from PyEGRO.meta.egocokriging import CoKrigingKernel Constructor \u00b6 CoKrigingKernel ( base_kernel , num_dims : int , active_dims : Optional [ Tuple [ int , ... ]] = None ) Parameters: - base_kernel (gpytorch.kernels.Kernel): Base kernel to use for both kernel_c and kernel_d - num_dims (int): Number of input dimensions - active_dims (Tuple[int, ...], optional): Dimensions to apply kernel to Properties \u00b6 rho \u00b6 The scaling parameter that controls correlation between fidelity levels. Returns: - torch.Tensor : Transformed rho parameter value Methods \u00b6 forward(x1: torch.Tensor, x2: torch.Tensor, diag: bool = False, **params) \u00b6 Forward pass to compute kernel matrix. Parameters: - x1 (torch.Tensor): First input tensor with fidelity indicator in last column - x2 (torch.Tensor): Second input tensor with fidelity indicator in last column - diag (bool, optional): Whether to return only diagonal elements Returns: - torch.Tensor : Kernel matrix CoKrigingModel \u00b6 Co-Kriging Gaussian Process Model for multi-fidelity optimization. from PyEGRO.meta.egocokriging import CoKrigingModel Constructor \u00b6 CoKrigingModel ( train_x : torch . Tensor , train_y : torch . Tensor , likelihood : gpytorch . likelihoods . Likelihood , kernel : str = 'matern15' ) Parameters: - train_x (torch.Tensor): Training input data with fidelity indicator in last column - train_y (torch.Tensor): Training target data - likelihood (gpytorch.likelihoods.Likelihood): GPyTorch likelihood - kernel (str, optional): Kernel type ('matern25', 'matern15', 'matern05', 'rbf') Methods \u00b6 forward(x: torch.Tensor) \u00b6 Forward pass of Co-Kriging model. Parameters: - x (torch.Tensor): Input data with fidelity indicator in last column Returns: - gpytorch.distributions.MultivariateNormal : Distribution representing GP predictions get_hyperparameters() \u00b6 Get current hyperparameter values including both kernel components and rho. Returns: - dict : Dictionary containing the hyperparameters predict(x: torch.Tensor) \u00b6 Make predictions with the Co-Kriging model. Parameters: - x (torch.Tensor): Input points with fidelity indicator in last column Returns: - tuple : (mean predictions, standard deviations) Configuration \u00b6 TrainingConfig \u00b6 Configuration class for EGO training parameters with multi-fidelity support. from PyEGRO.meta.egocokriging import TrainingConfig Constructor \u00b6 TrainingConfig ( max_iterations : int = 100 , rmse_threshold : float = 0.001 , rmse_patience : int = 10 , relative_improvement : float = 0.01 , acquisition_name : str = \"ei\" , acquisition_params : Dict [ str , Any ] = field ( default_factory = dict ), training_iter : int = 100 , verbose : bool = False , show_summary : bool = True , device : str = \"cuda\" if torch . cuda . is_available () else \"cpu\" , save_dir : str = \"RESULT_MODEL_COKRIGING\" , learning_rate : float = 0.01 , early_stopping_patience : int = 20 , jitter : float = 1e-3 , kernel : str = \"matern15\" , multi_fidelity : bool = True , rho_threshold : float = 0.05 , rho_patience : int = 3 ) Parameters: - max_iterations (int): Maximum number of optimization iterations - rmse_threshold (float): RMSE threshold for early stopping - rmse_patience (int): Number of iterations without improvement before stopping - relative_improvement (float): Minimum relative improvement required - acquisition_name (str): Name of acquisition function - acquisition_params (Dict[str, Any]): Parameters for the acquisition function - training_iter (int): Number of training iterations for GP model - verbose (bool): Whether to print detailed training information - show_summary (bool): Whether to show parameter summary before training - device (str): Device to use for training ('cpu' or 'cuda') - save_dir (str): Directory to save results and models - learning_rate (float): Learning rate for model optimization - early_stopping_patience (int): Patience for early stopping during model training - jitter (float): Jitter value for numerical stability - kernel (str): Kernel type for base kernel - multi_fidelity (bool): Whether to use multi-fidelity modeling - rho_threshold (float): Convergence threshold for rho parameter (as percentage) - rho_patience (int): Number of iterations without improvement in rho before stopping Methods \u00b6 to_dict() \u00b6 Convert configuration to dictionary. Returns: - dict : Dictionary containing all configuration parameters Acquisition Functions \u00b6 Base Class \u00b6 AcquisitionFunction \u00b6 Base class for all acquisition functions with multi-fidelity support. from PyEGRO.meta.egocokriging.acquisition import AcquisitionFunction Methods \u00b6 evaluate(X) \u00b6 Evaluate the acquisition function at points X. Parameters: - X (np.ndarray): Points to evaluate Returns: - np.ndarray : Acquisition function values (negated for minimization) optimize() \u00b6 Optimize the acquisition function to find the next sampling point. Returns: - np.ndarray : Next point to evaluate _prepare_input_with_fidelity(X, fidelity=1) \u00b6 Add fidelity indicator to input if the model is multi-fidelity. Parameters: - X (torch.Tensor): Input data without fidelity indicator - fidelity (int): Fidelity level (0 for low, 1 for high) Returns: - torch.Tensor : Input data suitable for the model Specific Acquisition Functions \u00b6 ExpectedImprovement \u00b6 Expected Improvement acquisition function with multi-fidelity support. from PyEGRO.meta.egocokriging import ExpectedImprovement LowerConfidenceBound \u00b6 Lower Confidence Bound acquisition function with multi-fidelity support. from PyEGRO.meta.egocokriging import LowerConfidenceBound PredictiveVariance \u00b6 Predictive Variance acquisition function with multi-fidelity support. from PyEGRO.meta.egocokriging import PredictiveVariance ProbabilityImprovement \u00b6 Probability of Improvement acquisition function with multi-fidelity support. from PyEGRO.meta.egocokriging import ProbabilityImprovement ExpectedImprovementGlobalFit \u00b6 Expected Improvement for Global Fit acquisition function with multi-fidelity support. from PyEGRO.meta.egocokriging import ExpectedImprovementGlobalFit Criterion3 \u00b6 Implementation of Criterion 3 acquisition function with multi-fidelity support. from PyEGRO.meta.egocokriging import Criterion3 ExplorationEnhancedEI \u00b6 Exploration Enhanced Expected Improvement (E\u00b3I) acquisition function with multi-fidelity support. from PyEGRO.meta.egocokriging import ExplorationEnhancedEI Factory Functions \u00b6 create_acquisition_function \u00b6 Create an acquisition function by name with multi-fidelity support. from PyEGRO.meta.egocokriging import create_acquisition_function acquisition = create_acquisition_function ( name : str , model , likelihood , bounds , scaler_x , scaler_y , y_train = None , ** kwargs ) propose_location \u00b6 Find the next location to sample using a specified acquisition function. from PyEGRO.meta.egocokriging import propose_location X_next = propose_location ( acquisition_name : str , model , likelihood , y_train , bounds , scaler_x , scaler_y , ** kwargs ) Utilities \u00b6 Training Utilities \u00b6 train_ck_model \u00b6 Train a Co-Kriging model with optimization settings. from PyEGRO.meta.egocokriging import train_ck_model rho_value = train_ck_model ( model , likelihood , X_train : torch . Tensor , y_train : torch . Tensor , config : TrainingConfig ) Returns: - float : Current rho value after training check_rho_convergence \u00b6 Check if rho parameter has converged. from PyEGRO.meta.egocokriging import check_rho_convergence has_converged = check_rho_convergence ( rho_history : List [ float ], config : TrainingConfig ) Returns: - bool : True if rho has converged, False otherwise Model Storage Utilities \u00b6 save_model_data \u00b6 Save model, scalers, hyperparameters and metadata with multi-fidelity support. from PyEGRO.meta.egocokriging import save_model_data save_model_data ( model , likelihood , scalers : Dict , metadata : Dict , save_dir : str ) load_model_data \u00b6 Load saved model data. from PyEGRO.meta.egocokriging import load_model_data data = load_model_data ( save_dir : str ) Evaluation Utilities \u00b6 evaluate_model_performance \u00b6 Calculate model performance metrics using LOOCV. from PyEGRO.meta.egocokriging.utils import evaluate_model_performance metrics = evaluate_model_performance ( model , likelihood , X_train : torch . Tensor , y_train : torch . Tensor ) Data Preparation Utilities \u00b6 prepare_data_with_fidelity \u00b6 Prepare input data with fidelity indicator. from PyEGRO.meta.egocokriging import prepare_data_with_fidelity X_with_fidelity = prepare_data_with_fidelity ( X : np . ndarray , fidelity : int ) Parameters: - X (np.ndarray): Input data without fidelity indicator - fidelity (int): Fidelity level (0 for low, 1 for high) Returns: - np.ndarray : Input data with fidelity indicator in the last column Directory Utilities \u00b6 setup_directories \u00b6 Create necessary directories for saving results. from PyEGRO.meta.egocokriging import setup_directories setup_directories ( save_dir : str , create_plots : bool = True ) Visualization \u00b6 EGOAnimator \u00b6 Class for creating animations of the optimization process with multi-fidelity support. from PyEGRO.meta.egocokriging import EGOAnimator Constructor \u00b6 EGOAnimator ( save_dir : str = 'RESULT_MODEL_COKRIGING/animation' , frame_duration : int = 500 ) Methods \u00b6 save_1D_frame \u00b6 Save a 1D visualization frame with multi-fidelity support. animator . save_1D_frame ( model : gpytorch . models . ExactGP , likelihood : gpytorch . likelihoods . GaussianLikelihood , X_train : np . ndarray , y_train : np . ndarray , scaler_x : object , scaler_y : object , bounds : np . ndarray , iteration : int , variable_names : List [ str ], device : torch . device = None , true_function : Optional [ callable ] = None , batch_size : int = 1000 , fidelities : Optional [ np . ndarray ] = None ) save_2D_frame \u00b6 Save a 2D visualization frame with multi-fidelity support. animator . save_2D_frame ( model : gpytorch . models . ExactGP , likelihood : gpytorch . likelihoods . GaussianLikelihood , X_train : np . ndarray , y_train : np . ndarray , scaler_x : object , scaler_y : object , bounds : np . ndarray , iteration : int , variable_names : List [ str ], n_initial_samples : int , n_points : int = 50 , batch_size : int = 1000 , device : torch . device = None , fidelities : Optional [ np . ndarray ] = None ) create_gif \u00b6 Create GIF animation from saved frames. animator . create_gif ( duration : Optional [ int ] = None ) ModelVisualizer \u00b6 Class for creating final model visualizations with multi-fidelity support. from PyEGRO.meta.egocokriging import ModelVisualizer Constructor \u00b6 ModelVisualizer ( model , likelihood , scaler_x , scaler_y , bounds , variable_names , history , device , save_dir = 'RESULT_MODEL_COKRIGING' , multi_fidelity = False , rho_history = None ) Methods \u00b6 plot_rho_evolution \u00b6 Plot the evolution of rho parameter over iterations. visualizer . plot_rho_evolution () plot_final_prediction \u00b6 Create final prediction plots with multi-fidelity support. visualizer . plot_final_prediction ( X_train , y_train , true_function = None , fidelities = None ) plot_convergence_metrics \u00b6 Plot optimization metrics over iterations including rho values. visualizer . plot_convergence_metrics ( history : dict ) plot_error_analysis \u00b6 Plot prediction errors and residuals with separate plots for high and low fidelity data. visualizer . plot_error_analysis ( X_train , y_train , fidelities = None )","title":"EGO-Cokriging"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#pyegrometaegocokriging-api-reference","text":"This document provides detailed API documentation for the PyEGRO.meta.egocokriging module, a framework for multi-fidelity surrogate modeling and optimization using Co-Kriging.","title":"PyEGRO.meta.egocokriging API Reference"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#table-of-contents","text":"Main Optimizer Multi-Fidelity Model Configuration Acquisition Functions Utilities Visualization","title":"Table of Contents"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#main-optimizer","text":"","title":"Main Optimizer"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#efficientglobaloptimization","text":"Main class for Efficient Global Optimization with multi-fidelity support. from PyEGRO.meta.egocokriging import EfficientGlobalOptimization","title":"EfficientGlobalOptimization"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#multi-fidelity-model","text":"","title":"Multi-Fidelity Model"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#cokrigingkernel","text":"Co-Kriging kernel for multi-fidelity Gaussian process modeling. from PyEGRO.meta.egocokriging import CoKrigingKernel","title":"CoKrigingKernel"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#cokrigingmodel","text":"Co-Kriging Gaussian Process Model for multi-fidelity optimization. from PyEGRO.meta.egocokriging import CoKrigingModel","title":"CoKrigingModel"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#configuration","text":"","title":"Configuration"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#trainingconfig","text":"Configuration class for EGO training parameters with multi-fidelity support. from PyEGRO.meta.egocokriging import TrainingConfig","title":"TrainingConfig"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#acquisition-functions","text":"","title":"Acquisition Functions"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#base-class","text":"","title":"Base Class"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#specific-acquisition-functions","text":"","title":"Specific Acquisition Functions"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#factory-functions","text":"","title":"Factory Functions"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#utilities","text":"","title":"Utilities"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#training-utilities","text":"","title":"Training Utilities"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#model-storage-utilities","text":"","title":"Model Storage Utilities"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#evaluation-utilities","text":"","title":"Evaluation Utilities"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#data-preparation-utilities","text":"","title":"Data Preparation Utilities"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#directory-utilities","text":"","title":"Directory Utilities"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#visualization","text":"","title":"Visualization"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#egoanimator","text":"Class for creating animations of the optimization process with multi-fidelity support. from PyEGRO.meta.egocokriging import EGOAnimator","title":"EGOAnimator"},{"location":"api-reference/meta/ego-cokriging/egocokriging_api_reference/#modelvisualizer","text":"Class for creating final model visualizations with multi-fidelity support. from PyEGRO.meta.egocokriging import ModelVisualizer","title":"ModelVisualizer"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/","text":"PyEGRO.meta.egogpr API Reference \u00b6 This document provides detailed API documentation for the PyEGRO.meta.egogpr module, a framework for building highly accurate surrogate models through intelligent adaptive sampling strategies. Table of Contents \u00b6 Main Optimizer Model Configuration Acquisition Functions Utilities Visualization Main Optimizer \u00b6 EfficientGlobalOptimization \u00b6 Main class for performing Efficient Global Optimization. from PyEGRO.meta.egogpr import EfficientGlobalOptimization Constructor \u00b6 EfficientGlobalOptimization ( objective_func , bounds : np . ndarray , variable_names : list , config : TrainingConfig = None , initial_data : pd . DataFrame = None ) Parameters: - objective_func (callable): Function to be optimized - bounds (np.ndarray): Bounds for each variable, shape (n_dimensions, 2) - variable_names (list): Names of input variables - config (TrainingConfig, optional): Configuration for the optimization process - initial_data (pd.DataFrame, optional): Initial data points (if already available) Methods \u00b6 run() \u00b6 Run the optimization process. history = optimizer . run () Returns: - dict : History of the optimization process containing iterations, performance metrics, and chosen points Model \u00b6 GPRegressionModel \u00b6 Gaussian Process Regression Model with configurable kernel. from PyEGRO.meta.egogpr import GPRegressionModel Constructor \u00b6 GPRegressionModel ( train_x : torch . Tensor , train_y : torch . Tensor , likelihood : gpytorch . likelihoods . Likelihood , kernel : str = 'matern25' ) Parameters: - train_x (torch.Tensor): Training input data - train_y (torch.Tensor): Training target data - likelihood (gpytorch.likelihoods.Likelihood): GPyTorch likelihood - kernel (str, optional): Kernel type ('matern25', 'matern15', 'matern05', 'rbf', 'linear') Methods \u00b6 forward(x: torch.Tensor) \u00b6 Forward pass of GP model. Parameters: - x (torch.Tensor): Input data Returns: - gpytorch.distributions.MultivariateNormal : Distribution representing GP predictions get_hyperparameters() \u00b6 Get current hyperparameter values. Returns: - dict : Dictionary containing the hyperparameters predict(x: torch.Tensor) \u00b6 Make predictions with the model. Parameters: - x (torch.Tensor): Input points Returns: - tuple : (mean predictions, standard deviations) Configuration \u00b6 TrainingConfig \u00b6 Configuration class for EGO training parameters. from PyEGRO.meta.egogpr import TrainingConfig Constructor \u00b6 TrainingConfig ( max_iterations : int = 100 , rmse_threshold : float = 0.001 , rmse_patience : int = 10 , relative_improvement : float = 0.01 , acquisition_name : str = \"ei\" , acquisition_params : Dict [ str , Any ] = field ( default_factory = dict ), training_iter : int = 100 , verbose : bool = False , show_summary : bool = True , device : str = \"cuda\" if torch . cuda . is_available () else \"cpu\" , save_dir : str = \"RESULT_MODEL_GPR\" , learning_rate : float = 0.01 , early_stopping_patience : int = 20 , jitter : float = 1e-3 , kernel : str = \"matern25\" ) Parameters: - max_iterations (int): Maximum number of optimization iterations - rmse_threshold (float): RMSE threshold for early stopping - rmse_patience (int): Number of iterations without improvement before stopping - relative_improvement (float): Minimum relative improvement required - acquisition_name (str): Name of acquisition function - acquisition_params (Dict[str, Any]): Parameters for the acquisition function - training_iter (int): Number of training iterations for GP model - verbose (bool): Whether to print detailed training information - show_summary (bool): Whether to show parameter summary before training - device (str): Device to use for training ('cpu' or 'cuda') - save_dir (str): Directory to save results and models - learning_rate (float): Learning rate for model optimization - early_stopping_patience (int): Patience for early stopping during model training - jitter (float): Jitter value for numerical stability - kernel (str): Kernel type for GPR model Methods \u00b6 to_dict() \u00b6 Convert configuration to dictionary. Returns: - dict : Dictionary containing all configuration parameters Acquisition Functions \u00b6 Base Class \u00b6 AcquisitionFunction \u00b6 Base class for all acquisition functions. from egogpr.acquisition import AcquisitionFunction Methods \u00b6 evaluate(X) \u00b6 Evaluate the acquisition function at points X. Parameters: - X (np.ndarray): Points to evaluate Returns: - np.ndarray : Acquisition function values (negated for minimization) optimize() \u00b6 Optimize the acquisition function to find the next sampling point. Returns: - np.ndarray : Next point to evaluate Specific Acquisition Functions \u00b6 ExpectedImprovement \u00b6 Expected Improvement acquisition function. from PyEGRO.meta.egogpr import ExpectedImprovement LowerConfidenceBound \u00b6 Lower Confidence Bound acquisition function. from PyEGRO.meta.egogpr import LowerConfidenceBound PredictiveVariance \u00b6 Predictive Variance acquisition function. from PyEGRO.meta.egogpr import PredictiveVariance ProbabilityImprovement \u00b6 Probability of Improvement acquisition function. from PyEGRO.meta.egogpr import ProbabilityImprovement ExpectedImprovementGlobalFit \u00b6 Expected Improvement for Global Fit acquisition function. from PyEGRO.meta.egogpr import ExpectedImprovementGlobalFit Criterion3 \u00b6 Implementation of Criterion 3 acquisition function. from PyEGRO.meta.egogpr import Criterion3 ExplorationEnhancedEI \u00b6 Exploration Enhanced Expected Improvement (E\u00b3I) acquisition function. from PyEGRO.meta.egogpr import ExplorationEnhancedEI Factory Functions \u00b6 create_acquisition_function \u00b6 Create an acquisition function by name. from PyEGRO.meta.egogpr import create_acquisition_function acquisition = create_acquisition_function ( name : str , model , likelihood , bounds , scaler_x , scaler_y , y_train = None , ** kwargs ) propose_location \u00b6 Find the next location to sample using a specified acquisition function. from PyEGRO.meta.egogpr import propose_location X_next = propose_location ( acquisition_name : str , model , likelihood , y_train , bounds , scaler_x , scaler_y , ** kwargs ) Utilities \u00b6 Training Utilities \u00b6 train_gp_model \u00b6 Train a GP model with optimization settings. from PyEGRO.meta.egogpr.utils import train_gp_model train_gp_model ( model , likelihood , X_train : torch . Tensor , y_train : torch . Tensor , config : TrainingConfig ) Model Storage Utilities \u00b6 save_model_data \u00b6 Save model, scalers, hyperparameters and metadata. from PyEGRO.meta.egogpr import save_model_data save_model_data ( model , likelihood , scalers : Dict , metadata : Dict , save_dir : str ) load_model_data \u00b6 Load saved model data. from PyEGRO.meta.egogpr import load_model_data data = load_model_data ( save_dir : str ) Evaluation Utilities \u00b6 evaluate_model_performance \u00b6 Calculate model performance metrics using LOOCV. from PyEGRO.meta.egogpr.utils import evaluate_model_performance metrics = evaluate_model_performance ( model , likelihood , X_train : torch . Tensor , y_train : torch . Tensor ) Directory Utilities \u00b6 setup_directories \u00b6 Create necessary directories for saving results. from PyEGRO.meta.egogpr import setup_directories setup_directories ( save_dir : str , create_plots : bool = True ) Visualization \u00b6 EGOAnimator \u00b6 Class for creating animations of the optimization process. from PyEGRO.meta.egogpr import EGOAnimator Constructor \u00b6 EGOAnimator ( save_dir : str = 'RESULT_MODEL_GPR/animation' , frame_duration : int = 500 ) Methods \u00b6 save_1D_frame \u00b6 Save a 1D visualization frame. animator . save_1D_frame ( model : gpytorch . models . ExactGP , likelihood : gpytorch . likelihoods . GaussianLikelihood , X_train : np . ndarray , y_train : np . ndarray , scaler_x : object , scaler_y : object , bounds : np . ndarray , iteration : int , variable_names : List [ str ], device : torch . device = None , true_function : Optional [ callable ] = None , batch_size : int = 1000 ) save_2D_frame \u00b6 Save a 2D visualization frame. animator . save_2D_frame ( model : gpytorch . models . ExactGP , likelihood : gpytorch . likelihoods . GaussianLikelihood , X_train : np . ndarray , y_train : np . ndarray , scaler_x : object , scaler_y : object , bounds : np . ndarray , iteration : int , variable_names : List [ str ], n_initial_samples : int , n_points : int = 50 , batch_size : int = 1000 , device : torch . device = None ) create_gif \u00b6 Create GIF animation from saved frames. animator . create_gif ( duration : Optional [ int ] = None ) ModelVisualizer \u00b6 Class for creating final model visualizations. from PyEGRO.meta.egogpr import ModelVisualizer Constructor \u00b6 ModelVisualizer ( model , likelihood , scaler_x , scaler_y , bounds , variable_names , history , device , save_dir = 'RESULT_MODEL_GPR' ) Methods \u00b6 plot_final_prediction \u00b6 Create final prediction plots. visualizer . plot_final_prediction ( X_train , y_train , true_function = None ) plot_convergence_metrics \u00b6 Plot optimization metrics over iterations. visualizer . plot_convergence_metrics ( history : dict ) plot_error_analysis \u00b6 Plot prediction errors and residuals. visualizer . plot_error_analysis ( X_train , y_train )","title":"EGO-GPR"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#pyegrometaegogpr-api-reference","text":"This document provides detailed API documentation for the PyEGRO.meta.egogpr module, a framework for building highly accurate surrogate models through intelligent adaptive sampling strategies.","title":"PyEGRO.meta.egogpr API Reference"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#table-of-contents","text":"Main Optimizer Model Configuration Acquisition Functions Utilities Visualization","title":"Table of Contents"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#main-optimizer","text":"","title":"Main Optimizer"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#efficientglobaloptimization","text":"Main class for performing Efficient Global Optimization. from PyEGRO.meta.egogpr import EfficientGlobalOptimization","title":"EfficientGlobalOptimization"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#model","text":"","title":"Model"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#gpregressionmodel","text":"Gaussian Process Regression Model with configurable kernel. from PyEGRO.meta.egogpr import GPRegressionModel","title":"GPRegressionModel"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#configuration","text":"","title":"Configuration"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#trainingconfig","text":"Configuration class for EGO training parameters. from PyEGRO.meta.egogpr import TrainingConfig","title":"TrainingConfig"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#acquisition-functions","text":"","title":"Acquisition Functions"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#base-class","text":"","title":"Base Class"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#specific-acquisition-functions","text":"","title":"Specific Acquisition Functions"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#factory-functions","text":"","title":"Factory Functions"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#utilities","text":"","title":"Utilities"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#training-utilities","text":"","title":"Training Utilities"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#model-storage-utilities","text":"","title":"Model Storage Utilities"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#evaluation-utilities","text":"","title":"Evaluation Utilities"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#directory-utilities","text":"","title":"Directory Utilities"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#visualization","text":"","title":"Visualization"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#egoanimator","text":"Class for creating animations of the optimization process. from PyEGRO.meta.egogpr import EGOAnimator","title":"EGOAnimator"},{"location":"api-reference/meta/ego-gpr/egogpr_api_reference/#modelvisualizer","text":"Class for creating final model visualizations. from PyEGRO.meta.egogpr import ModelVisualizer","title":"ModelVisualizer"},{"location":"api-reference/meta/gpr/gpr_api_reference/","text":"PyEGRO GPR Module API Reference \u00b6 This document provides detailed API reference for the Gaussian Process Regression (GPR) module in the PyEGRO package. Table of Contents \u00b6 MetaTraining Class GPRegressionModel Class DeviceAgnosticGPR Class Visualization Functions MetaTraining Class \u00b6 The MetaTraining class provides a high-level interface for training and managing Gaussian Process Regression models. Constructor \u00b6 MetaTraining ( test_size = 0.3 , num_iterations = 1000 , prefer_gpu = True , show_progress = True , show_hardware_info = True , show_model_info = True , output_dir = 'RESULT_MODEL_GPR' , data_dir = 'DATA_PREPARATION' , data_info_file = None , data_training_file = None , kernel = 'matern15' , learning_rate = 0.01 , patience = 50 ) Parameters \u00b6 test_size (float, optional): Fraction of data to use for testing if no test data is provided. Default: 0.3 num_iterations (int, optional): Number of training iterations. Default: 1000 prefer_gpu (bool, optional): Whether to use GPU if available. Default: True show_progress (bool, optional): Whether to show detailed progress. Default: True show_hardware_info (bool, optional): Whether to show system hardware info. Default: True show_model_info (bool, optional): Whether to show model architecture info. Default: True output_dir (str, optional): Directory for saving results. Default: 'RESULT_MODEL_GPR' data_dir (str, optional): Directory containing input data. Default: 'DATA_PREPARATION' data_info_file (str, optional): Path to data info JSON file. Default: None data_training_file (str, optional): Path to training data CSV file. Default: None kernel (str, optional): Kernel to use for GPR model. Options: 'matern25' , 'matern15' , 'matern05' , 'rbf' , 'linear' . Default: 'matern15' learning_rate (float, optional): Learning rate for optimizer. Default: 0.01 patience (int, optional): Number of iterations to wait for improvement before early stopping. Default: 50 Methods \u00b6 train \u00b6 Train the GPR model with more flexible data options. train ( X = None , y = None , X_test = None , y_test = None , feature_names = None , custom_data = False ) Parameters \u00b6 X (numpy.ndarray, pandas.DataFrame, or str, optional): Training features or path to training data CSV. Default: None y (numpy.ndarray, pandas.DataFrame, or pandas.Series, optional): Training targets (only needed if custom_data=True). Default: None X_test (numpy.ndarray or pandas.DataFrame, optional): Test features. Default: None y_test (numpy.ndarray, pandas.DataFrame, or pandas.Series, optional): Test targets. Default: None feature_names (list of str, optional): List of feature names. Default: None custom_data (bool, optional): Whether using custom data instead of loading from files. Default: False Returns \u00b6 model (GPRegressionModel): Trained model scaler_X (StandardScaler): Feature scaler scaler_y (StandardScaler): Target scaler predict \u00b6 Make predictions using the trained model. predict ( X ) Parameters \u00b6 X (numpy.ndarray or pandas.DataFrame): Input features Returns \u00b6 mean (numpy.ndarray): Mean predictions std (numpy.ndarray): Standard deviations of predictions load_model \u00b6 Load a trained model from disk. load_model ( model_path = None ) Parameters \u00b6 model_path (str, optional): Path to the saved model file. If None, uses the default path. Default: None Returns \u00b6 model (GPRegressionModel): Loaded model print_hyperparameters \u00b6 Print the learned hyperparameters of the model. print_hyperparameters () GPRegressionModel Class \u00b6 The GPRegressionModel class implements a Gaussian Process Regression model with configurable kernels. Constructor \u00b6 GPRegressionModel ( train_x , train_y , likelihood , kernel = 'matern15' ) Parameters \u00b6 train_x (torch.Tensor): Training input data train_y (torch.Tensor): Training target data likelihood (gpytorch.likelihoods.Likelihood): GP likelihood function kernel (str, optional): Kernel type. Options: 'matern25' , 'matern15' , 'matern05' , 'rbf' , 'linear' . Default: 'matern15' Methods \u00b6 forward \u00b6 Computes the mean and covariance of the GP posterior. forward ( x ) Parameters \u00b6 x (torch.Tensor): Input data Returns \u00b6 gpytorch.distributions.MultivariateNormal : Distribution with predicted mean and covariance DeviceAgnosticGPR Class \u00b6 The DeviceAgnosticGPR class provides a device-agnostic handler for GPR models, making it easier to use models on both CPU and GPU. Constructor \u00b6 DeviceAgnosticGPR ( prefer_gpu = False ) Parameters \u00b6 prefer_gpu (bool, optional): Whether to use GPU if available. Default: False Methods \u00b6 load_model \u00b6 Load model using state dict approach. load_model ( model_dir = 'RESULT_MODEL_GPR' ) Parameters \u00b6 model_dir (str, optional): Directory containing the model files. Default: 'RESULT_MODEL_GPR' Returns \u00b6 bool : True if model was loaded successfully, False otherwise predict \u00b6 Make predictions with the loaded GPR model. predict ( X , batch_size = 1000 ) Parameters \u00b6 X (numpy.ndarray): Input features (n_samples, n_features) batch_size (int, optional): Batch size for processing large datasets. Default: 1000 Returns \u00b6 Tuple of (mean_predictions, std_predictions) as numpy arrays Visualization Functions \u00b6 visualize_gpr \u00b6 Create comprehensive visualizations for GPR model performance. visualize_gpr ( meta , X_train , y_train , X_test = None , y_test = None , variable_names = None , bounds = None , savefig = False , output_dir = None ) Parameters \u00b6 meta (MetaTraining): Trained MetaTraining instance X_train (numpy.ndarray or pandas.DataFrame): Training inputs y_train (numpy.ndarray, pandas.DataFrame, or pandas.Series): Training targets X_test (numpy.ndarray or pandas.DataFrame, optional): Test inputs. Default: None y_test (numpy.ndarray, pandas.DataFrame, or pandas.Series, optional): Test targets. Default: None variable_names (list of str, optional): Names of input variables. Default: None bounds (numpy.ndarray, optional): Bounds of input variables for sampling. Default: None savefig (bool, optional): Whether to save figures to disk. Default: False output_dir (str, optional): Directory to save figures. Default: None , uses meta.output_dir Returns \u00b6 figures (dict): Dictionary of figure handles","title":"GPR"},{"location":"api-reference/meta/gpr/gpr_api_reference/#pyegro-gpr-module-api-reference","text":"This document provides detailed API reference for the Gaussian Process Regression (GPR) module in the PyEGRO package.","title":"PyEGRO GPR Module API Reference"},{"location":"api-reference/meta/gpr/gpr_api_reference/#table-of-contents","text":"MetaTraining Class GPRegressionModel Class DeviceAgnosticGPR Class Visualization Functions","title":"Table of Contents"},{"location":"api-reference/meta/gpr/gpr_api_reference/#metatraining-class","text":"The MetaTraining class provides a high-level interface for training and managing Gaussian Process Regression models.","title":"MetaTraining Class"},{"location":"api-reference/meta/gpr/gpr_api_reference/#constructor","text":"MetaTraining ( test_size = 0.3 , num_iterations = 1000 , prefer_gpu = True , show_progress = True , show_hardware_info = True , show_model_info = True , output_dir = 'RESULT_MODEL_GPR' , data_dir = 'DATA_PREPARATION' , data_info_file = None , data_training_file = None , kernel = 'matern15' , learning_rate = 0.01 , patience = 50 )","title":"Constructor"},{"location":"api-reference/meta/gpr/gpr_api_reference/#methods","text":"","title":"Methods"},{"location":"api-reference/meta/gpr/gpr_api_reference/#gpregressionmodel-class","text":"The GPRegressionModel class implements a Gaussian Process Regression model with configurable kernels.","title":"GPRegressionModel Class"},{"location":"api-reference/meta/gpr/gpr_api_reference/#constructor_1","text":"GPRegressionModel ( train_x , train_y , likelihood , kernel = 'matern15' )","title":"Constructor"},{"location":"api-reference/meta/gpr/gpr_api_reference/#methods_1","text":"","title":"Methods"},{"location":"api-reference/meta/gpr/gpr_api_reference/#deviceagnosticgpr-class","text":"The DeviceAgnosticGPR class provides a device-agnostic handler for GPR models, making it easier to use models on both CPU and GPU.","title":"DeviceAgnosticGPR Class"},{"location":"api-reference/meta/gpr/gpr_api_reference/#constructor_2","text":"DeviceAgnosticGPR ( prefer_gpu = False )","title":"Constructor"},{"location":"api-reference/meta/gpr/gpr_api_reference/#methods_2","text":"","title":"Methods"},{"location":"api-reference/meta/gpr/gpr_api_reference/#visualization-functions","text":"","title":"Visualization Functions"},{"location":"api-reference/meta/gpr/gpr_api_reference/#visualize_gpr","text":"Create comprehensive visualizations for GPR model performance. visualize_gpr ( meta , X_train , y_train , X_test = None , y_test = None , variable_names = None , bounds = None , savefig = False , output_dir = None )","title":"visualize_gpr"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/","text":"PyEGRO ModelTesting API Reference \u00b6 This document provides detailed API reference for the Model Testing module in the PyEGRO package, which allows evaluation of trained GPR and Co-Kriging models on unseen data. Table of Contents \u00b6 ModelTester Class Utility Functions ModelTester Class \u00b6 The ModelTester class provides functionality for loading trained models and evaluating their performance on test data. Constructor \u00b6 ModelTester ( model_dir = 'RESULT_MODEL_GPR' , model_name = None , model_path = None , logger = None ) Parameters \u00b6 model_dir (str, optional): Directory containing trained model files. Default: 'RESULT_MODEL_GPR' model_name (str, optional): Base name of the model file without extension. Default: None (will be inferred from directory or file path) model_path (str, optional): Direct path to the model file. Default: None logger (logging.Logger, optional): Logger object for logging messages. Default: None Methods \u00b6 load_model \u00b6 Load the trained model and scalers from disk. load_model () Returns \u00b6 self : Returns the ModelTester instance for method chaining load_test_data \u00b6 Load test data from CSV file or generate synthetic test data. load_test_data ( data_path = None , feature_cols = None , target_col = 'y' , n_samples = 100 , n_features = 2 ) Parameters \u00b6 data_path (str, optional): Path to the CSV file containing test data. Default: None feature_cols (list of str, optional): List of feature column names. Default: None (all columns except target) target_col (str, optional): Target column name. Default: 'y' n_samples (int, optional): Number of samples for synthetic data if no data_path provided. Default: 100 n_features (int, optional): Number of features for synthetic data if no data_path provided. Default: 2 Returns \u00b6 Tuple of (X_test, y_test) : Test features and targets as numpy arrays evaluate \u00b6 Evaluate the model performance on test data. evaluate ( X_test , y_test ) Parameters \u00b6 X_test (numpy.ndarray): Test features y_test (numpy.ndarray): Test targets Returns \u00b6 self : Returns the ModelTester instance for method chaining with updated test_results property save_results \u00b6 Save test results and generate plots. save_results ( output_dir = 'test_results' ) Parameters \u00b6 output_dir (str, optional): Directory to save results. Default: 'test_results' Returns \u00b6 self : Returns the ModelTester instance for method chaining plot_results \u00b6 Generate and optionally save plots of test results. plot_results ( output_dir = 'test_results' , show_plots = True , save_plots = True , smooth = True , smooth_window = 11 ) Parameters \u00b6 output_dir (str, optional): Directory to save plots. Default: 'test_results' show_plots (bool, optional): Whether to display plots. Default: True save_plots (bool, optional): Whether to save plots to disk. Default: True smooth (bool, optional): Whether to apply smoothing to uncertainty plots. Default: True smooth_window (int, optional): Window size for smoothing filter. Default: 11 Returns \u00b6 figures (dict): Dictionary of matplotlib figure objects Properties \u00b6 test_results (dict): Contains test metrics and predictions after running evaluate . Keys include: r2 : R\u00b2 score mse : Mean squared error rmse : Root mean squared error mae : Mean absolute error y_test : Test targets y_pred : Model predictions std_dev : Standard deviations of predictions model_type : Type of model ('gpr' or 'cokriging') Utility Functions \u00b6 load_and_test_model \u00b6 A convenience function to load a model and test it in one call. load_and_test_model ( data_path = None , model_dir = None , model_name = None , model_path = None , output_dir = 'test_results' , feature_cols = None , target_col = 'y' , logger = None , show_plots = True , smooth = True , smooth_window = 11 ) Parameters \u00b6 data_path (str, optional): Path to the CSV file containing test data. Default: None model_dir (str, optional): Directory containing trained model files. Default: None model_name (str, optional): Base name of the model file without extension. Default: None model_path (str, optional): Direct path to the model file. Default: None output_dir (str, optional): Directory to save results. Default: 'test_results' feature_cols (list of str, optional): List of feature column names. Default: None target_col (str, optional): Target column name. Default: 'y' logger (logging.Logger, optional): Logger object for logging messages. Default: None show_plots (bool, optional): Whether to display plots. Default: True smooth (bool, optional): Whether to apply smoothing to uncertainty plots. Default: True smooth_window (int, optional): Window size for smoothing filter. Default: 11 Returns \u00b6 test_results (dict): Dictionary containing test metrics and predictions","title":"Model Evaluation"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#pyegro-modeltesting-api-reference","text":"This document provides detailed API reference for the Model Testing module in the PyEGRO package, which allows evaluation of trained GPR and Co-Kriging models on unseen data.","title":"PyEGRO ModelTesting API Reference"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#table-of-contents","text":"ModelTester Class Utility Functions","title":"Table of Contents"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#modeltester-class","text":"The ModelTester class provides functionality for loading trained models and evaluating their performance on test data.","title":"ModelTester Class"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#constructor","text":"ModelTester ( model_dir = 'RESULT_MODEL_GPR' , model_name = None , model_path = None , logger = None )","title":"Constructor"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#methods","text":"","title":"Methods"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#properties","text":"test_results (dict): Contains test metrics and predictions after running evaluate . Keys include: r2 : R\u00b2 score mse : Mean squared error rmse : Root mean squared error mae : Mean absolute error y_test : Test targets y_pred : Model predictions std_dev : Standard deviations of predictions model_type : Type of model ('gpr' or 'cokriging')","title":"Properties"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#utility-functions","text":"","title":"Utility Functions"},{"location":"api-reference/meta/model-evalulation/modeltesting_api_reference/#load_and_test_model","text":"A convenience function to load a model and test it in one call. load_and_test_model ( data_path = None , model_dir = None , model_name = None , model_path = None , output_dir = 'test_results' , feature_cols = None , target_col = 'y' , logger = None , show_plots = True , smooth = True , smooth_window = 11 )","title":"load_and_test_model"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/","text":"PyEGRO.robustopt.method_mcs API Reference \u00b6 This document provides detailed API documentation for the PyEGRO.robustopt.method_mcs module, a framework for robust optimization using Monte Carlo Simulation (MCS) for uncertainty quantification. Table of Contents \u00b6 Problem Definition Main Optimization Function Problem Class Algorithm Setup Results Handling Problem Definition \u00b6 The robust optimization problem can be defined in three different ways: Using a dictionary format with variables and their properties Loading a pre-defined problem from a JSON file 1. Dictionary Format \u00b6 data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] } 2. Loading from JSON File \u00b6 import json # Load existing problem definition with open ( 'data_info.json' , 'r' ) as f : data_info = json . load ( f ) Variable Definition Format \u00b6 Design Variables ( vars_type = 'design_vars' ) \u00b6 Common parameters for design variables: - name : Name of the variable (str) - vars_type : Must be 'design_vars' (str) - distribution : Distribution type (str) - range_bounds : [lower_bound, upper_bound] (List[float]) - cov : Coefficient of variation (float, optional) - std : Standard deviation (float, optional, alternative to cov) - description : Description of the variable (str, optional) Note: - cov > 0 : Uncertain design variable with specified coefficient of variation - cov = 0 : Deterministic design variable Environmental Variables ( vars_type = 'env_vars' ) \u00b6 Common parameters for environmental variables: - name : Name of the variable (str) - vars_type : Must be 'env_vars' (str) - distribution : Distribution type (str) - description : Description of the variable (str, optional) Additional parameters depend on the distribution type. Supported Distribution Types \u00b6 The following distributions are supported for both design and environmental variables: 1. Uniform Distribution \u00b6 # Required parameters: 'distribution' : 'uniform' , 'low' : float , # Lower bound 'high' : float # Upper bound 2. Normal Distribution \u00b6 # Required parameters: 'distribution' : 'normal' , 'mean' : float , # Mean value # Plus one of the following: 'cov' : float , # Coefficient of variation 'std' : float # Standard deviation # Optional: 'low' : float , # Lower bound (if truncated) 'high' : float # Upper bound (if truncated) 3. Lognormal Distribution \u00b6 # Required parameters: 'distribution' : 'lognormal' , 'mean' : float , # Mean value # Plus one of the following: 'cov' : float , # Coefficient of variation 'std' : float # Standard deviation # Optional: 'low' : float , # Lower bound (if truncated) 'high' : float # Upper bound (if truncated) Main Optimization Function \u00b6 run_robust_optimization \u00b6 Runs robust optimization using Monte Carlo Simulation for uncertainty quantification. from PyEGRO.robustopt.method_mcs.mcs import run_robust_optimization results = run_robust_optimization ( data_info : Dict , true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , mcs_samples : int = 10000 , pop_size : int = 50 , n_gen : int = 100 , metric : str = 'hv' , reference_point : Optional [ np . ndarray ] = None , show_info : bool = True , verbose : bool = False ) Parameters \u00b6 data_info (Dict): Problem definition data containing variables true_func (callable, optional): True objective function for direct evaluation model_handler (Any, optional): Handler for surrogate model evaluations mcs_samples (int): Number of Monte Carlo samples per evaluation pop_size (int): Population size for NSGA-II n_gen (int): Number of generations metric (str): Performance metric ('hv' for hypervolume) reference_point (np.ndarray, optional): Reference point for hypervolume calculation show_info (bool): Whether to display information during optimization verbose (bool): Whether to print detailed progress Returns \u00b6 Dict : Results dictionary containing: pareto_front : Array of objective values (Mean, StdDev) pareto_set : Array of decision variables convergence_history : Dict with metric history runtime : Total optimization time success : Bool indicating successful completion Notes \u00b6 Either true_func or model_handler must be provided The optimization minimizes both mean performance and standard deviation Hypervolume (HV) metric is used to track convergence Problem Class \u00b6 RobustOptimizationProblemMCS \u00b6 Multi-objective optimization problem using Monte Carlo Sampling for uncertainty quantification. from PyEGRO.robustopt.method_mcs.mcs import RobustOptimizationProblemMCS problem = RobustOptimizationProblemMCS ( variables : List [ Dict ] or List [ Variable ], true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , num_mcs_samples : int = 100000 , batch_size : int = 1000 ) Parameters \u00b6 variables (List): List of variable definitions (dicts or Variable objects) true_func (callable, optional): True objective function for direct evaluation model_handler (Any, optional): Handler for surrogate model evaluations num_mcs_samples (int): Number of Monte Carlo samples per evaluation batch_size (int): Batch size for model evaluations Methods \u00b6 _evaluate_samples(samples: np.ndarray) -> np.ndarray \u00b6 Evaluates samples using either the true function or surrogate model. _generate_samples(design_point: np.ndarray) -> np.ndarray \u00b6 Generates Monte Carlo samples for a given design point. _evaluate(X: np.ndarray, out: Dict, *args, **kwargs) \u00b6 Evaluates objectives using Monte Carlo simulation. Algorithm Setup \u00b6 setup_algorithm \u00b6 Sets up the NSGA-II algorithm with standard parameters. from PyEGRO.robustopt.method_mcs.mcs import setup_algorithm algorithm = setup_algorithm ( pop_size : int ) Parameters \u00b6 pop_size (int): Population size for NSGA-II Returns \u00b6 NSGA2 : Configured NSGA-II algorithm Results Handling \u00b6 save_optimization_results \u00b6 Saves optimization results and creates visualizations. from PyEGRO.robustopt.method_mcs.mcs import save_optimization_results save_optimization_results ( results : Dict , data_info : Dict , save_dir : str = 'RESULT_PARETO_FRONT_MCS' ) Parameters \u00b6 results (Dict): Optimization results dictionary data_info (Dict): Problem definition data save_dir (str): Directory to save results Saved Outputs \u00b6 pareto_solutions.csv : CSV file with Pareto front solutions optimization_summary.txt : Text summary of optimization results convergence.csv : CSV file with convergence history Various visualization plots: Pareto front plot Convergence plot Parameter correlation plots Trade-off analysis Notes \u00b6 Creates specified directory if it doesn't exist Automatically determines appropriate visualizations based on problem dimension","title":"MCS Approach"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#pyegrorobustoptmethod_mcs-api-reference","text":"This document provides detailed API documentation for the PyEGRO.robustopt.method_mcs module, a framework for robust optimization using Monte Carlo Simulation (MCS) for uncertainty quantification.","title":"PyEGRO.robustopt.method_mcs API Reference"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#table-of-contents","text":"Problem Definition Main Optimization Function Problem Class Algorithm Setup Results Handling","title":"Table of Contents"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#problem-definition","text":"The robust optimization problem can be defined in three different ways: Using a dictionary format with variables and their properties Loading a pre-defined problem from a JSON file","title":"Problem Definition"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#1-dictionary-format","text":"data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] }","title":"1. Dictionary Format"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#2-loading-from-json-file","text":"import json # Load existing problem definition with open ( 'data_info.json' , 'r' ) as f : data_info = json . load ( f )","title":"2. Loading from JSON File"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#variable-definition-format","text":"","title":"Variable Definition Format"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#supported-distribution-types","text":"The following distributions are supported for both design and environmental variables:","title":"Supported Distribution Types"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#main-optimization-function","text":"","title":"Main Optimization Function"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#run_robust_optimization","text":"Runs robust optimization using Monte Carlo Simulation for uncertainty quantification. from PyEGRO.robustopt.method_mcs.mcs import run_robust_optimization results = run_robust_optimization ( data_info : Dict , true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , mcs_samples : int = 10000 , pop_size : int = 50 , n_gen : int = 100 , metric : str = 'hv' , reference_point : Optional [ np . ndarray ] = None , show_info : bool = True , verbose : bool = False )","title":"run_robust_optimization"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#problem-class","text":"","title":"Problem Class"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#robustoptimizationproblemmcs","text":"Multi-objective optimization problem using Monte Carlo Sampling for uncertainty quantification. from PyEGRO.robustopt.method_mcs.mcs import RobustOptimizationProblemMCS problem = RobustOptimizationProblemMCS ( variables : List [ Dict ] or List [ Variable ], true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , num_mcs_samples : int = 100000 , batch_size : int = 1000 )","title":"RobustOptimizationProblemMCS"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#algorithm-setup","text":"","title":"Algorithm Setup"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#setup_algorithm","text":"Sets up the NSGA-II algorithm with standard parameters. from PyEGRO.robustopt.method_mcs.mcs import setup_algorithm algorithm = setup_algorithm ( pop_size : int )","title":"setup_algorithm"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#results-handling","text":"","title":"Results Handling"},{"location":"api-reference/robustopt/approach-mcs/robustopt_mcs_api_reference/#save_optimization_results","text":"Saves optimization results and creates visualizations. from PyEGRO.robustopt.method_mcs.mcs import save_optimization_results save_optimization_results ( results : Dict , data_info : Dict , save_dir : str = 'RESULT_PARETO_FRONT_MCS' )","title":"save_optimization_results"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/","text":"PyEGRO.robustopt.method_nnmcs API Reference \u00b6 This document provides detailed API documentation for the PyEGRO.robustopt.method_nnmcs module, a two-stage approach for robust optimization that combines Neural Networks (NN) with Monte Carlo Simulation (MCS) for efficient uncertainty quantification. Table of Contents \u00b6 Overview Configuration Classes Main Optimization Class Pipeline Stages Usage Notes Overview \u00b6 The Two-Stage Robust Optimization approach addresses the computational challenge in traditional robust optimization by using neural networks to accelerate the optimization process. It consists of the following main steps: Uncertainty Propagation : Sample design points and perform low-fidelity MCS to generate training data Hyperparameter Optimization : Find optimal neural network architectures for predicting mean and standard deviation Neural Network Training : Train surrogate models to predict statistical moments Two-Stage Optimization : Use neural networks in optimization loop with verification by high-fidelity MCS This approach provides a balance between computational efficiency and solution accuracy. Configuration Classes \u00b6 ANNConfig \u00b6 Configuration class for neural network hyperparameters and training settings. from PyEGRO.robustopt.method_nnmcs import ANNConfig ann_config = ANNConfig ( n_trials = 50 , n_jobs =- 1 , hyperparameter_ranges = { 'n_layers' : ( 2 , 4 ), 'n_units' : ( 64 , 256 ), 'learning_rate' : ( 1e-4 , 1e-2 ), 'batch_size' : ( 32 , 64 ), 'max_epochs' : ( 100 , 300 ), 'activations' : [ 'ReLU' , 'LeakyReLU' ], 'optimizers' : [ 'Adam' ], 'loss_functions' : [ 'MSELoss' ] } ) Parameters \u00b6 n_trials (int): Number of hyperparameter optimization trials n_jobs (int): Number of parallel jobs (-1 for all available cores) hyperparameter_ranges (Dict): Dictionary of hyperparameter ranges for optimization SamplingConfig \u00b6 Configuration class for sampling parameters in the pipeline. from PyEGRO.robustopt.method_nnmcs import SamplingConfig sampling_config = SamplingConfig ( n_lhs_samples = 2000 , n_mcs_samples_low = 10000 , n_mcs_samples_high = 1000000 , random_seed = 42 ) Parameters \u00b6 n_lhs_samples (int): Number of LHS design samples for initial sampling n_mcs_samples_low (int): Number of MCS samples for low-fidelity uncertainty propagation (training) n_mcs_samples_high (int): Number of MCS samples for high-fidelity verification random_seed (int): Random seed for reproducibility OptimizationConfig \u00b6 Configuration class for optimization parameters. from PyEGRO.robustopt.method_nnmcs import OptimizationConfig opt_config = OptimizationConfig ( pop_size = 50 , n_gen = 100 , prefer_gpu = True , verbose = False , random_seed = 42 , metric = 'hv' , reference_point = np . array ([ 5.0 , 5.0 ]) ) Parameters \u00b6 pop_size (int): Population size for NSGA-II n_gen (int): Number of generations for optimization prefer_gpu (bool): Whether to use GPU if available verbose (bool): Whether to print detailed information random_seed (int): Random seed for reproducibility metric (str): Performance metric ('hv' for hypervolume) reference_point (np.ndarray, optional): Reference point for hypervolume calculation PathConfig \u00b6 Configuration class for file paths used in the pipeline. from PyEGRO.robustopt.method_nnmcs import PathConfig path_config = PathConfig ( model_type = 'gpr' , model_dir = 'RESULT_MODEL_GPR' , ann_model_dir = 'RESULT_MODEL_ANN' , data_info_path = 'DATA_PREPARATION/data_info.json' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE' , qoi_dir = 'RESULT_QOI' ) Parameters \u00b6 model_type (str): Type of metamodel ('gpr', 'cokriging', etc.) model_dir (str): Directory for surrogate model ann_model_dir (str): Directory for neural network models data_info_path (str): Path to data_info.json file results_dir (str): Directory for optimization results qoi_dir (str): Directory for uncertainty quantification results Main Optimization Class \u00b6 RobustOptimization \u00b6 Main class that orchestrates the two-stage robust optimization process. from PyEGRO.robustopt.method_nnmcs import RobustOptimization optimizer = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config ) pareto_set , pareto_front = optimizer . run () Constructor Parameters \u00b6 ann_config (ANNConfig): Neural network configuration sampling_config (SamplingConfig): Sampling configuration optimization_config (OptimizationConfig): Optimization configuration path_config (PathConfig): Path configuration Methods \u00b6 run() -> Tuple[np.ndarray, np.ndarray] \u00b6 Run the complete two-stage robust optimization pipeline. Returns: Tuple containing the Pareto set and Pareto front Pipeline Stages \u00b6 The two-stage robust optimization pipeline consists of four main stages: 1. Uncertainty Propagation \u00b6 Samples design points using Latin Hypercube Sampling (LHS) and performs low-fidelity Monte Carlo Simulation (MCS) to generate training data for the neural networks. This stage produces a dataset of design points and their corresponding statistical moments (mean and standard deviation). 2. Hyperparameter Optimization \u00b6 Uses Optuna to find optimal neural network architectures for predicting the mean and standard deviation. This stage searches through the hyperparameter space defined in ANNConfig to find configurations that minimize validation loss. 3. Neural Network Training \u00b6 Trains two neural networks using the best hyperparameters: - One for predicting the mean of the objective function - One for predicting the standard deviation of the objective function These neural networks serve as fast surrogate models for the statistical moments. 4. Two-Stage Optimization \u00b6 Performs multi-objective optimization using the trained neural networks for fast evaluations. The optimization seeks to find the Pareto front balancing performance (mean) and robustness (standard deviation). Final solutions are verified using high-fidelity MCS. Usage Notes \u00b6 GPU Acceleration \u00b6 The pipeline supports GPU acceleration for both neural network training and surrogate model evaluations. Set prefer_gpu=True in OptimizationConfig to use GPU if available. File Structure \u00b6 The pipeline creates several directories to store intermediate and final results: - qoi_dir : Contains uncertainty propagation results - ann_model_dir : Contains trained neural network models - results_dir : Contains final optimization results Model Types \u00b6 The pipeline supports different types of surrogate models through the model_type parameter in PathConfig : - 'gpr': Gaussian Process Regression - 'cokriging': Cokriging (multi-fidelity) models Customization \u00b6 The pipeline can be customized through the configuration classes: - Modify hyperparameter_ranges in ANNConfig to explore different neural network architectures - Adjust sample sizes in SamplingConfig to balance accuracy and computational cost - Change optimization parameters in OptimizationConfig to control convergence behavior","title":"NNMCS Approach"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#pyegrorobustoptmethod_nnmcs-api-reference","text":"This document provides detailed API documentation for the PyEGRO.robustopt.method_nnmcs module, a two-stage approach for robust optimization that combines Neural Networks (NN) with Monte Carlo Simulation (MCS) for efficient uncertainty quantification.","title":"PyEGRO.robustopt.method_nnmcs API Reference"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#table-of-contents","text":"Overview Configuration Classes Main Optimization Class Pipeline Stages Usage Notes","title":"Table of Contents"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#overview","text":"The Two-Stage Robust Optimization approach addresses the computational challenge in traditional robust optimization by using neural networks to accelerate the optimization process. It consists of the following main steps: Uncertainty Propagation : Sample design points and perform low-fidelity MCS to generate training data Hyperparameter Optimization : Find optimal neural network architectures for predicting mean and standard deviation Neural Network Training : Train surrogate models to predict statistical moments Two-Stage Optimization : Use neural networks in optimization loop with verification by high-fidelity MCS This approach provides a balance between computational efficiency and solution accuracy.","title":"Overview"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#configuration-classes","text":"","title":"Configuration Classes"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#annconfig","text":"Configuration class for neural network hyperparameters and training settings. from PyEGRO.robustopt.method_nnmcs import ANNConfig ann_config = ANNConfig ( n_trials = 50 , n_jobs =- 1 , hyperparameter_ranges = { 'n_layers' : ( 2 , 4 ), 'n_units' : ( 64 , 256 ), 'learning_rate' : ( 1e-4 , 1e-2 ), 'batch_size' : ( 32 , 64 ), 'max_epochs' : ( 100 , 300 ), 'activations' : [ 'ReLU' , 'LeakyReLU' ], 'optimizers' : [ 'Adam' ], 'loss_functions' : [ 'MSELoss' ] } )","title":"ANNConfig"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#samplingconfig","text":"Configuration class for sampling parameters in the pipeline. from PyEGRO.robustopt.method_nnmcs import SamplingConfig sampling_config = SamplingConfig ( n_lhs_samples = 2000 , n_mcs_samples_low = 10000 , n_mcs_samples_high = 1000000 , random_seed = 42 )","title":"SamplingConfig"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#optimizationconfig","text":"Configuration class for optimization parameters. from PyEGRO.robustopt.method_nnmcs import OptimizationConfig opt_config = OptimizationConfig ( pop_size = 50 , n_gen = 100 , prefer_gpu = True , verbose = False , random_seed = 42 , metric = 'hv' , reference_point = np . array ([ 5.0 , 5.0 ]) )","title":"OptimizationConfig"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#pathconfig","text":"Configuration class for file paths used in the pipeline. from PyEGRO.robustopt.method_nnmcs import PathConfig path_config = PathConfig ( model_type = 'gpr' , model_dir = 'RESULT_MODEL_GPR' , ann_model_dir = 'RESULT_MODEL_ANN' , data_info_path = 'DATA_PREPARATION/data_info.json' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE' , qoi_dir = 'RESULT_QOI' )","title":"PathConfig"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#main-optimization-class","text":"","title":"Main Optimization Class"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#robustoptimization","text":"Main class that orchestrates the two-stage robust optimization process. from PyEGRO.robustopt.method_nnmcs import RobustOptimization optimizer = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config ) pareto_set , pareto_front = optimizer . run ()","title":"RobustOptimization"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#pipeline-stages","text":"The two-stage robust optimization pipeline consists of four main stages:","title":"Pipeline Stages"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#1-uncertainty-propagation","text":"Samples design points using Latin Hypercube Sampling (LHS) and performs low-fidelity Monte Carlo Simulation (MCS) to generate training data for the neural networks. This stage produces a dataset of design points and their corresponding statistical moments (mean and standard deviation).","title":"1. Uncertainty Propagation"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#2-hyperparameter-optimization","text":"Uses Optuna to find optimal neural network architectures for predicting the mean and standard deviation. This stage searches through the hyperparameter space defined in ANNConfig to find configurations that minimize validation loss.","title":"2. Hyperparameter Optimization"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#3-neural-network-training","text":"Trains two neural networks using the best hyperparameters: - One for predicting the mean of the objective function - One for predicting the standard deviation of the objective function These neural networks serve as fast surrogate models for the statistical moments.","title":"3. Neural Network Training"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#4-two-stage-optimization","text":"Performs multi-objective optimization using the trained neural networks for fast evaluations. The optimization seeks to find the Pareto front balancing performance (mean) and robustness (standard deviation). Final solutions are verified using high-fidelity MCS.","title":"4. Two-Stage Optimization"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#usage-notes","text":"","title":"Usage Notes"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#gpu-acceleration","text":"The pipeline supports GPU acceleration for both neural network training and surrogate model evaluations. Set prefer_gpu=True in OptimizationConfig to use GPU if available.","title":"GPU Acceleration"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#file-structure","text":"The pipeline creates several directories to store intermediate and final results: - qoi_dir : Contains uncertainty propagation results - ann_model_dir : Contains trained neural network models - results_dir : Contains final optimization results","title":"File Structure"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#model-types","text":"The pipeline supports different types of surrogate models through the model_type parameter in PathConfig : - 'gpr': Gaussian Process Regression - 'cokriging': Cokriging (multi-fidelity) models","title":"Model Types"},{"location":"api-reference/robustopt/approach-nnmcs/robustopt_nnmcs_api_reference/#customization","text":"The pipeline can be customized through the configuration classes: - Modify hyperparameter_ranges in ANNConfig to explore different neural network architectures - Adjust sample sizes in SamplingConfig to balance accuracy and computational cost - Change optimization parameters in OptimizationConfig to control convergence behavior","title":"Customization"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/","text":"PyEGRO.robustopt.method_pce API Reference \u00b6 This document provides detailed API documentation for the PyEGRO.robustopt.method_pce module, a framework for robust optimization using Polynomial Chaos Expansion (PCE) for uncertainty quantification. Table of Contents \u00b6 Overview Problem Definition Main Optimization Function PCE Classes Algorithm Setup Results Handling Overview \u00b6 Polynomial Chaos Expansion (PCE) is a powerful spectral method for uncertainty quantification that represents stochastic responses using a series of orthogonal polynomials. Compared to Monte Carlo Simulation (MCS), PCE often requires fewer samples to achieve accurate statistical moments and offers rapid convergence for smooth responses. The PyEGRO.robustopt.method_pce module implements PCE-based uncertainty quantification within a robust optimization framework. The optimization is performed using the NSGA-II algorithm to find solutions that balance performance (mean) and robustness (standard deviation). Problem Definition \u00b6 The robust optimization problem can be defined in three different ways, just as with the MCS method: Using a dictionary format with variables and their properties Loading a pre-defined problem from a JSON file For detailed variable definition formats and supported distributions, please refer to the PyEGRO.robustopt.method_mcs API Reference . Main Optimization Function \u00b6 run_robust_optimization \u00b6 Runs robust optimization using Polynomial Chaos Expansion for uncertainty quantification. from PyEGRO.robustopt.method_pce import run_robust_optimization results = run_robust_optimization ( data_info : Dict , true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , pce_samples : int = 200 , pce_order : int = 3 , pop_size : int = 50 , n_gen : int = 100 , metric : str = 'hv' , reference_point : Optional [ np . ndarray ] = None , show_info : bool = True , verbose : bool = False ) Parameters \u00b6 data_info (Dict): Problem definition data containing variables true_func (callable, optional): True objective function for direct evaluation model_handler (Any, optional): Handler for surrogate model evaluations pce_samples (int): Number of PCE samples per evaluation pce_order (int): Order of the PCE polynomial expansion pop_size (int): Population size for NSGA-II n_gen (int): Number of generations metric (str): Performance metric ('hv' for hypervolume) reference_point (np.ndarray, optional): Reference point for hypervolume calculation show_info (bool): Whether to display information during optimization verbose (bool): Whether to print detailed progress Returns \u00b6 Dict : Results dictionary containing: pareto_front : Array of objective values (Mean, StdDev) pareto_set : Array of decision variables convergence_history : Dict with metric history runtime : Total optimization time success : Bool indicating successful completion Notes \u00b6 Either true_func or model_handler must be provided The optimization minimizes both mean performance and standard deviation PCE parameters ( pce_samples and pce_order ) control the accuracy and computational cost of the uncertainty quantification Higher PCE orders provide more accurate approximations but require more samples PCE Classes \u00b6 PCESampler \u00b6 Class for generating samples using Polynomial Chaos Expansion techniques. from PyEGRO.robustopt.method_pce import PCESampler sampler = PCESampler ( n_samples = 200 , order = 3 ) Parameters \u00b6 n_samples (int): Number of samples to generate order (int): Order of the polynomial expansion Methods \u00b6 generate_samples(mean: float, std: float, bounds: Optional[tuple] = None) -> np.ndarray \u00b6 Generate PCE samples with improved uncertainty quantification. Parameters: - mean (float): Mean value - std (float): Standard deviation - bounds (tuple, optional): Tuple containing (lower, upper) bounds Returns: numpy.ndarray of samples RobustOptimizationProblemPCE \u00b6 Multi-objective optimization problem using Polynomial Chaos Expansion for uncertainty quantification. from PyEGRO.robustopt.method_pce import RobustOptimizationProblemPCE problem = RobustOptimizationProblemPCE ( variables : List [ Dict ], true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , num_pce_samples : int = 200 , pce_order : int = 3 , batch_size : int = 1000 ) Parameters \u00b6 variables (List): List of variable definitions (dicts or Variable objects) true_func (callable, optional): True objective function for direct evaluation model_handler (Any, optional): Handler for surrogate model evaluations num_pce_samples (int): Number of PCE samples per evaluation pce_order (int): Order of the PCE polynomial expansion batch_size (int): Batch size for model evaluations Methods \u00b6 _generate_samples(design_point: np.ndarray) -> np.ndarray \u00b6 Generate PCE samples for a given design point. _evaluate_samples(samples: np.ndarray) -> np.ndarray \u00b6 Evaluate samples using either the true function or surrogate model. _evaluate(X: np.ndarray, out: Dict, *args, **kwargs) \u00b6 Evaluate objectives using PCE-based uncertainty quantification. Algorithm Setup \u00b6 setup_algorithm \u00b6 Sets up the NSGA-II algorithm with standard parameters. from PyEGRO.robustopt.method_pce import setup_algorithm algorithm = setup_algorithm ( pop_size : int ) Parameters \u00b6 pop_size (int): Population size for NSGA-II Returns \u00b6 NSGA2 : Configured NSGA-II algorithm Results Handling \u00b6 save_optimization_results \u00b6 Saves optimization results and creates visualizations. from PyEGRO.robustopt.method_pce import save_optimization_results save_optimization_results ( results : Dict , data_info : Dict , save_dir : str = 'RESULT_PARETO_FRONT_PCE' ) Parameters \u00b6 results (Dict): Optimization results dictionary data_info (Dict): Problem definition data save_dir (str): Directory to save results Saved Outputs \u00b6 pareto_solutions.csv : CSV file with Pareto front solutions optimization_summary.txt : Text summary of optimization results convergence.csv : CSV file with convergence history Various visualization plots: Pareto front plot Convergence plot Parameter correlation plots Trade-off analysis Notes \u00b6 The summary output includes PCE-specific information such as PCE evaluations Creates specified directory if it doesn't exist Automatically determines appropriate visualizations based on problem dimension","title":"PCE Approach"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#pyegrorobustoptmethod_pce-api-reference","text":"This document provides detailed API documentation for the PyEGRO.robustopt.method_pce module, a framework for robust optimization using Polynomial Chaos Expansion (PCE) for uncertainty quantification.","title":"PyEGRO.robustopt.method_pce API Reference"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#table-of-contents","text":"Overview Problem Definition Main Optimization Function PCE Classes Algorithm Setup Results Handling","title":"Table of Contents"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#overview","text":"Polynomial Chaos Expansion (PCE) is a powerful spectral method for uncertainty quantification that represents stochastic responses using a series of orthogonal polynomials. Compared to Monte Carlo Simulation (MCS), PCE often requires fewer samples to achieve accurate statistical moments and offers rapid convergence for smooth responses. The PyEGRO.robustopt.method_pce module implements PCE-based uncertainty quantification within a robust optimization framework. The optimization is performed using the NSGA-II algorithm to find solutions that balance performance (mean) and robustness (standard deviation).","title":"Overview"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#problem-definition","text":"The robust optimization problem can be defined in three different ways, just as with the MCS method: Using a dictionary format with variables and their properties Loading a pre-defined problem from a JSON file For detailed variable definition formats and supported distributions, please refer to the PyEGRO.robustopt.method_mcs API Reference .","title":"Problem Definition"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#main-optimization-function","text":"","title":"Main Optimization Function"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#run_robust_optimization","text":"Runs robust optimization using Polynomial Chaos Expansion for uncertainty quantification. from PyEGRO.robustopt.method_pce import run_robust_optimization results = run_robust_optimization ( data_info : Dict , true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , pce_samples : int = 200 , pce_order : int = 3 , pop_size : int = 50 , n_gen : int = 100 , metric : str = 'hv' , reference_point : Optional [ np . ndarray ] = None , show_info : bool = True , verbose : bool = False )","title":"run_robust_optimization"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#pce-classes","text":"","title":"PCE Classes"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#pcesampler","text":"Class for generating samples using Polynomial Chaos Expansion techniques. from PyEGRO.robustopt.method_pce import PCESampler sampler = PCESampler ( n_samples = 200 , order = 3 )","title":"PCESampler"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#robustoptimizationproblempce","text":"Multi-objective optimization problem using Polynomial Chaos Expansion for uncertainty quantification. from PyEGRO.robustopt.method_pce import RobustOptimizationProblemPCE problem = RobustOptimizationProblemPCE ( variables : List [ Dict ], true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , num_pce_samples : int = 200 , pce_order : int = 3 , batch_size : int = 1000 )","title":"RobustOptimizationProblemPCE"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#algorithm-setup","text":"","title":"Algorithm Setup"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#setup_algorithm","text":"Sets up the NSGA-II algorithm with standard parameters. from PyEGRO.robustopt.method_pce import setup_algorithm algorithm = setup_algorithm ( pop_size : int )","title":"setup_algorithm"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#results-handling","text":"","title":"Results Handling"},{"location":"api-reference/robustopt/approach-pce/robustopt_pce_api_reference/#save_optimization_results","text":"Saves optimization results and creates visualizations. from PyEGRO.robustopt.method_pce import save_optimization_results save_optimization_results ( results : Dict , data_info : Dict , save_dir : str = 'RESULT_PARETO_FRONT_PCE' )","title":"save_optimization_results"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/","text":"PyEGRO.sensitivity.SAmcs API Reference \u00b6 This document provides detailed API documentation for the PyEGRO.sensitivity.SAmcs module, a framework for sensitivity analysis using Monte Carlo Simulation (MCS) with the Sobol method based on Saltelli's sampling approach. Table of Contents \u00b6 Overview Main Classes MCSSensitivityAnalysis SensitivityVisualization Main Function run_sensitivity_analysis Usage Notes Overview \u00b6 The PyEGRO.SAmcs module provides tools for global sensitivity analysis using Monte Carlo Simulation with the Sobol method. This approach offers insights into how different input variables affect the output of a model or function. The module focuses on calculating first-order and total-order sensitivity indices. Key features: - Implementation of Sobol method using Saltelli's sampling approach - Support for both true function evaluation and surrogate models - Automatic visualization of sensitivity indices - Comprehensive reporting of results Main Classes \u00b6 MCSSensitivityAnalysis \u00b6 Main class for performing sensitivity analysis using the Monte Carlo Simulation with Sobol method. from PyEGRO.sensitivity.SAmcs import MCSSensitivityAnalysis analyzer = MCSSensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or use model_handler output_dir = \"RESULT_SA\" , show_variables_info = True , random_seed = 42 ) results_df = analyzer . run_analysis ( num_samples = 1024 , show_progress = True ) Constructor Parameters \u00b6 data_info_path (str): Path to the data configuration file model_handler (Any, optional): Model handler for surrogate model (if using surrogate) true_func (callable, optional): True objective function (if using direct evaluation) model_path (str, optional): Path to trained GPR model (if using surrogate) output_dir (str): Directory for saving results show_variables_info (bool): Whether to display variable information random_seed (int): Random seed for reproducibility Alternative Constructor \u00b6 analyzer = MCSSensitivityAnalysis . from_initial_design ( design = your_design_object , # InitialDesign instance true_func = None , # Optional override for objective function output_dir = \"RESULT_SA\" , show_variables_info = True , random_seed = 42 ) Methods \u00b6 run_analysis(num_samples: int = 1024, show_progress: bool = True) -> pd.DataFrame \u00b6 Run the Monte Carlo-based sensitivity analysis using Sobol method. Parameters: - num_samples (int): Base sample size for Sobol sequence generation. Actual evaluations will be N*(D+2). - show_progress (bool): Whether to show progress information. Returns: - pd.DataFrame : DataFrame containing the sensitivity indices. display_variable_info() \u00b6 Display detailed information about all variables. SensitivityVisualization \u00b6 Handles visualization of sensitivity analysis results. from PyEGRO.sensitivity.SAmcs import SensitivityVisualization visualizer = SensitivityVisualization ( output_dir = \"RESULT_SA\" ) visualizer . create_visualization ( results_df ) Constructor Parameters \u00b6 output_dir (str, optional): Directory for saving visualization outputs. Methods \u00b6 create_visualization(sensitivity_df: pd.DataFrame) \u00b6 Create visualization for sensitivity indices. Parameters: - sensitivity_df (pd.DataFrame): DataFrame containing sensitivity analysis results. Main Function \u00b6 run_sensitivity_analysis \u00b6 Convenience function to run Monte Carlo-based sensitivity analysis with Sobol method. from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or model_handler num_samples = 1024 , output_dir = \"RESULT_SA\" , random_seed = 42 , show_progress = True ) Parameters \u00b6 data_info_path (str): Path to the data configuration file true_func (callable, optional): True objective function (if using direct evaluation) model_handler (Any, optional): Model handler for surrogate model model_path (str, optional): Path to trained GPR model (if using surrogate) num_samples (int): Base sample size for Sobol sequence generation output_dir (str): Directory for saving results random_seed (int): Random seed for reproducibility show_progress (bool): Whether to show progress bar Returns \u00b6 pd.DataFrame : DataFrame containing the sensitivity indices Usage Notes \u00b6 Sample Size Considerations \u00b6 The total number of model evaluations will be num_samples * (D + 2) , where: - num_samples is the base sample size - D is the number of variables This is due to the Saltelli sampling method's requirements. Outputs \u00b6 The analysis produces several outputs in the specified output directory: - sensitivity_analysis_results.csv : CSV file with all sensitivity indices - sensitivity_indices.png : Visualization of first-order and total-order indices - sensitivity_indices.fig.pkl : Pickled matplotlib figure for further customization - analysis_summary.json : Summary statistics including most influential parameters and interaction strengths Computational Considerations \u00b6 For computationally expensive models, consider: 1. Using surrogate models with the model_handler parameter 2. Starting with a smaller num_samples and gradually increasing for convergence 3. Using show_progress=True to monitor the analysis progress","title":"MCS Approach"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#pyegrosensitivitysamcs-api-reference","text":"This document provides detailed API documentation for the PyEGRO.sensitivity.SAmcs module, a framework for sensitivity analysis using Monte Carlo Simulation (MCS) with the Sobol method based on Saltelli's sampling approach.","title":"PyEGRO.sensitivity.SAmcs API Reference"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#table-of-contents","text":"Overview Main Classes MCSSensitivityAnalysis SensitivityVisualization Main Function run_sensitivity_analysis Usage Notes","title":"Table of Contents"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#overview","text":"The PyEGRO.SAmcs module provides tools for global sensitivity analysis using Monte Carlo Simulation with the Sobol method. This approach offers insights into how different input variables affect the output of a model or function. The module focuses on calculating first-order and total-order sensitivity indices. Key features: - Implementation of Sobol method using Saltelli's sampling approach - Support for both true function evaluation and surrogate models - Automatic visualization of sensitivity indices - Comprehensive reporting of results","title":"Overview"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#main-classes","text":"","title":"Main Classes"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#mcssensitivityanalysis","text":"Main class for performing sensitivity analysis using the Monte Carlo Simulation with Sobol method. from PyEGRO.sensitivity.SAmcs import MCSSensitivityAnalysis analyzer = MCSSensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or use model_handler output_dir = \"RESULT_SA\" , show_variables_info = True , random_seed = 42 ) results_df = analyzer . run_analysis ( num_samples = 1024 , show_progress = True )","title":"MCSSensitivityAnalysis"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#sensitivityvisualization","text":"Handles visualization of sensitivity analysis results. from PyEGRO.sensitivity.SAmcs import SensitivityVisualization visualizer = SensitivityVisualization ( output_dir = \"RESULT_SA\" ) visualizer . create_visualization ( results_df )","title":"SensitivityVisualization"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#main-function","text":"","title":"Main Function"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#run_sensitivity_analysis","text":"Convenience function to run Monte Carlo-based sensitivity analysis with Sobol method. from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or model_handler num_samples = 1024 , output_dir = \"RESULT_SA\" , random_seed = 42 , show_progress = True )","title":"run_sensitivity_analysis"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#usage-notes","text":"","title":"Usage Notes"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#sample-size-considerations","text":"The total number of model evaluations will be num_samples * (D + 2) , where: - num_samples is the base sample size - D is the number of variables This is due to the Saltelli sampling method's requirements.","title":"Sample Size Considerations"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#outputs","text":"The analysis produces several outputs in the specified output directory: - sensitivity_analysis_results.csv : CSV file with all sensitivity indices - sensitivity_indices.png : Visualization of first-order and total-order indices - sensitivity_indices.fig.pkl : Pickled matplotlib figure for further customization - analysis_summary.json : Summary statistics including most influential parameters and interaction strengths","title":"Outputs"},{"location":"api-reference/sensitivity/approach-mcs/samcs_api_reference/#computational-considerations","text":"For computationally expensive models, consider: 1. Using surrogate models with the model_handler parameter 2. Starting with a smaller num_samples and gradually increasing for convergence 3. Using show_progress=True to monitor the analysis progress","title":"Computational Considerations"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/","text":"PyEGRO.sensitivity.SApce API Reference \u00b6 This document provides detailed API documentation for the PyEGRO.sensitivity.SApce module, a framework for sensitivity analysis using Polynomial Chaos Expansion (PCE). Table of Contents \u00b6 Overview Main Classes PCESensitivityAnalysis SensitivityVisualization Main Function run_sensitivity_analysis Usage Notes Overview \u00b6 The PyEGRO.sensitivity.SApce module provides tools for global sensitivity analysis using Polynomial Chaos Expansion (PCE). This approach offers an efficient alternative to sampling-based methods for analyzing how different input variables affect the output of a model or function. The module focuses on calculating first-order and total-order sensitivity indices. Key features: - Implementation of sensitivity analysis using Polynomial Chaos Expansion - Support for both true function evaluation and surrogate models - Automatic visualization of sensitivity indices - Comprehensive reporting of results Main Classes \u00b6 PCESensitivityAnalysis \u00b6 Main class for performing sensitivity analysis using Polynomial Chaos Expansion. from PyEGRO.sensitivity.SApce import PCESensitivityAnalysis analyzer = PCESensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or use model_handler output_dir = \"RESULT_SA\" , show_variables_info = True , random_seed = 42 ) results_df = analyzer . run_analysis ( polynomial_order = 3 , num_samples = 1000 , show_progress = True ) Constructor Parameters \u00b6 data_info_path (str): Path to the data configuration file model_handler (Any, optional): Model handler for surrogate model (if using surrogate) true_func (callable, optional): True objective function (if using direct evaluation) model_path (str, optional): Path to trained GPR model (if using surrogate) output_dir (str): Directory for saving results show_variables_info (bool): Whether to display variable information random_seed (int): Random seed for reproducibility Alternative Constructor \u00b6 analyzer = PCESensitivityAnalysis . from_initial_design ( design = your_design_object , # InitialDesign instance true_func = None , # Optional override for objective function output_dir = \"RESULT_SA\" , show_variables_info = True , random_seed = 42 ) Methods \u00b6 run_analysis(polynomial_order: int = 3, num_samples: int = 1000, show_progress: bool = True) -> pd.DataFrame \u00b6 Run the PCE-based sensitivity analysis. Parameters: - polynomial_order (int): Order of the polynomial expansion - num_samples (int): Number of samples for PCE fitting - show_progress (bool): Whether to show progress information Returns: - pd.DataFrame : DataFrame containing the sensitivity indices display_variable_info() \u00b6 Display detailed information about all variables. SensitivityVisualization \u00b6 Handles visualization of sensitivity analysis results. from PyEGRO.sensitivity.SApce import SensitivityVisualization visualizer = SensitivityVisualization ( output_dir = \"RESULT_SA\" ) visualizer . create_visualization ( results_df ) Constructor Parameters \u00b6 output_dir (str, optional): Directory for saving visualization outputs. Methods \u00b6 create_visualization(sensitivity_df: pd.DataFrame) \u00b6 Create visualization for sensitivity indices. Parameters: - sensitivity_df (pd.DataFrame): DataFrame containing sensitivity analysis results. Main Function \u00b6 run_sensitivity_analysis \u00b6 Convenience function to run PCE-based sensitivity analysis. from PyEGRO.sensitivity.SApce import run_sensitivity_analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or model_handler polynomial_order = 3 , num_samples = 1000 , output_dir = \"RESULT_SA\" , random_seed = 42 , show_progress = True ) Parameters \u00b6 data_info_path (str): Path to the data configuration file true_func (callable, optional): True objective function (if using direct evaluation) model_handler (Any, optional): Model handler for surrogate model model_path (str, optional): Path to trained GPR model (if using surrogate) polynomial_order (int): Order of the polynomial expansion num_samples (int): Number of samples for PCE fitting output_dir (str): Directory for saving results random_seed (int): Random seed for reproducibility show_progress (bool): Whether to show progress bar Returns \u00b6 pd.DataFrame : DataFrame containing the sensitivity indices Usage Notes \u00b6 Polynomial Order Selection \u00b6 The polynomial order controls the complexity of the PCE model: - Higher order captures more complex variable interactions but requires more samples - Lower order is more efficient but may miss higher-order interactions - Typical values range from 2-5 depending on problem complexity Sample Size Considerations \u00b6 PCE generally requires fewer samples than Monte Carlo methods, but the required number depends on: - The polynomial order (higher order requires more samples) - The number of variables (more variables require more samples) - A general rule of thumb: use at least 10 \u00d7 P samples, where P is the number of PCE terms Outputs \u00b6 The analysis produces several outputs in the specified output directory: - sensitivity_analysis_results.csv : CSV file with all sensitivity indices - sensitivity_indices.png : Visualization of first-order and total-order indices - sensitivity_indices.fig.pkl : Pickled matplotlib figure for further customization - analysis_summary.json : Summary statistics including most influential parameters and interaction strengths Computational Efficiency \u00b6 PCE is generally more computationally efficient than traditional Monte Carlo methods: - Requires fewer function evaluations for comparable accuracy - Provides a smooth surrogate model that can be used for additional analyses - Most suitable for smooth response surfaces with moderate dimensionality","title":"PCE Approach"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#pyegrosensitivitysapce-api-reference","text":"This document provides detailed API documentation for the PyEGRO.sensitivity.SApce module, a framework for sensitivity analysis using Polynomial Chaos Expansion (PCE).","title":"PyEGRO.sensitivity.SApce API Reference"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#table-of-contents","text":"Overview Main Classes PCESensitivityAnalysis SensitivityVisualization Main Function run_sensitivity_analysis Usage Notes","title":"Table of Contents"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#overview","text":"The PyEGRO.sensitivity.SApce module provides tools for global sensitivity analysis using Polynomial Chaos Expansion (PCE). This approach offers an efficient alternative to sampling-based methods for analyzing how different input variables affect the output of a model or function. The module focuses on calculating first-order and total-order sensitivity indices. Key features: - Implementation of sensitivity analysis using Polynomial Chaos Expansion - Support for both true function evaluation and surrogate models - Automatic visualization of sensitivity indices - Comprehensive reporting of results","title":"Overview"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#main-classes","text":"","title":"Main Classes"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#pcesensitivityanalysis","text":"Main class for performing sensitivity analysis using Polynomial Chaos Expansion. from PyEGRO.sensitivity.SApce import PCESensitivityAnalysis analyzer = PCESensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or use model_handler output_dir = \"RESULT_SA\" , show_variables_info = True , random_seed = 42 ) results_df = analyzer . run_analysis ( polynomial_order = 3 , num_samples = 1000 , show_progress = True )","title":"PCESensitivityAnalysis"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#sensitivityvisualization","text":"Handles visualization of sensitivity analysis results. from PyEGRO.sensitivity.SApce import SensitivityVisualization visualizer = SensitivityVisualization ( output_dir = \"RESULT_SA\" ) visualizer . create_visualization ( results_df )","title":"SensitivityVisualization"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#main-function","text":"","title":"Main Function"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#run_sensitivity_analysis","text":"Convenience function to run PCE-based sensitivity analysis. from PyEGRO.sensitivity.SApce import run_sensitivity_analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = your_objective_function , # or model_handler polynomial_order = 3 , num_samples = 1000 , output_dir = \"RESULT_SA\" , random_seed = 42 , show_progress = True )","title":"run_sensitivity_analysis"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#usage-notes","text":"","title":"Usage Notes"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#polynomial-order-selection","text":"The polynomial order controls the complexity of the PCE model: - Higher order captures more complex variable interactions but requires more samples - Lower order is more efficient but may miss higher-order interactions - Typical values range from 2-5 depending on problem complexity","title":"Polynomial Order Selection"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#sample-size-considerations","text":"PCE generally requires fewer samples than Monte Carlo methods, but the required number depends on: - The polynomial order (higher order requires more samples) - The number of variables (more variables require more samples) - A general rule of thumb: use at least 10 \u00d7 P samples, where P is the number of PCE terms","title":"Sample Size Considerations"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#outputs","text":"The analysis produces several outputs in the specified output directory: - sensitivity_analysis_results.csv : CSV file with all sensitivity indices - sensitivity_indices.png : Visualization of first-order and total-order indices - sensitivity_indices.fig.pkl : Pickled matplotlib figure for further customization - analysis_summary.json : Summary statistics including most influential parameters and interaction strengths","title":"Outputs"},{"location":"api-reference/sensitivity/approach-pce/sapce_api_reference/#computational-efficiency","text":"PCE is generally more computationally efficient than traditional Monte Carlo methods: - Requires fewer function evaluations for comparable accuracy - Provides a smooth surrogate model that can be used for additional analyses - Most suitable for smooth response surfaces with moderate dimensionality","title":"Computational Efficiency"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/","text":"PyEGRO UQmcs API Reference \u00b6 This document provides detailed information about the API for PyEGRO's Uncertainty Quantification Monte Carlo Simulation (UQmcs) module. Table of Contents \u00b6 UncertaintyPropagation Class Constructor Parameters Methods Convenience Functions run_uncertainty_analysis analyze_specific_point InformationVisualization Class Constructor Parameters Methods Data Configuration Format Design Variables Environmental Variables Example Configuration UncertaintyPropagation Class \u00b6 The main class for uncertainty propagation analysis in PyEGRO. Constructor Parameters \u00b6 UncertaintyPropagation ( data_info_path : str = \"DATA_PREPARATION/data_info.json\" , model_handler : Optional [ Any ] = None , true_func : Optional [ callable ] = None , model_path : Optional [ str ] = None , use_gpu : bool = True , output_dir : str = \"RESULT_QOI\" , show_variables_info : bool = True , random_seed : int = 42 ) Parameter Type Description Default data_info_path str Path to the data configuration JSON file \"DATA_PREPARATION/data_info.json\" model_handler Optional[Any] Model handler for surrogate model None true_func Optional[callable] True objective function for direct evaluation None model_path Optional[str] Path to trained GPR model (if using surrogate) None use_gpu bool Whether to use GPU if available for surrogate model True output_dir str Directory for saving results \"RESULT_QOI\" show_variables_info bool Whether to display variable information True random_seed int Random seed for reproducibility 42 Note : At least one of true_func , model_handler , or model_path must be provided. Methods \u00b6 from_initial_design (class method) \u00b6 Creates a UncertaintyPropagation instance directly from an InitialDesign object. @classmethod def from_initial_design ( cls , design = None , design_infos = None , true_func = None , output_dir = \"RESULT_QOI\" , use_gpu = True , show_variables_info = True , random_seed = 42 ) Parameter Type Description Default design InitialDesign InitialDesign instance None design_infos InitialDesign Alternative name for design (for backward compatibility) None true_func Optional[callable] Optional override for objective function None output_dir str Directory for saving results \"RESULT_QOI\" use_gpu bool Whether to use GPU if available True show_variables_info bool Whether to display variable information True random_seed int Random seed for reproducibility 42 display_variable_info \u00b6 Displays detailed information about all variables. def display_variable_info ( self ) run_analysis \u00b6 Runs the uncertainty propagation analysis. def run_analysis ( self , true_func : Optional [ callable ] = None , num_design_samples : int = 1000 , num_mcs_samples : int = 100000 , show_progress : bool = True ) -> pd . DataFrame Parameter Type Description Default true_func Optional[callable] Optional override for the true function None num_design_samples int Number of design points to evaluate 1000 num_mcs_samples int Number of Monte Carlo samples per design point 100000 show_progress bool Whether to show progress bar True Returns : pandas.DataFrame containing the analysis results. analyze_specific_point \u00b6 Analyzes uncertainty at a specific design point with customized environmental variables. def analyze_specific_point ( self , design_point : Dict [ str , float ], env_vars_override : Optional [ Dict [ str , Dict [ str , float ]]] = None , num_mcs_samples : int = 100000 , create_pdf : bool = True , create_reliability : bool = True ) -> Dict [ str , Any ] Parameter Type Description Default design_point Dict[str, float] Dictionary of design variable values Required env_vars_override Optional[Dict[str, Dict[str, float]]] Dictionary to override env variable distributions None num_mcs_samples int Number of Monte Carlo samples 100000 create_pdf bool Whether to create PDF visualization True create_reliability bool Whether to create reliability plot True Returns : Dictionary containing analysis results. Convenience Functions \u00b6 run_uncertainty_analysis \u00b6 Convenience function to run uncertainty propagation analysis. def run_uncertainty_analysis ( data_info_path : str = \"DATA_PREPARATION/data_info.json\" , true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , model_path : Optional [ str ] = None , num_design_samples : int = 1000 , num_mcs_samples : int = 100000 , use_gpu : bool = True , output_dir : str = \"RESULT_QOI\" , random_seed : int = 42 , show_progress : bool = True ) -> pd . DataFrame Parameter Type Description Default data_info_path str Path to the data configuration file \"DATA_PREPARATION/data_info.json\" true_func Optional[callable] True objective function (if using direct evaluation) None model_handler Optional[Any] Model handler for surrogate model None model_path Optional[str] Path to trained GPR model (if using surrogate) None num_design_samples int Number of design points to evaluate 1000 num_mcs_samples int Number of Monte Carlo samples per design point 100000 use_gpu bool Whether to use GPU if available for surrogate model True output_dir str Directory for saving results \"RESULT_QOI\" random_seed int Random seed for reproducibility 42 show_progress bool Whether to show progress bar True Returns : pandas.DataFrame containing the analysis results. analyze_specific_point \u00b6 Convenience function to analyze uncertainty at a specific design point. def analyze_specific_point ( data_info_path : str , design_point : Dict [ str , float ], true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , env_vars_override : Optional [ Dict [ str , Dict [ str , float ]]] = None , num_mcs_samples : int = 100000 , output_dir : str = \"RESULT_QOI_POINT\" , random_seed : int = 42 , create_pdf : bool = True , create_reliability : bool = True ) -> Dict [ str , Any ] Parameter Type Description Default data_info_path str Path to the data configuration file Required design_point Dict[str, float] Dictionary of design variable values Required true_func Optional[callable] True objective function (if using direct evaluation) None model_handler Optional[Any] Model handler for surrogate model None env_vars_override Optional[Dict[str, Dict[str, float]]] Optional dictionary to override env variable distributions None num_mcs_samples int Number of Monte Carlo samples 100000 output_dir str Directory for saving results \"RESULT_QOI_POINT\" random_seed int Random seed for reproducibility 42 create_pdf bool Whether to create PDF visualization True create_reliability bool Whether to create reliability plot True Returns : Dictionary containing analysis results. InformationVisualization Class \u00b6 Handles visualization of uncertainty analysis results. Constructor Parameters \u00b6 InformationVisualization ( output_dir = None ) Parameter Type Description Default output_dir str or Path Directory for saving visualizations None (defaults to 'RESULT_QOI') Methods \u00b6 create_visualization \u00b6 Creates appropriate visualization based on number of design variables. def create_visualization ( self , results_df : pd . DataFrame , design_vars : List [ Dict [ str , Any ]]) Parameter Type Description results_df pd.DataFrame DataFrame containing the analysis results design_vars List[Dict[str, Any]] List of design variable dictionaries create_pdf_visualization \u00b6 Creates PDF visualization for a specific design point. def create_pdf_visualization ( self , design_point : Dict [ str , Any ], samples : np . ndarray , title : str = \"Probability Density Function at Design Point\" , filename : str = \"pdf_visualization.png\" ) -> plt . Figure Parameter Type Description Default design_point Dict[str, Any] Dictionary of design variable values Required samples np.ndarray Array of samples from Monte Carlo simulation Required title str Title for the plot \"Probability Density Function at Design Point\" filename str Filename for saving the plot \"pdf_visualization.png\" Returns : Matplotlib figure object. create_reliability_plot \u00b6 Creates reliability plot showing probability of exceedance vs threshold value. def create_reliability_plot ( self , samples : np . ndarray , threshold_values : np . ndarray , threshold_type : str = 'upper' , title : str = \"Reliability Analysis\" , filename : str = \"reliability_analysis.png\" ) -> plt . Figure Parameter Type Description Default samples np.ndarray Array of samples from Monte Carlo simulation Required threshold_values np.ndarray Array of threshold values to evaluate Required threshold_type str 'upper' for P(X > threshold) or 'lower' for P(X < threshold) 'upper' title str Title for the plot \"Reliability Analysis\" filename str Filename for saving the plot \"reliability_analysis.png\" Returns : Matplotlib figure object. create_joint_pdf_visualization \u00b6 Creates a joint PDF visualization showing the relationship between an input variable and the output response. def create_joint_pdf_visualization ( self , design_point : Dict [ str , Any ], samples : np . ndarray , target_variable : str , input_variable : str , title : str = \"Joint Probability Distribution\" , filename : str = \"joint_pdf_visualization.png\" ) -> plt . Figure Parameter Type Description Default design_point Dict[str, Any] Dictionary of design variable values Required samples np.ndarray 2D array of samples - first column is input variable, second is output Required target_variable str Name of the target/output variable Required input_variable str Name of the input variable to analyze Required title str Title for the plot \"Joint Probability Distribution\" filename str Filename for saving the plot \"joint_pdf_visualization.png\" Returns : Matplotlib figure object. Data Configuration Format \u00b6 The data configuration is stored in a JSON file with specific structure for design and environmental variables. Design Variables \u00b6 Design variables are controllable inputs with the following attributes: Attribute Type Description Required name str Variable name Yes vars_type str Must be 'design_vars' Yes range_bounds [float, float] Lower and upper bounds for the variable Yes cov float Coefficient of Variation (uncertainty) No (default = 0, deterministic) distribution str Probability distribution (e.g., 'normal') No (required if cov > 0) description str Description of the variable No Environmental Variables \u00b6 Environmental variables are uncontrollable inputs or noise factors: Attribute Type Description Required name str Variable name Yes vars_type str Must be 'env_vars' Yes distribution str Probability distribution (e.g., 'normal', 'uniform') Yes mean float Mean value (for 'normal' distribution) Yes (for 'normal') std float Standard deviation (for 'normal' distribution) Yes (if 'cov' not specified) cov float Coefficient of variation (for 'normal' distribution) Yes (if 'std' not specified) low float Lower bound (for 'uniform' distribution) Yes (for 'uniform') high float Upper bound (for 'uniform' distribution) Yes (for 'uniform') description str Description of the variable No Example Configuration \u00b6 { \"variables\" : [ { \"name\" : \"x1\" , \"vars_type\" : \"design_vars\" , \"range_bounds\" : [ 0 , 10 ], \"cov\" : 0.05 , \"distribution\" : \"normal\" , \"description\" : \"First design variable\" }, { \"name\" : \"x2\" , \"vars_type\" : \"design_vars\" , \"range_bounds\" : [ -5 , 5 ], \"cov\" : 0 , \"description\" : \"Deterministic design variable\" }, { \"name\" : \"noise1\" , \"vars_type\" : \"env_vars\" , \"distribution\" : \"normal\" , \"mean\" : 0 , \"std\" : 0.1 , \"description\" : \"Measurement noise\" }, { \"name\" : \"temp\" , \"vars_type\" : \"env_vars\" , \"distribution\" : \"uniform\" , \"low\" : 20 , \"high\" : 30 , \"description\" : \"Operating temperature\" } ] }","title":"MCS Approach"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#pyegro-uqmcs-api-reference","text":"This document provides detailed information about the API for PyEGRO's Uncertainty Quantification Monte Carlo Simulation (UQmcs) module.","title":"PyEGRO UQmcs API Reference"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#table-of-contents","text":"UncertaintyPropagation Class Constructor Parameters Methods Convenience Functions run_uncertainty_analysis analyze_specific_point InformationVisualization Class Constructor Parameters Methods Data Configuration Format Design Variables Environmental Variables Example Configuration","title":"Table of Contents"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#uncertaintypropagation-class","text":"The main class for uncertainty propagation analysis in PyEGRO.","title":"UncertaintyPropagation Class"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#constructor-parameters","text":"UncertaintyPropagation ( data_info_path : str = \"DATA_PREPARATION/data_info.json\" , model_handler : Optional [ Any ] = None , true_func : Optional [ callable ] = None , model_path : Optional [ str ] = None , use_gpu : bool = True , output_dir : str = \"RESULT_QOI\" , show_variables_info : bool = True , random_seed : int = 42 ) Parameter Type Description Default data_info_path str Path to the data configuration JSON file \"DATA_PREPARATION/data_info.json\" model_handler Optional[Any] Model handler for surrogate model None true_func Optional[callable] True objective function for direct evaluation None model_path Optional[str] Path to trained GPR model (if using surrogate) None use_gpu bool Whether to use GPU if available for surrogate model True output_dir str Directory for saving results \"RESULT_QOI\" show_variables_info bool Whether to display variable information True random_seed int Random seed for reproducibility 42 Note : At least one of true_func , model_handler , or model_path must be provided.","title":"Constructor Parameters"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#methods","text":"","title":"Methods"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#convenience-functions","text":"","title":"Convenience Functions"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#run_uncertainty_analysis","text":"Convenience function to run uncertainty propagation analysis. def run_uncertainty_analysis ( data_info_path : str = \"DATA_PREPARATION/data_info.json\" , true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , model_path : Optional [ str ] = None , num_design_samples : int = 1000 , num_mcs_samples : int = 100000 , use_gpu : bool = True , output_dir : str = \"RESULT_QOI\" , random_seed : int = 42 , show_progress : bool = True ) -> pd . DataFrame Parameter Type Description Default data_info_path str Path to the data configuration file \"DATA_PREPARATION/data_info.json\" true_func Optional[callable] True objective function (if using direct evaluation) None model_handler Optional[Any] Model handler for surrogate model None model_path Optional[str] Path to trained GPR model (if using surrogate) None num_design_samples int Number of design points to evaluate 1000 num_mcs_samples int Number of Monte Carlo samples per design point 100000 use_gpu bool Whether to use GPU if available for surrogate model True output_dir str Directory for saving results \"RESULT_QOI\" random_seed int Random seed for reproducibility 42 show_progress bool Whether to show progress bar True Returns : pandas.DataFrame containing the analysis results.","title":"run_uncertainty_analysis"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#analyze_specific_point_1","text":"Convenience function to analyze uncertainty at a specific design point. def analyze_specific_point ( data_info_path : str , design_point : Dict [ str , float ], true_func : Optional [ callable ] = None , model_handler : Optional [ Any ] = None , env_vars_override : Optional [ Dict [ str , Dict [ str , float ]]] = None , num_mcs_samples : int = 100000 , output_dir : str = \"RESULT_QOI_POINT\" , random_seed : int = 42 , create_pdf : bool = True , create_reliability : bool = True ) -> Dict [ str , Any ] Parameter Type Description Default data_info_path str Path to the data configuration file Required design_point Dict[str, float] Dictionary of design variable values Required true_func Optional[callable] True objective function (if using direct evaluation) None model_handler Optional[Any] Model handler for surrogate model None env_vars_override Optional[Dict[str, Dict[str, float]]] Optional dictionary to override env variable distributions None num_mcs_samples int Number of Monte Carlo samples 100000 output_dir str Directory for saving results \"RESULT_QOI_POINT\" random_seed int Random seed for reproducibility 42 create_pdf bool Whether to create PDF visualization True create_reliability bool Whether to create reliability plot True Returns : Dictionary containing analysis results.","title":"analyze_specific_point"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#informationvisualization-class","text":"Handles visualization of uncertainty analysis results.","title":"InformationVisualization Class"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#constructor-parameters_1","text":"InformationVisualization ( output_dir = None ) Parameter Type Description Default output_dir str or Path Directory for saving visualizations None (defaults to 'RESULT_QOI')","title":"Constructor Parameters"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#methods_1","text":"","title":"Methods"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#data-configuration-format","text":"The data configuration is stored in a JSON file with specific structure for design and environmental variables.","title":"Data Configuration Format"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#design-variables","text":"Design variables are controllable inputs with the following attributes: Attribute Type Description Required name str Variable name Yes vars_type str Must be 'design_vars' Yes range_bounds [float, float] Lower and upper bounds for the variable Yes cov float Coefficient of Variation (uncertainty) No (default = 0, deterministic) distribution str Probability distribution (e.g., 'normal') No (required if cov > 0) description str Description of the variable No","title":"Design Variables"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#environmental-variables","text":"Environmental variables are uncontrollable inputs or noise factors: Attribute Type Description Required name str Variable name Yes vars_type str Must be 'env_vars' Yes distribution str Probability distribution (e.g., 'normal', 'uniform') Yes mean float Mean value (for 'normal' distribution) Yes (for 'normal') std float Standard deviation (for 'normal' distribution) Yes (if 'cov' not specified) cov float Coefficient of variation (for 'normal' distribution) Yes (if 'std' not specified) low float Lower bound (for 'uniform' distribution) Yes (for 'uniform') high float Upper bound (for 'uniform' distribution) Yes (for 'uniform') description str Description of the variable No","title":"Environmental Variables"},{"location":"api-reference/uncertainty/approach-mcs/uqmcs_api_reference/#example-configuration","text":"{ \"variables\" : [ { \"name\" : \"x1\" , \"vars_type\" : \"design_vars\" , \"range_bounds\" : [ 0 , 10 ], \"cov\" : 0.05 , \"distribution\" : \"normal\" , \"description\" : \"First design variable\" }, { \"name\" : \"x2\" , \"vars_type\" : \"design_vars\" , \"range_bounds\" : [ -5 , 5 ], \"cov\" : 0 , \"description\" : \"Deterministic design variable\" }, { \"name\" : \"noise1\" , \"vars_type\" : \"env_vars\" , \"distribution\" : \"normal\" , \"mean\" : 0 , \"std\" : 0.1 , \"description\" : \"Measurement noise\" }, { \"name\" : \"temp\" , \"vars_type\" : \"env_vars\" , \"distribution\" : \"uniform\" , \"low\" : 20 , \"high\" : 30 , \"description\" : \"Operating temperature\" } ] }","title":"Example Configuration"},{"location":"basic-usage/doe/doe_examples/","text":"PyEGRO.doe Module - Usage Examples \u00b6 This document provides comprehensive examples of how to use the PyEGRO Design of Experiments (DOE) module for various scenarios. These examples illustrate different sampling methods, variable types, uncertainty specifications, and objective functions. Table of Contents \u00b6 Basic Usage Uncertainty Specification Methods 1D Test Function Examples 2D Test Function Examples Comparing Sampling Methods Environmental Variables Advanced Usage Patterns Basic Usage \u00b6 Here's a simple example to get started with the DOE module: import numpy as np from PyEGRO.doe import InitialDesign # Define a simple objective function def simple_objective ( x ): return np . sum ( x ** 2 , axis = 1 ) # Create an initial design design = InitialDesign ( output_dir = 'simple_doe' , sampling_method = 'lhs' ) # Add two design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) # Save variable data to data_info.json (useful when using metamodel in optimization loop) design . save ( 'data_info' ) # Run sampling with 20 samples results = design . run ( objective_function = simple_objective , num_samples = 20 ) # Display the first few results print ( results . head ()) Uncertainty Specification Methods \u00b6 There are multiple ways to specify uncertainty for design variables. Here are examples for each method: Using Coefficient of Variation (CoV) \u00b6 import numpy as np from PyEGRO.doe import InitialDesign # Define a simple objective function def objective_func ( x ): return np . sum ( x ** 2 , axis = 1 ) # Create design with CoV for uncertainty design_cov = InitialDesign ( output_dir = 'doe_cov_example' , sampling_method = 'lhs' ) # Add design variable with CoV design_cov . add_design_variable ( name = 'x' , range_bounds = [ - 5 , 5 ], cov = 0.1 , # 10% coefficient of variation distribution = 'normal' , description = 'design variable with CoV-based uncertainty' ) # Run sampling results_cov = design_cov . run ( objective_function = objective_func , num_samples = 50 ) print ( f \"CoV example - Results range: [ { min ( results_cov [ 'y' ]) : .2f } , { max ( results_cov [ 'y' ]) : .2f } ]\" ) Using Standard Deviation (Std) \u00b6 # Create design with standard deviation for uncertainty design_std = InitialDesign ( output_dir = 'doe_std_example' , sampling_method = 'lhs' ) # Add design variable with standard deviation design_std . add_design_variable ( name = 'x' , range_bounds = [ - 5 , 5 ], std = 0.2 , # Fixed standard deviation of 0.2 distribution = 'normal' , description = 'design variable with std-based uncertainty' ) # Run sampling results_std = design_std . run ( objective_function = objective_func , num_samples = 50 ) print ( f \"Std example - Results range: [ { min ( results_std [ 'y' ]) : .2f } , { max ( results_std [ 'y' ]) : .2f } ]\" ) Using Delta for Uniform Uncertainty \u00b6 # Create design with delta for uniform uncertainty design_delta = InitialDesign ( output_dir = 'doe_delta_example' , sampling_method = 'lhs' ) # Add design variable with uniform uncertainty design_delta . add_design_variable ( name = 'x' , range_bounds = [ - 5 , 5 ], delta = 0.1 , # Half-width of 0.1 around design points distribution = 'uniform' , description = 'design variable with uniform uncertainty' ) # Run sampling results_delta = design_delta . run ( objective_function = objective_func , num_samples = 50 ) print ( f \"Delta example - Results range: [ { min ( results_delta [ 'y' ]) : .2f } , { max ( results_delta [ 'y' ]) : .2f } ]\" ) Handling Near-Zero Regions \u00b6 import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt # Define an objective function that crosses zero def sinusoidal_func ( x ): return np . sin ( x [:, 0 ]) # Create designs with different uncertainty specifications design_cov = InitialDesign ( output_dir = 'doe_near_zero_cov' , sampling_method = 'lhs' ) design_std = InitialDesign ( output_dir = 'doe_near_zero_std' , sampling_method = 'lhs' ) # Add design variables that will include zero in their range design_cov . add_design_variable ( name = 'x' , range_bounds = [ - 3.14 , 3.14 ], cov = 0.1 , # CoV will have issues near zero distribution = 'normal' ) design_std . add_design_variable ( name = 'x' , range_bounds = [ - 3.14 , 3.14 ], std = 0.2 , # Std works well even at zero distribution = 'normal' ) # Run sampling results_cov = design_cov . run ( objective_function = sinusoidal_func , num_samples = 200 ) results_std = design_std . run ( objective_function = sinusoidal_func , num_samples = 200 ) # Plot the results plt . figure ( figsize = ( 12 , 6 )) plt . subplot ( 1 , 2 , 1 ) plt . scatter ( results_cov [ 'x' ], results_cov [ 'y' ], alpha = 0.7 ) plt . title ( 'Using CoV for Near-Zero Regions' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) plt . grid ( True ) plt . subplot ( 1 , 2 , 2 ) plt . scatter ( results_std [ 'x' ], results_std [ 'y' ], alpha = 0.7 ) plt . title ( 'Using Std for Near-Zero Regions' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) plt . grid ( True ) plt . tight_layout () plt . show () 1D Test Function Examples \u00b6 Gaussian Mixture Function \u00b6 This example demonstrates how to use the DOE module with a 1D Gaussian mixture function. import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt # Define 1D Gaussian Mixture test function def objective_func_1D ( x ): \"\"\"1D test function with multiple Gaussian peaks\"\"\" if isinstance ( x , np . ndarray ) and x . ndim > 1 : x = x [:, 0 ] # Extract first column if it's a 2D array term1 = 100 * np . exp ( - (( x - 10 ) ** 2 ) / 0.8 ) term2 = 80 * np . exp ( - (( x - 20 ) ** 2 ) / 50 ) term3 = 20 * np . exp ( - (( x - 30 ) ** 2 ) / 18 ) return - ( term1 + term2 + term3 - 200 ) # Create design with LHS sampling design_1d = InitialDesign ( output_dir = 'doe_1d_example' , sampling_method = 'lhs' , sampling_criterion = 'maximin' ) # Add design variable using standard deviation instead of cov design_1d . add_design_variable ( name = 'x' , range_bounds = [ 0 , 40 ], std = 0.5 , # Fixed standard deviation description = 'design parameter with std uncertainty' ) # Run sampling results_1d = design_1d . run ( objective_function = objective_func_1D , num_samples = 50 ) # Plot results plt . figure ( figsize = ( 10 , 6 )) plt . scatter ( results_1d [ 'x' ], results_1d [ 'y' ], color = 'red' , alpha = 0.7 , label = 'Sampled Points' ) # Generate the true function x_true = np . linspace ( 0 , 40 , 1000 ) y_true = objective_func_1D ( x_true ) plt . plot ( x_true , y_true , 'k-' , label = 'True Function' ) plt . title ( '1D Gaussian Mixture Function' ) plt . xlabel ( 'x' ) plt . ylabel ( 'f(x)' ) plt . legend () plt . grid ( True ) plt . show () 2D Test Function Examples \u00b6 Gaussian Peaks Function with Delta Uncertainty \u00b6 This example shows how to sample from a 2D function with two Gaussian peaks using uniform uncertainty. import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt # Define 2D test function with two Gaussian peaks def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] # Parameters for two Gaussian peaks a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 # Lower and more sensitive a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 # Higher and more robust # Calculate negative sum of two Gaussian peaks f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f # Create design with Sobol sequence and uniform uncertainty design_2d = InitialDesign ( output_dir = 'doe_2d_example_delta' , sampling_method = 'sobol' ) # Add design variables with delta for uniform uncertainty design_2d . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], delta = 0.05 , # Half-width of 0.05 around design points distribution = 'uniform' , description = 'first design variable with uniform uncertainty' ) design_2d . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], delta = 0.05 , # Half-width of 0.05 around design points distribution = 'uniform' , description = 'second design variable with uniform uncertainty' ) # Run sampling results_2d = design_2d . run ( objective_function = objective_func_2D , num_samples = 50 ) # Create a contour plot of the design space plt . figure ( figsize = ( 10 , 8 )) # Generate grid for visualization x1_grid = np . linspace ( - 5 , 5 , 100 ) x2_grid = np . linspace ( - 6 , 6 , 100 ) X1 , X2 = np . meshgrid ( x1_grid , x2_grid ) grid_points = np . vstack ([ X1 . ravel (), X2 . ravel ()]) . T Z = objective_func_2D ( grid_points ) . reshape ( X1 . shape ) # Create contour plot plt . contourf ( X1 , X2 , Z , 50 , cmap = 'viridis' ) plt . colorbar ( label = 'Objective Value' ) plt . scatter ( results_2d [ 'x1' ], results_2d [ 'x2' ], c = 'red' , s = 50 , alpha = 0.7 , edgecolors = 'white' ) plt . title ( '2D Gaussian Peaks Function with Sobol Sampling (Uniform Uncertainty)' ) plt . xlabel ( 'x1' ) plt . ylabel ( 'x2' ) plt . grid ( True ) plt . show () Comparing Sampling Methods \u00b6 This example compares different sampling methods for a 2D design space. import numpy as np from PyEGRO.doe import InitialDesign from PyEGRO.doe.sampling import AdaptiveDistributionSampler import matplotlib.pyplot as plt # Compare different sampling methods sampling_methods = [ 'lhs' , 'sobol' , 'halton' , 'random' ] num_samples = 50 # Create figure plt . figure ( figsize = ( 16 , 12 )) for i , method in enumerate ( sampling_methods ): # Create design design = InitialDesign ( output_dir = f 'doe_compare_ { method } ' , sampling_method = method , sampling_criterion = 'maximin' if method == 'lhs' else None ) # Add design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], std = 0.2 # Using std instead of cov ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 5 , 5 ], std = 0.2 ) # Generate samples (without evaluating objective function) sampler = AdaptiveDistributionSampler () samples = sampler . generate_all_samples ( design . variables , num_samples , method = method , criterion = 'maximin' if method == 'lhs' else None ) # Plot plt . subplot ( 2 , 2 , i + 1 ) plt . scatter ( samples [:, 0 ], samples [:, 1 ], alpha = 0.7 ) plt . title ( f \" { method . upper () } Sampling ( { num_samples } points)\" ) plt . xlabel ( 'x1' ) plt . ylabel ( 'x2' ) plt . xlim ( - 5 , 5 ) plt . ylim ( - 5 , 5 ) plt . grid ( True ) plt . tight_layout () plt . show () Environmental Variables \u00b6 This example demonstrates how to incorporate environmental variables in your experiments with both min/max and std notation. ```python import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt Define objective function with both design and environmental variables \u00b6 def objective_with_env_vars(x): # Extract design and environmental variables design_var = x[:, 0] # Design variable env_normal = x[:, 1] # Normal distributed environmental variable env_uniform = x[:, 2] # Uniform distributed environmental variable # Base function (using design variable) base_result = -design_var**2 # Apply environmental effects # Normal env var: additive effect # Uniform env var: multiplicative effect result = (base_result + 5*env_normal) * (1 + env_uniform) return result Create design \u00b6 design_with_env = InitialDesign( output_dir='doe_with_env_vars_new', sampling_method='lhs' ) Add design variable using std for uncertainty \u00b6 design_with_env.add_design_variable( name='x', range_bounds=[-3, 3], std=0.2, # Standard deviation (instead of cov) description='design parameter with std-based uncertainty' ) Add environmental variables with different distributions and notation \u00b6 design_with_env.add_env_variable( name='noise_additive', distribution='normal', mean=0.0, std=0.5, # Using std instead of cov description='additive noise (normal) with std' ) design_with_env.add_env_variable( name","title":"Design of Experiment"},{"location":"basic-usage/doe/doe_examples/#pyegrodoe-module-usage-examples","text":"This document provides comprehensive examples of how to use the PyEGRO Design of Experiments (DOE) module for various scenarios. These examples illustrate different sampling methods, variable types, uncertainty specifications, and objective functions.","title":"PyEGRO.doe Module - Usage Examples"},{"location":"basic-usage/doe/doe_examples/#table-of-contents","text":"Basic Usage Uncertainty Specification Methods 1D Test Function Examples 2D Test Function Examples Comparing Sampling Methods Environmental Variables Advanced Usage Patterns","title":"Table of Contents"},{"location":"basic-usage/doe/doe_examples/#basic-usage","text":"Here's a simple example to get started with the DOE module: import numpy as np from PyEGRO.doe import InitialDesign # Define a simple objective function def simple_objective ( x ): return np . sum ( x ** 2 , axis = 1 ) # Create an initial design design = InitialDesign ( output_dir = 'simple_doe' , sampling_method = 'lhs' ) # Add two design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 5 , 5 ], cov = 0.1 ) # Save variable data to data_info.json (useful when using metamodel in optimization loop) design . save ( 'data_info' ) # Run sampling with 20 samples results = design . run ( objective_function = simple_objective , num_samples = 20 ) # Display the first few results print ( results . head ())","title":"Basic Usage"},{"location":"basic-usage/doe/doe_examples/#uncertainty-specification-methods","text":"There are multiple ways to specify uncertainty for design variables. Here are examples for each method:","title":"Uncertainty Specification Methods"},{"location":"basic-usage/doe/doe_examples/#using-coefficient-of-variation-cov","text":"import numpy as np from PyEGRO.doe import InitialDesign # Define a simple objective function def objective_func ( x ): return np . sum ( x ** 2 , axis = 1 ) # Create design with CoV for uncertainty design_cov = InitialDesign ( output_dir = 'doe_cov_example' , sampling_method = 'lhs' ) # Add design variable with CoV design_cov . add_design_variable ( name = 'x' , range_bounds = [ - 5 , 5 ], cov = 0.1 , # 10% coefficient of variation distribution = 'normal' , description = 'design variable with CoV-based uncertainty' ) # Run sampling results_cov = design_cov . run ( objective_function = objective_func , num_samples = 50 ) print ( f \"CoV example - Results range: [ { min ( results_cov [ 'y' ]) : .2f } , { max ( results_cov [ 'y' ]) : .2f } ]\" )","title":"Using Coefficient of Variation (CoV)"},{"location":"basic-usage/doe/doe_examples/#using-standard-deviation-std","text":"# Create design with standard deviation for uncertainty design_std = InitialDesign ( output_dir = 'doe_std_example' , sampling_method = 'lhs' ) # Add design variable with standard deviation design_std . add_design_variable ( name = 'x' , range_bounds = [ - 5 , 5 ], std = 0.2 , # Fixed standard deviation of 0.2 distribution = 'normal' , description = 'design variable with std-based uncertainty' ) # Run sampling results_std = design_std . run ( objective_function = objective_func , num_samples = 50 ) print ( f \"Std example - Results range: [ { min ( results_std [ 'y' ]) : .2f } , { max ( results_std [ 'y' ]) : .2f } ]\" )","title":"Using Standard Deviation (Std)"},{"location":"basic-usage/doe/doe_examples/#using-delta-for-uniform-uncertainty","text":"# Create design with delta for uniform uncertainty design_delta = InitialDesign ( output_dir = 'doe_delta_example' , sampling_method = 'lhs' ) # Add design variable with uniform uncertainty design_delta . add_design_variable ( name = 'x' , range_bounds = [ - 5 , 5 ], delta = 0.1 , # Half-width of 0.1 around design points distribution = 'uniform' , description = 'design variable with uniform uncertainty' ) # Run sampling results_delta = design_delta . run ( objective_function = objective_func , num_samples = 50 ) print ( f \"Delta example - Results range: [ { min ( results_delta [ 'y' ]) : .2f } , { max ( results_delta [ 'y' ]) : .2f } ]\" )","title":"Using Delta for Uniform Uncertainty"},{"location":"basic-usage/doe/doe_examples/#handling-near-zero-regions","text":"import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt # Define an objective function that crosses zero def sinusoidal_func ( x ): return np . sin ( x [:, 0 ]) # Create designs with different uncertainty specifications design_cov = InitialDesign ( output_dir = 'doe_near_zero_cov' , sampling_method = 'lhs' ) design_std = InitialDesign ( output_dir = 'doe_near_zero_std' , sampling_method = 'lhs' ) # Add design variables that will include zero in their range design_cov . add_design_variable ( name = 'x' , range_bounds = [ - 3.14 , 3.14 ], cov = 0.1 , # CoV will have issues near zero distribution = 'normal' ) design_std . add_design_variable ( name = 'x' , range_bounds = [ - 3.14 , 3.14 ], std = 0.2 , # Std works well even at zero distribution = 'normal' ) # Run sampling results_cov = design_cov . run ( objective_function = sinusoidal_func , num_samples = 200 ) results_std = design_std . run ( objective_function = sinusoidal_func , num_samples = 200 ) # Plot the results plt . figure ( figsize = ( 12 , 6 )) plt . subplot ( 1 , 2 , 1 ) plt . scatter ( results_cov [ 'x' ], results_cov [ 'y' ], alpha = 0.7 ) plt . title ( 'Using CoV for Near-Zero Regions' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) plt . grid ( True ) plt . subplot ( 1 , 2 , 2 ) plt . scatter ( results_std [ 'x' ], results_std [ 'y' ], alpha = 0.7 ) plt . title ( 'Using Std for Near-Zero Regions' ) plt . xlabel ( 'x' ) plt . ylabel ( 'sin(x)' ) plt . grid ( True ) plt . tight_layout () plt . show ()","title":"Handling Near-Zero Regions"},{"location":"basic-usage/doe/doe_examples/#1d-test-function-examples","text":"","title":"1D Test Function Examples"},{"location":"basic-usage/doe/doe_examples/#gaussian-mixture-function","text":"This example demonstrates how to use the DOE module with a 1D Gaussian mixture function. import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt # Define 1D Gaussian Mixture test function def objective_func_1D ( x ): \"\"\"1D test function with multiple Gaussian peaks\"\"\" if isinstance ( x , np . ndarray ) and x . ndim > 1 : x = x [:, 0 ] # Extract first column if it's a 2D array term1 = 100 * np . exp ( - (( x - 10 ) ** 2 ) / 0.8 ) term2 = 80 * np . exp ( - (( x - 20 ) ** 2 ) / 50 ) term3 = 20 * np . exp ( - (( x - 30 ) ** 2 ) / 18 ) return - ( term1 + term2 + term3 - 200 ) # Create design with LHS sampling design_1d = InitialDesign ( output_dir = 'doe_1d_example' , sampling_method = 'lhs' , sampling_criterion = 'maximin' ) # Add design variable using standard deviation instead of cov design_1d . add_design_variable ( name = 'x' , range_bounds = [ 0 , 40 ], std = 0.5 , # Fixed standard deviation description = 'design parameter with std uncertainty' ) # Run sampling results_1d = design_1d . run ( objective_function = objective_func_1D , num_samples = 50 ) # Plot results plt . figure ( figsize = ( 10 , 6 )) plt . scatter ( results_1d [ 'x' ], results_1d [ 'y' ], color = 'red' , alpha = 0.7 , label = 'Sampled Points' ) # Generate the true function x_true = np . linspace ( 0 , 40 , 1000 ) y_true = objective_func_1D ( x_true ) plt . plot ( x_true , y_true , 'k-' , label = 'True Function' ) plt . title ( '1D Gaussian Mixture Function' ) plt . xlabel ( 'x' ) plt . ylabel ( 'f(x)' ) plt . legend () plt . grid ( True ) plt . show ()","title":"Gaussian Mixture Function"},{"location":"basic-usage/doe/doe_examples/#2d-test-function-examples","text":"","title":"2D Test Function Examples"},{"location":"basic-usage/doe/doe_examples/#gaussian-peaks-function-with-delta-uncertainty","text":"This example shows how to sample from a 2D function with two Gaussian peaks using uniform uncertainty. import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt # Define 2D test function with two Gaussian peaks def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] # Parameters for two Gaussian peaks a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 # Lower and more sensitive a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 # Higher and more robust # Calculate negative sum of two Gaussian peaks f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f # Create design with Sobol sequence and uniform uncertainty design_2d = InitialDesign ( output_dir = 'doe_2d_example_delta' , sampling_method = 'sobol' ) # Add design variables with delta for uniform uncertainty design_2d . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], delta = 0.05 , # Half-width of 0.05 around design points distribution = 'uniform' , description = 'first design variable with uniform uncertainty' ) design_2d . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], delta = 0.05 , # Half-width of 0.05 around design points distribution = 'uniform' , description = 'second design variable with uniform uncertainty' ) # Run sampling results_2d = design_2d . run ( objective_function = objective_func_2D , num_samples = 50 ) # Create a contour plot of the design space plt . figure ( figsize = ( 10 , 8 )) # Generate grid for visualization x1_grid = np . linspace ( - 5 , 5 , 100 ) x2_grid = np . linspace ( - 6 , 6 , 100 ) X1 , X2 = np . meshgrid ( x1_grid , x2_grid ) grid_points = np . vstack ([ X1 . ravel (), X2 . ravel ()]) . T Z = objective_func_2D ( grid_points ) . reshape ( X1 . shape ) # Create contour plot plt . contourf ( X1 , X2 , Z , 50 , cmap = 'viridis' ) plt . colorbar ( label = 'Objective Value' ) plt . scatter ( results_2d [ 'x1' ], results_2d [ 'x2' ], c = 'red' , s = 50 , alpha = 0.7 , edgecolors = 'white' ) plt . title ( '2D Gaussian Peaks Function with Sobol Sampling (Uniform Uncertainty)' ) plt . xlabel ( 'x1' ) plt . ylabel ( 'x2' ) plt . grid ( True ) plt . show ()","title":"Gaussian Peaks Function with Delta Uncertainty"},{"location":"basic-usage/doe/doe_examples/#comparing-sampling-methods","text":"This example compares different sampling methods for a 2D design space. import numpy as np from PyEGRO.doe import InitialDesign from PyEGRO.doe.sampling import AdaptiveDistributionSampler import matplotlib.pyplot as plt # Compare different sampling methods sampling_methods = [ 'lhs' , 'sobol' , 'halton' , 'random' ] num_samples = 50 # Create figure plt . figure ( figsize = ( 16 , 12 )) for i , method in enumerate ( sampling_methods ): # Create design design = InitialDesign ( output_dir = f 'doe_compare_ { method } ' , sampling_method = method , sampling_criterion = 'maximin' if method == 'lhs' else None ) # Add design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], std = 0.2 # Using std instead of cov ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 5 , 5 ], std = 0.2 ) # Generate samples (without evaluating objective function) sampler = AdaptiveDistributionSampler () samples = sampler . generate_all_samples ( design . variables , num_samples , method = method , criterion = 'maximin' if method == 'lhs' else None ) # Plot plt . subplot ( 2 , 2 , i + 1 ) plt . scatter ( samples [:, 0 ], samples [:, 1 ], alpha = 0.7 ) plt . title ( f \" { method . upper () } Sampling ( { num_samples } points)\" ) plt . xlabel ( 'x1' ) plt . ylabel ( 'x2' ) plt . xlim ( - 5 , 5 ) plt . ylim ( - 5 , 5 ) plt . grid ( True ) plt . tight_layout () plt . show ()","title":"Comparing Sampling Methods"},{"location":"basic-usage/doe/doe_examples/#environmental-variables","text":"This example demonstrates how to incorporate environmental variables in your experiments with both min/max and std notation. ```python import numpy as np from PyEGRO.doe import InitialDesign import matplotlib.pyplot as plt","title":"Environmental Variables"},{"location":"basic-usage/doe/doe_examples/#define-objective-function-with-both-design-and-environmental-variables","text":"def objective_with_env_vars(x): # Extract design and environmental variables design_var = x[:, 0] # Design variable env_normal = x[:, 1] # Normal distributed environmental variable env_uniform = x[:, 2] # Uniform distributed environmental variable # Base function (using design variable) base_result = -design_var**2 # Apply environmental effects # Normal env var: additive effect # Uniform env var: multiplicative effect result = (base_result + 5*env_normal) * (1 + env_uniform) return result","title":"Define objective function with both design and environmental variables"},{"location":"basic-usage/doe/doe_examples/#create-design","text":"design_with_env = InitialDesign( output_dir='doe_with_env_vars_new', sampling_method='lhs' )","title":"Create design"},{"location":"basic-usage/doe/doe_examples/#add-design-variable-using-std-for-uncertainty","text":"design_with_env.add_design_variable( name='x', range_bounds=[-3, 3], std=0.2, # Standard deviation (instead of cov) description='design parameter with std-based uncertainty' )","title":"Add design variable using std for uncertainty"},{"location":"basic-usage/doe/doe_examples/#add-environmental-variables-with-different-distributions-and-notation","text":"design_with_env.add_env_variable( name='noise_additive', distribution='normal', mean=0.0, std=0.5, # Using std instead of cov description='additive noise (normal) with std' ) design_with_env.add_env_variable( name","title":"Add environmental variables with different distributions and notation"},{"location":"basic-usage/meta/cokriging/cokriging_examples/","text":"PyEGRO Co-Kriging Examples \u00b6 This document provides practical examples of using the PyEGRO Co-Kriging module for multi-fidelity modeling tasks. Table of Contents \u00b6 Introduction to Co-Kriging Installation Basic Usage Working with Synthetic Data Working with CSV Data 2D Examples Visualization Examples Model Loading and Prediction Advanced Usage Introduction to Co-Kriging \u00b6 Co-Kriging is a multi-fidelity modeling approach that combines data from different fidelity levels, typically: - Low-fidelity data : Larger dataset that is cheaper to obtain but less accurate - High-fidelity data : Smaller dataset that is more expensive to obtain but more accurate The Kennedy & O'Hagan (2000) approach used in this module creates a statistical relationship between the fidelity levels, allowing for more accurate predictions with fewer high-fidelity samples than would be needed with standard Gaussian Process Regression. Installation \u00b6 The PyEGRO Co-Kriging module depends on the following packages: - numpy - pandas - torch - gpytorch - scikit-learn - matplotlib - joblib - rich (for enhanced progress displays) Ensure these dependencies are installed before using the module: pip install numpy pandas torch gpytorch scikit-learn matplotlib joblib rich Basic Usage \u00b6 Here's a minimal example of training a Co-Kriging model with the PyEGRO module: import numpy as np from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Define high and low fidelity functions (for demonstration) def high_fidelity_function ( x ): return np . sin ( 8 * x ) + 0.2 * x def low_fidelity_function ( x ): return 0.5 * np . sin ( 8 * x ) + 0.15 * x + 0.5 # Generate synthetic data n_high = 15 # Fewer high-fidelity points n_low = 50 # More low-fidelity points X_high = np . random . uniform ( 0 , 1 , n_high ) . reshape ( - 1 , 1 ) y_high = high_fidelity_function ( X_high ) + 0.05 * np . random . randn ( n_high , 1 ) X_low = np . random . uniform ( 0 , 1 , n_low ) . reshape ( - 1 , 1 ) y_low = low_fidelity_function ( X_low ) + 0.1 * np . random . randn ( n_low , 1 ) # Initialize meta-training for Co-Kriging meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , kernel = 'matern25' ) # Train the model model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high ) # Make predictions X_new = np . linspace ( 0 , 1 , 100 ) . reshape ( - 1 , 1 ) y_pred_high , y_std_high = meta . predict ( X_new , fidelity = 'high' ) y_pred_low , y_std_low = meta . predict ( X_new , fidelity = 'low' ) print ( \"Co-Kriging Model trained successfully\" ) Working with Synthetic Data \u00b6 The following example demonstrates how to use the Co-Kriging module with synthetic data and visualize the results: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging from PyEGRO.meta.cokriging.visualization import visualize_cokriging # Set random seed for reproducibility np . random . seed ( 42 ) # Define high and low fidelity functions def high_fidelity_function ( x ): return ( 6 * x - 2 ) ** 2 * np . sin ( 12 * x - 4 ) def low_fidelity_function ( x ): return 0.5 * high_fidelity_function ( x ) + 10 * ( x - 0.5 ) - 5 # Generate synthetic data # Low fidelity: more samples but less accurate n_low = 80 X_low = np . random . uniform ( 0 , 1 , n_low ) . reshape ( - 1 , 1 ) y_low = low_fidelity_function ( X_low ) + np . random . normal ( 0 , 1.0 , X_low . shape ) # More noise # High fidelity: fewer samples but more accurate n_high = 20 X_high = np . random . uniform ( 0 , 1 , n_high ) . reshape ( - 1 , 1 ) y_high = high_fidelity_function ( X_high ) + np . random . normal ( 0 , 0.5 , X_high . shape ) # Less noise # Test data n_test = 40 X_test = np . linspace ( 0 , 1 , n_test ) . reshape ( - 1 , 1 ) y_test = high_fidelity_function ( X_test ) + np . random . normal ( 0 , 0.3 , X_test . shape ) # Even less noise # Define bounds and variable names bounds = np . array ([[ 0 , 1 ]]) variable_names = [ 'x' ] # Initialize and train Co-Kriging model print ( \"Training Co-Kriging model with synthetic data...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_COKRIGING_SYNTHETIC' ) # Train model with synthetic data model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) # Display figures plt . show () Working with CSV Data \u00b6 This example demonstrates how to use the Co-Kriging module with data stored in CSV files: import numpy as np import pandas as pd import json import os from PyEGRO.meta.cokriging import MetaTrainingCoKriging from PyEGRO.meta.cokriging.visualization import visualize_cokriging # Load initial data and problem configuration with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Load high and low fidelity training data high_fidelity_data = pd . read_csv ( 'DATA_PREPARATION/training_data_high.csv' ) low_fidelity_data = pd . read_csv ( 'DATA_PREPARATION/training_data_low.csv' ) # Load testing data test_data = pd . read_csv ( 'DATA_PREPARATION/testing_data.csv' ) # Get problem configuration bounds = np . array ( data_info [ 'input_bound' ]) variable_names = [ var [ 'name' ] for var in data_info [ 'variables' ]] # Get target column name (default to 'y' if not specified) target_column = data_info . get ( 'target_column' , 'y' ) # Extract features and targets X_high = high_fidelity_data . drop ([ target_column ], axis = 1 , errors = 'ignore' ) . values y_high = high_fidelity_data [ target_column ] . values . reshape ( - 1 , 1 ) X_low = low_fidelity_data . drop ([ target_column ], axis = 1 , errors = 'ignore' ) . values y_low = low_fidelity_data [ target_column ] . values . reshape ( - 1 , 1 ) # Extract testing data X_test = test_data . drop ([ target_column ], axis = 1 , errors = 'ignore' ) . values y_test = test_data [ target_column ] . values . reshape ( - 1 , 1 ) # Initialize and train model print ( \"Training Co-Kriging model with CSV data...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_COKRIGING' ) # Train model model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) Custom Data Preparation \u00b6 Example of preparing data files for Co-Kriging: import json import numpy as np import pandas as pd import os # Define fidelity functions def high_fidelity_function ( x1 , x2 ): return np . sin ( x1 ) * np . cos ( x2 ) + 0.2 * x1 * x2 def low_fidelity_function ( x1 , x2 ): return 0.5 * high_fidelity_function ( x1 , x2 ) + 0.2 * x1 - 0.1 * x2 + 0.5 # Generate data np . random . seed ( 42 ) # High-fidelity data (fewer samples) n_high = 30 X1_high = np . random . uniform ( - 2 , 2 , n_high ) X2_high = np . random . uniform ( - 2 , 2 , n_high ) y_high = high_fidelity_function ( X1_high , X2_high ) + np . random . normal ( 0 , 0.05 , n_high ) # Low-fidelity data (more samples) n_low = 100 X1_low = np . random . uniform ( - 2 , 2 , n_low ) X2_low = np . random . uniform ( - 2 , 2 , n_low ) y_low = low_fidelity_function ( X1_low , X2_low ) + np . random . normal ( 0 , 0.1 , n_low ) # Test data n_test = 50 X1_test = np . random . uniform ( - 2 , 2 , n_test ) X2_test = np . random . uniform ( - 2 , 2 , n_test ) y_test = high_fidelity_function ( X1_test , X2_test ) + np . random . normal ( 0 , 0.05 , n_test ) # Create DataFrames high_fidelity_df = pd . DataFrame ({ 'x1' : X1_high , 'x2' : X2_high , 'y' : y_high }) low_fidelity_df = pd . DataFrame ({ 'x1' : X1_low , 'x2' : X2_low , 'y' : y_low }) test_df = pd . DataFrame ({ 'x1' : X1_test , 'x2' : X2_test , 'y' : y_test }) # Create data_info.json data_info = { \"variables\" : [ { \"name\" : \"x1\" , \"type\" : \"continuous\" }, { \"name\" : \"x2\" , \"type\" : \"continuous\" } ], \"input_bound\" : [ [ - 2 , 2 ], [ - 2 , 2 ] ], \"target_column\" : \"y\" } # Create directory os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) # Save files with open ( \"DATA_PREPARATION/data_info.json\" , \"w\" ) as f : json . dump ( data_info , f , indent = 4 ) high_fidelity_df . to_csv ( \"DATA_PREPARATION/training_data_high.csv\" , index = False ) low_fidelity_df . to_csv ( \"DATA_PREPARATION/training_data_low.csv\" , index = False ) test_df . to_csv ( \"DATA_PREPARATION/testing_data.csv\" , index = False ) print ( \"Data preparation complete for Co-Kriging.\" ) 2D Examples \u00b6 Example of using Co-Kriging with 2D inputs: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging from PyEGRO.meta.cokriging.visualization import visualize_cokriging # Define high and low fidelity 2D functions def high_fidelity_function ( x1 , x2 ): return np . sin ( x1 ) * np . cos ( x2 ) + 0.2 * x1 * x2 def low_fidelity_function ( x1 , x2 ): return 0.5 * high_fidelity_function ( x1 , x2 ) + 0.2 * x1 - 0.1 * x2 + 0.5 # Generate synthetic data # Low fidelity: more samples but less accurate n_low = 100 X1_low = np . random . uniform ( - 2 , 2 , n_low ) X2_low = np . random . uniform ( - 2 , 2 , n_low ) X_low = np . column_stack ([ X1_low , X2_low ]) y_low = low_fidelity_function ( X1_low , X2_low ) . reshape ( - 1 , 1 ) + np . random . normal ( 0 , 0.1 , ( n_low , 1 )) # High fidelity: fewer samples but more accurate n_high = 25 X1_high = np . random . uniform ( - 2 , 2 , n_high ) X2_high = np . random . uniform ( - 2 , 2 , n_high ) X_high = np . column_stack ([ X1_high , X2_high ]) y_high = high_fidelity_function ( X1_high , X2_high ) . reshape ( - 1 , 1 ) + np . random . normal ( 0 , 0.05 , ( n_high , 1 )) # Test data: grid for visualization n_test = 16 X1_test = np . linspace ( - 2 , 2 , 4 ) X2_test = np . linspace ( - 2 , 2 , 4 ) X1_grid , X2_grid = np . meshgrid ( X1_test , X2_test ) X1_test = X1_grid . flatten () X2_test = X2_grid . flatten () X_test = np . column_stack ([ X1_test , X2_test ]) y_test = high_fidelity_function ( X1_test , X2_test ) . reshape ( - 1 , 1 ) + np . random . normal ( 0 , 0.02 , ( n_test , 1 )) # Define bounds and variable names bounds = np . array ([[ - 2 , 2 ], [ - 2 , 2 ]]) variable_names = [ 'x1' , 'x2' ] # Initialize and train Co-Kriging model print ( \"Training Co-Kriging model with 2D synthetic data...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_COKRIGING_2D' ) # Train model with synthetic data model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) plt . show () Visualization Examples \u00b6 The Co-Kriging module provides comprehensive visualization tools, accessible through the visualize_cokriging function: from PyEGRO.meta.cokriging.visualization import visualize_cokriging # After training a model (continuing from previous examples) figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True , output_dir = 'visualization_results' ) # Access individual figures actual_vs_predicted = figures [ 'actual_vs_predicted' ] r2_comparison = figures [ 'r2_comparison' ] response_surface = figures [ 'response_surface' ] # Customize and save a specific figure import matplotlib.pyplot as plt fig = figures [ 'response_surface' ] fig . suptitle ( 'Custom Title for Response Surface' ) fig . savefig ( 'custom_response_surface.png' , dpi = 300 ) Model Loading and Prediction \u00b6 Example of loading a trained model and making predictions: import numpy as np from PyEGRO.meta.cokriging.cokriging_utils import DeviceAgnosticCoKriging # Initialize device-agnostic loader cokriging_loader = DeviceAgnosticCoKriging ( prefer_gpu = True ) # Load the trained model loaded = cokriging_loader . load_model ( model_dir = 'RESULT_MODEL_COKRIGING' ) if loaded : # Generate new input data for prediction X_new = np . random . rand ( 10 , 2 ) * 4 - 2 # Values between -2 and 2 # Make high-fidelity predictions y_pred_high , y_std_high = cokriging_loader . predict ( X_new , fidelity = 'high' ) # Make low-fidelity predictions y_pred_low , y_std_low = cokriging_loader . predict ( X_new , fidelity = 'low' ) # Print results print ( \"High-fidelity predictions:\" ) for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred_high [ i ][ 0 ] : .4f } \u00b1 { y_std_high [ i ][ 0 ] : .4f } \" ) print ( \" \\n Low-fidelity predictions:\" ) for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred_low [ i ][ 0 ] : .4f } \u00b1 { y_std_low [ i ][ 0 ] : .4f } \" ) else : print ( \"Failed to load model\" ) Alternatively, use the MetaTrainingCoKriging class to load and use the model: from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Initialize meta meta = MetaTrainingCoKriging () # Load model meta . load_model ( 'RESULT_MODEL_COKRIGING/cokriging_model.pth' ) # Make predictions X_new = np . random . rand ( 10 , 2 ) * 4 - 2 # Values between -2 and 2 # High-fidelity predictions y_pred_high , y_std_high = meta . predict ( X_new , fidelity = 'high' ) # Low-fidelity predictions y_pred_low , y_std_low = meta . predict ( X_new , fidelity = 'low' ) # Print results for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } \" ) print ( f \" High-fidelity: { y_pred_high [ i ][ 0 ] : .4f } \u00b1 { y_std_high [ i ][ 0 ] : .4f } \" ) print ( f \" Low-fidelity: { y_pred_low [ i ][ 0 ] : .4f } \u00b1 { y_std_low [ i ][ 0 ] : .4f } \" ) # Print model hyperparameters meta . print_hyperparameters () Advanced Usage \u00b6 Comparing Different Kernels \u00b6 Co-Kriging can benefit from different kernel choices depending on the smoothness of your function: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Generate data (continuing with previous synthetic data example) # ... # List of kernels to compare kernels = [ 'matern25' , 'matern15' , 'matern05' , 'rbf' ] models = {} metrics = {} # Train a model with each kernel for kernel in kernels : print ( f \"Training with { kernel } kernel...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , kernel = kernel , output_dir = f 'RESULT_MODEL_COKRIGING_ { kernel } ' ) model , _ , _ = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test ) models [ kernel ] = model metrics [ kernel ] = meta . metrics # Compare test R\u00b2 scores plt . figure ( figsize = ( 10 , 6 )) plt . bar ( kernels , [ metrics [ k ][ 'test_r2' ] for k in kernels ]) plt . ylim ( 0 , 1 ) plt . title ( 'Test R\u00b2 Score by Kernel Type' ) plt . ylabel ( 'R\u00b2 Score' ) plt . grid ( axis = 'y' , alpha = 0.3 ) plt . show () Handling Large-Scale Multi-Fidelity Data \u00b6 For larger datasets, you can use batch processing: import numpy as np from PyEGRO.meta.cokriging.cokriging_utils import DeviceAgnosticCoKriging # Generate or load a large dataset n_samples = 10000 X_large = np . random . rand ( n_samples , 5 ) * 4 - 2 # 10,000 samples, 5 features # Load a trained model cokriging = DeviceAgnosticCoKriging ( prefer_gpu = True ) cokriging . load_model ( 'RESULT_MODEL_COKRIGING' ) # Make predictions with batch processing y_pred , y_std = cokriging . predict ( X_large , fidelity = 'high' , batch_size = 500 ) print ( f \"Processed { n_samples } samples with shapes: { y_pred . shape } , { y_std . shape } \" ) Uncertainty Quantification \u00b6 Co-Kriging is particularly useful for uncertainty quantification in multi-fidelity simulations: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging # After training a model (continuing from previous examples) # Create a fine grid for predictions x_grid = np . linspace ( - 2 , 2 , 200 ) . reshape ( - 1 , 1 ) # Get predictions at both fidelity levels y_high , std_high = meta . predict ( x_grid , fidelity = 'high' ) y_low , std_low = meta . predict ( x_grid , fidelity = 'low' ) # Plot with uncertainty bounds plt . figure ( figsize = ( 10 , 6 )) plt . plot ( x_grid , y_high , 'r-' , label = 'High-fidelity prediction' ) plt . fill_between ( x_grid . flatten (), ( y_high - 2 * std_high ) . flatten (), ( y_high + 2 * std_high ) . flatten (), alpha = 0.2 , color = 'red' , label = '95 % c onfidence interval' ) plt . plot ( x_grid , y_low , 'b--' , label = 'Low-fidelity prediction' ) plt . fill_between ( x_grid . flatten (), ( y_low - 2 * std_low ) . flatten (), ( y_low + 2 * std_low ) . flatten (), alpha = 0.1 , color = 'blue' ) # Plot training data plt . scatter ( X_high , y_high , c = 'red' , s = 60 , label = 'High-fidelity data' , marker = 'o' , edgecolor = 'black' , zorder = 5 ) plt . scatter ( X_low , y_low , c = 'blue' , s = 40 , label = 'Low-fidelity data' , marker = 's' , alpha = 0.7 , zorder = 4 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( 'Multi-fidelity predictions with uncertainty quantification' ) plt . legend () plt . grid ( True , alpha = 0.3 ) plt . show () Cross-Validation for Co-Kriging \u00b6 Implementing cross-validation for Co-Kriging models: import numpy as np from sklearn.model_selection import KFold from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Assuming X_high, y_high, X_low, y_low are already defined # Set up cross-validation n_splits = 5 kf = KFold ( n_splits = n_splits , shuffle = True , random_state = 42 ) # Metrics storage high_r2_scores = [] low_r2_scores = [] # Perform cross-validation for high-fidelity data for train_idx , test_idx in kf . split ( X_high ): # Split data X_high_train , X_high_test = X_high [ train_idx ], X_high [ test_idx ] y_high_train , y_high_test = y_high [ train_idx ], y_high [ test_idx ] # Use all low-fidelity data (this is a common approach in multi-fidelity modeling) meta = MetaTrainingCoKriging ( num_iterations = 200 , show_progress = False ) # Train model meta . train ( X_low = X_low , y_low = y_low , X_high = X_high_train , y_high = y_high_train ) # Make predictions y_high_pred , _ = meta . predict ( X_high_test , fidelity = 'high' ) y_low_pred , _ = meta . predict ( X_low , fidelity = 'low' ) # Calculate R\u00b2 scores high_r2 = 1 - np . sum (( y_high_test - y_high_pred ) ** 2 ) / np . sum (( y_high_test - np . mean ( y_high_test )) ** 2 ) low_r2 = 1 - np . sum (( y_low - y_low_pred ) ** 2 ) / np . sum (( y_low - np . mean ( y_low )) ** 2 ) high_r2_scores . append ( high_r2 ) low_r2_scores . append ( low_r2 ) print ( f \"High-fidelity CV R\u00b2 scores: { high_r2_scores } \" ) print ( f \"Mean high-fidelity CV R\u00b2: { np . mean ( high_r2_scores ) : .4f } \u00b1 { np . std ( high_r2_scores ) : .4f } \" ) print ( f \"Low-fidelity CV R\u00b2 scores: { low_r2_scores } \" ) print ( f \"Mean low-fidelity CV R\u00b2: { np . mean ( low_r2_scores ) : .4f } \u00b1 { np . std ( low_r2_scores ) : .4f } \" )","title":"Cokriging"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#pyegro-co-kriging-examples","text":"This document provides practical examples of using the PyEGRO Co-Kriging module for multi-fidelity modeling tasks.","title":"PyEGRO Co-Kriging Examples"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#table-of-contents","text":"Introduction to Co-Kriging Installation Basic Usage Working with Synthetic Data Working with CSV Data 2D Examples Visualization Examples Model Loading and Prediction Advanced Usage","title":"Table of Contents"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#introduction-to-co-kriging","text":"Co-Kriging is a multi-fidelity modeling approach that combines data from different fidelity levels, typically: - Low-fidelity data : Larger dataset that is cheaper to obtain but less accurate - High-fidelity data : Smaller dataset that is more expensive to obtain but more accurate The Kennedy & O'Hagan (2000) approach used in this module creates a statistical relationship between the fidelity levels, allowing for more accurate predictions with fewer high-fidelity samples than would be needed with standard Gaussian Process Regression.","title":"Introduction to Co-Kriging"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#installation","text":"The PyEGRO Co-Kriging module depends on the following packages: - numpy - pandas - torch - gpytorch - scikit-learn - matplotlib - joblib - rich (for enhanced progress displays) Ensure these dependencies are installed before using the module: pip install numpy pandas torch gpytorch scikit-learn matplotlib joblib rich","title":"Installation"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#basic-usage","text":"Here's a minimal example of training a Co-Kriging model with the PyEGRO module: import numpy as np from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Define high and low fidelity functions (for demonstration) def high_fidelity_function ( x ): return np . sin ( 8 * x ) + 0.2 * x def low_fidelity_function ( x ): return 0.5 * np . sin ( 8 * x ) + 0.15 * x + 0.5 # Generate synthetic data n_high = 15 # Fewer high-fidelity points n_low = 50 # More low-fidelity points X_high = np . random . uniform ( 0 , 1 , n_high ) . reshape ( - 1 , 1 ) y_high = high_fidelity_function ( X_high ) + 0.05 * np . random . randn ( n_high , 1 ) X_low = np . random . uniform ( 0 , 1 , n_low ) . reshape ( - 1 , 1 ) y_low = low_fidelity_function ( X_low ) + 0.1 * np . random . randn ( n_low , 1 ) # Initialize meta-training for Co-Kriging meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , kernel = 'matern25' ) # Train the model model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high ) # Make predictions X_new = np . linspace ( 0 , 1 , 100 ) . reshape ( - 1 , 1 ) y_pred_high , y_std_high = meta . predict ( X_new , fidelity = 'high' ) y_pred_low , y_std_low = meta . predict ( X_new , fidelity = 'low' ) print ( \"Co-Kriging Model trained successfully\" )","title":"Basic Usage"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#working-with-synthetic-data","text":"The following example demonstrates how to use the Co-Kriging module with synthetic data and visualize the results: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging from PyEGRO.meta.cokriging.visualization import visualize_cokriging # Set random seed for reproducibility np . random . seed ( 42 ) # Define high and low fidelity functions def high_fidelity_function ( x ): return ( 6 * x - 2 ) ** 2 * np . sin ( 12 * x - 4 ) def low_fidelity_function ( x ): return 0.5 * high_fidelity_function ( x ) + 10 * ( x - 0.5 ) - 5 # Generate synthetic data # Low fidelity: more samples but less accurate n_low = 80 X_low = np . random . uniform ( 0 , 1 , n_low ) . reshape ( - 1 , 1 ) y_low = low_fidelity_function ( X_low ) + np . random . normal ( 0 , 1.0 , X_low . shape ) # More noise # High fidelity: fewer samples but more accurate n_high = 20 X_high = np . random . uniform ( 0 , 1 , n_high ) . reshape ( - 1 , 1 ) y_high = high_fidelity_function ( X_high ) + np . random . normal ( 0 , 0.5 , X_high . shape ) # Less noise # Test data n_test = 40 X_test = np . linspace ( 0 , 1 , n_test ) . reshape ( - 1 , 1 ) y_test = high_fidelity_function ( X_test ) + np . random . normal ( 0 , 0.3 , X_test . shape ) # Even less noise # Define bounds and variable names bounds = np . array ([[ 0 , 1 ]]) variable_names = [ 'x' ] # Initialize and train Co-Kriging model print ( \"Training Co-Kriging model with synthetic data...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_COKRIGING_SYNTHETIC' ) # Train model with synthetic data model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) # Display figures plt . show ()","title":"Working with Synthetic Data"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#working-with-csv-data","text":"This example demonstrates how to use the Co-Kriging module with data stored in CSV files: import numpy as np import pandas as pd import json import os from PyEGRO.meta.cokriging import MetaTrainingCoKriging from PyEGRO.meta.cokriging.visualization import visualize_cokriging # Load initial data and problem configuration with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Load high and low fidelity training data high_fidelity_data = pd . read_csv ( 'DATA_PREPARATION/training_data_high.csv' ) low_fidelity_data = pd . read_csv ( 'DATA_PREPARATION/training_data_low.csv' ) # Load testing data test_data = pd . read_csv ( 'DATA_PREPARATION/testing_data.csv' ) # Get problem configuration bounds = np . array ( data_info [ 'input_bound' ]) variable_names = [ var [ 'name' ] for var in data_info [ 'variables' ]] # Get target column name (default to 'y' if not specified) target_column = data_info . get ( 'target_column' , 'y' ) # Extract features and targets X_high = high_fidelity_data . drop ([ target_column ], axis = 1 , errors = 'ignore' ) . values y_high = high_fidelity_data [ target_column ] . values . reshape ( - 1 , 1 ) X_low = low_fidelity_data . drop ([ target_column ], axis = 1 , errors = 'ignore' ) . values y_low = low_fidelity_data [ target_column ] . values . reshape ( - 1 , 1 ) # Extract testing data X_test = test_data . drop ([ target_column ], axis = 1 , errors = 'ignore' ) . values y_test = test_data [ target_column ] . values . reshape ( - 1 , 1 ) # Initialize and train model print ( \"Training Co-Kriging model with CSV data...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_COKRIGING' ) # Train model model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True )","title":"Working with CSV Data"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#custom-data-preparation","text":"Example of preparing data files for Co-Kriging: import json import numpy as np import pandas as pd import os # Define fidelity functions def high_fidelity_function ( x1 , x2 ): return np . sin ( x1 ) * np . cos ( x2 ) + 0.2 * x1 * x2 def low_fidelity_function ( x1 , x2 ): return 0.5 * high_fidelity_function ( x1 , x2 ) + 0.2 * x1 - 0.1 * x2 + 0.5 # Generate data np . random . seed ( 42 ) # High-fidelity data (fewer samples) n_high = 30 X1_high = np . random . uniform ( - 2 , 2 , n_high ) X2_high = np . random . uniform ( - 2 , 2 , n_high ) y_high = high_fidelity_function ( X1_high , X2_high ) + np . random . normal ( 0 , 0.05 , n_high ) # Low-fidelity data (more samples) n_low = 100 X1_low = np . random . uniform ( - 2 , 2 , n_low ) X2_low = np . random . uniform ( - 2 , 2 , n_low ) y_low = low_fidelity_function ( X1_low , X2_low ) + np . random . normal ( 0 , 0.1 , n_low ) # Test data n_test = 50 X1_test = np . random . uniform ( - 2 , 2 , n_test ) X2_test = np . random . uniform ( - 2 , 2 , n_test ) y_test = high_fidelity_function ( X1_test , X2_test ) + np . random . normal ( 0 , 0.05 , n_test ) # Create DataFrames high_fidelity_df = pd . DataFrame ({ 'x1' : X1_high , 'x2' : X2_high , 'y' : y_high }) low_fidelity_df = pd . DataFrame ({ 'x1' : X1_low , 'x2' : X2_low , 'y' : y_low }) test_df = pd . DataFrame ({ 'x1' : X1_test , 'x2' : X2_test , 'y' : y_test }) # Create data_info.json data_info = { \"variables\" : [ { \"name\" : \"x1\" , \"type\" : \"continuous\" }, { \"name\" : \"x2\" , \"type\" : \"continuous\" } ], \"input_bound\" : [ [ - 2 , 2 ], [ - 2 , 2 ] ], \"target_column\" : \"y\" } # Create directory os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) # Save files with open ( \"DATA_PREPARATION/data_info.json\" , \"w\" ) as f : json . dump ( data_info , f , indent = 4 ) high_fidelity_df . to_csv ( \"DATA_PREPARATION/training_data_high.csv\" , index = False ) low_fidelity_df . to_csv ( \"DATA_PREPARATION/training_data_low.csv\" , index = False ) test_df . to_csv ( \"DATA_PREPARATION/testing_data.csv\" , index = False ) print ( \"Data preparation complete for Co-Kriging.\" )","title":"Custom Data Preparation"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#2d-examples","text":"Example of using Co-Kriging with 2D inputs: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging from PyEGRO.meta.cokriging.visualization import visualize_cokriging # Define high and low fidelity 2D functions def high_fidelity_function ( x1 , x2 ): return np . sin ( x1 ) * np . cos ( x2 ) + 0.2 * x1 * x2 def low_fidelity_function ( x1 , x2 ): return 0.5 * high_fidelity_function ( x1 , x2 ) + 0.2 * x1 - 0.1 * x2 + 0.5 # Generate synthetic data # Low fidelity: more samples but less accurate n_low = 100 X1_low = np . random . uniform ( - 2 , 2 , n_low ) X2_low = np . random . uniform ( - 2 , 2 , n_low ) X_low = np . column_stack ([ X1_low , X2_low ]) y_low = low_fidelity_function ( X1_low , X2_low ) . reshape ( - 1 , 1 ) + np . random . normal ( 0 , 0.1 , ( n_low , 1 )) # High fidelity: fewer samples but more accurate n_high = 25 X1_high = np . random . uniform ( - 2 , 2 , n_high ) X2_high = np . random . uniform ( - 2 , 2 , n_high ) X_high = np . column_stack ([ X1_high , X2_high ]) y_high = high_fidelity_function ( X1_high , X2_high ) . reshape ( - 1 , 1 ) + np . random . normal ( 0 , 0.05 , ( n_high , 1 )) # Test data: grid for visualization n_test = 16 X1_test = np . linspace ( - 2 , 2 , 4 ) X2_test = np . linspace ( - 2 , 2 , 4 ) X1_grid , X2_grid = np . meshgrid ( X1_test , X2_test ) X1_test = X1_grid . flatten () X2_test = X2_grid . flatten () X_test = np . column_stack ([ X1_test , X2_test ]) y_test = high_fidelity_function ( X1_test , X2_test ) . reshape ( - 1 , 1 ) + np . random . normal ( 0 , 0.02 , ( n_test , 1 )) # Define bounds and variable names bounds = np . array ([[ - 2 , 2 ], [ - 2 , 2 ]]) variable_names = [ 'x1' , 'x2' ] # Initialize and train Co-Kriging model print ( \"Training Co-Kriging model with 2D synthetic data...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_COKRIGING_2D' ) # Train model with synthetic data model , scaler_X , scaler_y = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) plt . show ()","title":"2D Examples"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#visualization-examples","text":"The Co-Kriging module provides comprehensive visualization tools, accessible through the visualize_cokriging function: from PyEGRO.meta.cokriging.visualization import visualize_cokriging # After training a model (continuing from previous examples) figures = visualize_cokriging ( meta = meta , X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True , output_dir = 'visualization_results' ) # Access individual figures actual_vs_predicted = figures [ 'actual_vs_predicted' ] r2_comparison = figures [ 'r2_comparison' ] response_surface = figures [ 'response_surface' ] # Customize and save a specific figure import matplotlib.pyplot as plt fig = figures [ 'response_surface' ] fig . suptitle ( 'Custom Title for Response Surface' ) fig . savefig ( 'custom_response_surface.png' , dpi = 300 )","title":"Visualization Examples"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#model-loading-and-prediction","text":"Example of loading a trained model and making predictions: import numpy as np from PyEGRO.meta.cokriging.cokriging_utils import DeviceAgnosticCoKriging # Initialize device-agnostic loader cokriging_loader = DeviceAgnosticCoKriging ( prefer_gpu = True ) # Load the trained model loaded = cokriging_loader . load_model ( model_dir = 'RESULT_MODEL_COKRIGING' ) if loaded : # Generate new input data for prediction X_new = np . random . rand ( 10 , 2 ) * 4 - 2 # Values between -2 and 2 # Make high-fidelity predictions y_pred_high , y_std_high = cokriging_loader . predict ( X_new , fidelity = 'high' ) # Make low-fidelity predictions y_pred_low , y_std_low = cokriging_loader . predict ( X_new , fidelity = 'low' ) # Print results print ( \"High-fidelity predictions:\" ) for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred_high [ i ][ 0 ] : .4f } \u00b1 { y_std_high [ i ][ 0 ] : .4f } \" ) print ( \" \\n Low-fidelity predictions:\" ) for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred_low [ i ][ 0 ] : .4f } \u00b1 { y_std_low [ i ][ 0 ] : .4f } \" ) else : print ( \"Failed to load model\" ) Alternatively, use the MetaTrainingCoKriging class to load and use the model: from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Initialize meta meta = MetaTrainingCoKriging () # Load model meta . load_model ( 'RESULT_MODEL_COKRIGING/cokriging_model.pth' ) # Make predictions X_new = np . random . rand ( 10 , 2 ) * 4 - 2 # Values between -2 and 2 # High-fidelity predictions y_pred_high , y_std_high = meta . predict ( X_new , fidelity = 'high' ) # Low-fidelity predictions y_pred_low , y_std_low = meta . predict ( X_new , fidelity = 'low' ) # Print results for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } \" ) print ( f \" High-fidelity: { y_pred_high [ i ][ 0 ] : .4f } \u00b1 { y_std_high [ i ][ 0 ] : .4f } \" ) print ( f \" Low-fidelity: { y_pred_low [ i ][ 0 ] : .4f } \u00b1 { y_std_low [ i ][ 0 ] : .4f } \" ) # Print model hyperparameters meta . print_hyperparameters ()","title":"Model Loading and Prediction"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#comparing-different-kernels","text":"Co-Kriging can benefit from different kernel choices depending on the smoothness of your function: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Generate data (continuing with previous synthetic data example) # ... # List of kernels to compare kernels = [ 'matern25' , 'matern15' , 'matern05' , 'rbf' ] models = {} metrics = {} # Train a model with each kernel for kernel in kernels : print ( f \"Training with { kernel } kernel...\" ) meta = MetaTrainingCoKriging ( num_iterations = 300 , kernel = kernel , output_dir = f 'RESULT_MODEL_COKRIGING_ { kernel } ' ) model , _ , _ = meta . train ( X_low = X_low , y_low = y_low , X_high = X_high , y_high = y_high , X_test = X_test , y_test = y_test ) models [ kernel ] = model metrics [ kernel ] = meta . metrics # Compare test R\u00b2 scores plt . figure ( figsize = ( 10 , 6 )) plt . bar ( kernels , [ metrics [ k ][ 'test_r2' ] for k in kernels ]) plt . ylim ( 0 , 1 ) plt . title ( 'Test R\u00b2 Score by Kernel Type' ) plt . ylabel ( 'R\u00b2 Score' ) plt . grid ( axis = 'y' , alpha = 0.3 ) plt . show ()","title":"Comparing Different Kernels"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#handling-large-scale-multi-fidelity-data","text":"For larger datasets, you can use batch processing: import numpy as np from PyEGRO.meta.cokriging.cokriging_utils import DeviceAgnosticCoKriging # Generate or load a large dataset n_samples = 10000 X_large = np . random . rand ( n_samples , 5 ) * 4 - 2 # 10,000 samples, 5 features # Load a trained model cokriging = DeviceAgnosticCoKriging ( prefer_gpu = True ) cokriging . load_model ( 'RESULT_MODEL_COKRIGING' ) # Make predictions with batch processing y_pred , y_std = cokriging . predict ( X_large , fidelity = 'high' , batch_size = 500 ) print ( f \"Processed { n_samples } samples with shapes: { y_pred . shape } , { y_std . shape } \" )","title":"Handling Large-Scale Multi-Fidelity Data"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#uncertainty-quantification","text":"Co-Kriging is particularly useful for uncertainty quantification in multi-fidelity simulations: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.cokriging import MetaTrainingCoKriging # After training a model (continuing from previous examples) # Create a fine grid for predictions x_grid = np . linspace ( - 2 , 2 , 200 ) . reshape ( - 1 , 1 ) # Get predictions at both fidelity levels y_high , std_high = meta . predict ( x_grid , fidelity = 'high' ) y_low , std_low = meta . predict ( x_grid , fidelity = 'low' ) # Plot with uncertainty bounds plt . figure ( figsize = ( 10 , 6 )) plt . plot ( x_grid , y_high , 'r-' , label = 'High-fidelity prediction' ) plt . fill_between ( x_grid . flatten (), ( y_high - 2 * std_high ) . flatten (), ( y_high + 2 * std_high ) . flatten (), alpha = 0.2 , color = 'red' , label = '95 % c onfidence interval' ) plt . plot ( x_grid , y_low , 'b--' , label = 'Low-fidelity prediction' ) plt . fill_between ( x_grid . flatten (), ( y_low - 2 * std_low ) . flatten (), ( y_low + 2 * std_low ) . flatten (), alpha = 0.1 , color = 'blue' ) # Plot training data plt . scatter ( X_high , y_high , c = 'red' , s = 60 , label = 'High-fidelity data' , marker = 'o' , edgecolor = 'black' , zorder = 5 ) plt . scatter ( X_low , y_low , c = 'blue' , s = 40 , label = 'Low-fidelity data' , marker = 's' , alpha = 0.7 , zorder = 4 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( 'Multi-fidelity predictions with uncertainty quantification' ) plt . legend () plt . grid ( True , alpha = 0.3 ) plt . show ()","title":"Uncertainty Quantification"},{"location":"basic-usage/meta/cokriging/cokriging_examples/#cross-validation-for-co-kriging","text":"Implementing cross-validation for Co-Kriging models: import numpy as np from sklearn.model_selection import KFold from PyEGRO.meta.cokriging import MetaTrainingCoKriging # Assuming X_high, y_high, X_low, y_low are already defined # Set up cross-validation n_splits = 5 kf = KFold ( n_splits = n_splits , shuffle = True , random_state = 42 ) # Metrics storage high_r2_scores = [] low_r2_scores = [] # Perform cross-validation for high-fidelity data for train_idx , test_idx in kf . split ( X_high ): # Split data X_high_train , X_high_test = X_high [ train_idx ], X_high [ test_idx ] y_high_train , y_high_test = y_high [ train_idx ], y_high [ test_idx ] # Use all low-fidelity data (this is a common approach in multi-fidelity modeling) meta = MetaTrainingCoKriging ( num_iterations = 200 , show_progress = False ) # Train model meta . train ( X_low = X_low , y_low = y_low , X_high = X_high_train , y_high = y_high_train ) # Make predictions y_high_pred , _ = meta . predict ( X_high_test , fidelity = 'high' ) y_low_pred , _ = meta . predict ( X_low , fidelity = 'low' ) # Calculate R\u00b2 scores high_r2 = 1 - np . sum (( y_high_test - y_high_pred ) ** 2 ) / np . sum (( y_high_test - np . mean ( y_high_test )) ** 2 ) low_r2 = 1 - np . sum (( y_low - y_low_pred ) ** 2 ) / np . sum (( y_low - np . mean ( y_low )) ** 2 ) high_r2_scores . append ( high_r2 ) low_r2_scores . append ( low_r2 ) print ( f \"High-fidelity CV R\u00b2 scores: { high_r2_scores } \" ) print ( f \"Mean high-fidelity CV R\u00b2: { np . mean ( high_r2_scores ) : .4f } \u00b1 { np . std ( high_r2_scores ) : .4f } \" ) print ( f \"Low-fidelity CV R\u00b2 scores: { low_r2_scores } \" ) print ( f \"Mean low-fidelity CV R\u00b2: { np . mean ( low_r2_scores ) : .4f } \u00b1 { np . std ( low_r2_scores ) : .4f } \" )","title":"Cross-Validation for Co-Kriging"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/","text":"PyEGRO.meta.egocokriging Usage Examples \u00b6 This document provides examples of how to use the PyEGRO.meta.egocokriging module to build accurate surrogate models through multi-fidelity optimization. The Co-Kriging approach is particularly valuable when high-fidelity function evaluations are expensive and low-fidelity approximations are available. Table of Contents \u00b6 Basic Multi-Fidelity Modeling Working with Initial Data Custom Configuration Different Acquisition Functions Multi-Dimensional Multi-Fidelity Optimization Saving and Loading Models Visualizing Multi-Fidelity Data Custom Acquisition Strategies Basic Multi-Fidelity Modeling \u00b6 In this example, we build a surrogate model using both low-fidelity (cheap) and high-fidelity (expensive) evaluations. import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.egocokriging import EfficientGlobalOptimization # Define high-fidelity (expensive but accurate) function def high_fidelity_func ( x ): \"\"\"Expensive but accurate simulation\"\"\" return np . sin ( 8 * x ) + x # Define low-fidelity (cheap but less accurate) function def low_fidelity_func ( x ): \"\"\"Cheap approximation\"\"\" return 0.8 * np . sin ( 8 * x ) + 1.2 * x # Set up the bounds and variable names bounds = np . array ([[ 0 , 1 ]]) variable_names = [ 'x' ] # Create and run the optimizer with both fidelity levels optimizer = EfficientGlobalOptimization ( objective_func = high_fidelity_func , bounds = bounds , variable_names = variable_names , low_fidelity_func = low_fidelity_func ) # Run the optimization history = optimizer . run () # Print results best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x = { best_x [ 0 ] : .4f } , y = { best_y : .4f } \" ) # Print final correlation parameter (rho) final_rho = optimizer . rho_history [ - 1 ] print ( f \"Final correlation parameter (rho): { final_rho : .4f } \" ) Working with Initial Data \u00b6 This example demonstrates how to start with pre-existing data from different fidelity levels. import numpy as np import pandas as pd from PyEGRO.meta.egocokriging import EfficientGlobalOptimization # Define objective functions def high_fidelity_func ( x ): return ( 6 * x - 2 ) ** 2 * np . sin ( 12 * x - 4 ) def low_fidelity_func ( x ): return 0.5 * high_fidelity_func ( x ) + 10 * ( x - 0.5 ) + 5 # Set up bounds and variable names bounds = np . array ([[ 0 , 1 ]]) variable_names = [ 'x' ] # Create initial data with different fidelity levels # High fidelity points (expensive, use fewer points) x_high = np . array ([ 0.0 , 0.4 , 0.8 , 1.0 ]) . reshape ( - 1 , 1 ) y_high = high_fidelity_func ( x_high ) # Low fidelity points (cheap, use more points) x_low = np . linspace ( 0 , 1 , 10 ) . reshape ( - 1 , 1 ) y_low = low_fidelity_func ( x_low ) # Combine data, marking fidelity levels initial_data = pd . DataFrame ({ 'x' : np . vstack ([ x_high , x_low ]) . flatten (), 'y' : np . vstack ([ y_high , y_low ]) . flatten (), 'fidelity' : np . vstack ([ np . ones_like ( x_high ), # 1 = high fidelity np . zeros_like ( x_low ) # 0 = low fidelity ]) . flatten () }) # Create optimizer with initial data optimizer = EfficientGlobalOptimization ( objective_func = high_fidelity_func , bounds = bounds , variable_names = variable_names , low_fidelity_func = low_fidelity_func , initial_data = initial_data ) # Run optimization (will adaptively choose which fidelity to evaluate) history = optimizer . run () # Analyze results best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x = { best_x [ 0 ] : .4f } , y = { best_y : .4f } \" ) print ( f \"Final rho value: { optimizer . rho_history [ - 1 ] : .4f } \" ) # Count high vs low fidelity evaluations high_fidelity_count = sum ( optimizer . fidelities == 1 ) low_fidelity_count = sum ( optimizer . fidelities == 0 ) print ( f \"High fidelity evaluations: { high_fidelity_count } \" ) print ( f \"Low fidelity evaluations: { low_fidelity_count } \" ) Custom Configuration \u00b6 This example shows how to customize the multi-fidelity optimization process. import numpy as np from PyEGRO.meta.egocokriging import EfficientGlobalOptimization , TrainingConfig # Define objective functions def high_fidelity_func ( x ): \"\"\"Expensive but accurate 2D function\"\"\" return ( np . sin ( 10 * x [:, 0 ]) + np . cos ( 10 * x [:, 1 ])) * np . exp ( - 0.5 * ( x [:, 0 ] ** 2 + x [:, 1 ] ** 2 )) def low_fidelity_func ( x ): \"\"\"Cheap approximation\"\"\" return 0.8 * ( np . sin ( 10 * x [:, 0 ]) + np . cos ( 10 * x [:, 1 ])) * np . exp ( - 0.5 * ( x [:, 0 ] ** 2 + x [:, 1 ] ** 2 )) + 0.2 # Set up bounds and variable names bounds = np . array ([[ - 2 , 2 ], [ - 2 , 2 ]]) variable_names = [ 'x1' , 'x2' ] # Create custom configuration for multi-fidelity optimization config = TrainingConfig ( max_iterations = 50 , # Maximum optimization iterations acquisition_name = \"eigf\" , # Global fit acquisition function multi_fidelity = True , # Enable multi-fidelity modeling rho_threshold = 0.03 , # Tighter convergence on rho parameter rho_patience = 5 , # More patience for rho convergence kernel = \"matern25\" , # Use Mat\u00e9rn 2.5 kernel learning_rate = 0.005 , # Lower learning rate for stability training_iter = 200 , # More training iterations early_stopping_patience = 30 , # More patience for early stopping save_dir = \"RESULT_MULTIFIDELITY\" # Custom save directory ) # Create and run the optimizer with custom config optimizer = EfficientGlobalOptimization ( objective_func = high_fidelity_func , bounds = bounds , variable_names = variable_names , low_fidelity_func = low_fidelity_func , config = config ) # Run the optimization history = optimizer . run () # Get results best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" ) print ( f \"Final rho value: { optimizer . rho_history [ - 1 ] : .4f } \" ) Different Acquisition Functions \u00b6 This example demonstrates how different acquisition functions perform in multi-fidelity optimization. ```python import numpy as np from PyEGRO.meta.egocokriging import EfficientGlobalOptimization, TrainingConfig Define objective functions \u00b6 def high_fidelity_func(x): \"\"\"Branin function (expensive)\"\"\" a = 1 b = 5.1 / (4 * np.pi**2) c = 5 / np.pi r = 6 s = 10 t = 1 / (8 * np.pi) x1 = x[:, 0] x2 = x[:, 1] term1 = a * (x2 - b * x1**2 + c * x1 - r)**2 term2 = s * (1 - t) * np.cos(x1) return term1 + term2 + s def low_fidelity_func(x): \"\"\"Approximation of Branin with bias\"\"\" result = high_fidelity_func(x) # Add systematic bias and noise to create low-fidelity approximation return 0.75 * result + 15 + np.sin(x[:, 0] * 2) Set up bounds and variable names \u00b6 bounds = np.array([[-5, 10], [0, 15]]) variable_names = ['x1', 'x2'] List of acquisition functions to try \u00b6 acquisition_functions = [ \"ei\", # Expected Improvement \"pi\", # Probability of Improvement \"lcb\", # Lower Confidence Bound \"eigf\", # Expected Improvement for Global Fit \"e3i\", # Exploration Enhanced Expected Improvement \"cri3\" # Criterion 3 ] results = {} Try each acquisition function \u00b6 for acq_func in acquisition_functions: print(f\"\\nOptimizing with {acq_func.upper()}","title":"EGO-Cokriging"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#pyegrometaegocokriging-usage-examples","text":"This document provides examples of how to use the PyEGRO.meta.egocokriging module to build accurate surrogate models through multi-fidelity optimization. The Co-Kriging approach is particularly valuable when high-fidelity function evaluations are expensive and low-fidelity approximations are available.","title":"PyEGRO.meta.egocokriging Usage Examples"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#table-of-contents","text":"Basic Multi-Fidelity Modeling Working with Initial Data Custom Configuration Different Acquisition Functions Multi-Dimensional Multi-Fidelity Optimization Saving and Loading Models Visualizing Multi-Fidelity Data Custom Acquisition Strategies","title":"Table of Contents"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#basic-multi-fidelity-modeling","text":"In this example, we build a surrogate model using both low-fidelity (cheap) and high-fidelity (expensive) evaluations. import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.egocokriging import EfficientGlobalOptimization # Define high-fidelity (expensive but accurate) function def high_fidelity_func ( x ): \"\"\"Expensive but accurate simulation\"\"\" return np . sin ( 8 * x ) + x # Define low-fidelity (cheap but less accurate) function def low_fidelity_func ( x ): \"\"\"Cheap approximation\"\"\" return 0.8 * np . sin ( 8 * x ) + 1.2 * x # Set up the bounds and variable names bounds = np . array ([[ 0 , 1 ]]) variable_names = [ 'x' ] # Create and run the optimizer with both fidelity levels optimizer = EfficientGlobalOptimization ( objective_func = high_fidelity_func , bounds = bounds , variable_names = variable_names , low_fidelity_func = low_fidelity_func ) # Run the optimization history = optimizer . run () # Print results best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x = { best_x [ 0 ] : .4f } , y = { best_y : .4f } \" ) # Print final correlation parameter (rho) final_rho = optimizer . rho_history [ - 1 ] print ( f \"Final correlation parameter (rho): { final_rho : .4f } \" )","title":"Basic Multi-Fidelity Modeling"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#working-with-initial-data","text":"This example demonstrates how to start with pre-existing data from different fidelity levels. import numpy as np import pandas as pd from PyEGRO.meta.egocokriging import EfficientGlobalOptimization # Define objective functions def high_fidelity_func ( x ): return ( 6 * x - 2 ) ** 2 * np . sin ( 12 * x - 4 ) def low_fidelity_func ( x ): return 0.5 * high_fidelity_func ( x ) + 10 * ( x - 0.5 ) + 5 # Set up bounds and variable names bounds = np . array ([[ 0 , 1 ]]) variable_names = [ 'x' ] # Create initial data with different fidelity levels # High fidelity points (expensive, use fewer points) x_high = np . array ([ 0.0 , 0.4 , 0.8 , 1.0 ]) . reshape ( - 1 , 1 ) y_high = high_fidelity_func ( x_high ) # Low fidelity points (cheap, use more points) x_low = np . linspace ( 0 , 1 , 10 ) . reshape ( - 1 , 1 ) y_low = low_fidelity_func ( x_low ) # Combine data, marking fidelity levels initial_data = pd . DataFrame ({ 'x' : np . vstack ([ x_high , x_low ]) . flatten (), 'y' : np . vstack ([ y_high , y_low ]) . flatten (), 'fidelity' : np . vstack ([ np . ones_like ( x_high ), # 1 = high fidelity np . zeros_like ( x_low ) # 0 = low fidelity ]) . flatten () }) # Create optimizer with initial data optimizer = EfficientGlobalOptimization ( objective_func = high_fidelity_func , bounds = bounds , variable_names = variable_names , low_fidelity_func = low_fidelity_func , initial_data = initial_data ) # Run optimization (will adaptively choose which fidelity to evaluate) history = optimizer . run () # Analyze results best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x = { best_x [ 0 ] : .4f } , y = { best_y : .4f } \" ) print ( f \"Final rho value: { optimizer . rho_history [ - 1 ] : .4f } \" ) # Count high vs low fidelity evaluations high_fidelity_count = sum ( optimizer . fidelities == 1 ) low_fidelity_count = sum ( optimizer . fidelities == 0 ) print ( f \"High fidelity evaluations: { high_fidelity_count } \" ) print ( f \"Low fidelity evaluations: { low_fidelity_count } \" )","title":"Working with Initial Data"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#custom-configuration","text":"This example shows how to customize the multi-fidelity optimization process. import numpy as np from PyEGRO.meta.egocokriging import EfficientGlobalOptimization , TrainingConfig # Define objective functions def high_fidelity_func ( x ): \"\"\"Expensive but accurate 2D function\"\"\" return ( np . sin ( 10 * x [:, 0 ]) + np . cos ( 10 * x [:, 1 ])) * np . exp ( - 0.5 * ( x [:, 0 ] ** 2 + x [:, 1 ] ** 2 )) def low_fidelity_func ( x ): \"\"\"Cheap approximation\"\"\" return 0.8 * ( np . sin ( 10 * x [:, 0 ]) + np . cos ( 10 * x [:, 1 ])) * np . exp ( - 0.5 * ( x [:, 0 ] ** 2 + x [:, 1 ] ** 2 )) + 0.2 # Set up bounds and variable names bounds = np . array ([[ - 2 , 2 ], [ - 2 , 2 ]]) variable_names = [ 'x1' , 'x2' ] # Create custom configuration for multi-fidelity optimization config = TrainingConfig ( max_iterations = 50 , # Maximum optimization iterations acquisition_name = \"eigf\" , # Global fit acquisition function multi_fidelity = True , # Enable multi-fidelity modeling rho_threshold = 0.03 , # Tighter convergence on rho parameter rho_patience = 5 , # More patience for rho convergence kernel = \"matern25\" , # Use Mat\u00e9rn 2.5 kernel learning_rate = 0.005 , # Lower learning rate for stability training_iter = 200 , # More training iterations early_stopping_patience = 30 , # More patience for early stopping save_dir = \"RESULT_MULTIFIDELITY\" # Custom save directory ) # Create and run the optimizer with custom config optimizer = EfficientGlobalOptimization ( objective_func = high_fidelity_func , bounds = bounds , variable_names = variable_names , low_fidelity_func = low_fidelity_func , config = config ) # Run the optimization history = optimizer . run () # Get results best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" ) print ( f \"Final rho value: { optimizer . rho_history [ - 1 ] : .4f } \" )","title":"Custom Configuration"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#different-acquisition-functions","text":"This example demonstrates how different acquisition functions perform in multi-fidelity optimization. ```python import numpy as np from PyEGRO.meta.egocokriging import EfficientGlobalOptimization, TrainingConfig","title":"Different Acquisition Functions"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#define-objective-functions","text":"def high_fidelity_func(x): \"\"\"Branin function (expensive)\"\"\" a = 1 b = 5.1 / (4 * np.pi**2) c = 5 / np.pi r = 6 s = 10 t = 1 / (8 * np.pi) x1 = x[:, 0] x2 = x[:, 1] term1 = a * (x2 - b * x1**2 + c * x1 - r)**2 term2 = s * (1 - t) * np.cos(x1) return term1 + term2 + s def low_fidelity_func(x): \"\"\"Approximation of Branin with bias\"\"\" result = high_fidelity_func(x) # Add systematic bias and noise to create low-fidelity approximation return 0.75 * result + 15 + np.sin(x[:, 0] * 2)","title":"Define objective functions"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#set-up-bounds-and-variable-names","text":"bounds = np.array([[-5, 10], [0, 15]]) variable_names = ['x1', 'x2']","title":"Set up bounds and variable names"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#list-of-acquisition-functions-to-try","text":"acquisition_functions = [ \"ei\", # Expected Improvement \"pi\", # Probability of Improvement \"lcb\", # Lower Confidence Bound \"eigf\", # Expected Improvement for Global Fit \"e3i\", # Exploration Enhanced Expected Improvement \"cri3\" # Criterion 3 ] results = {}","title":"List of acquisition functions to try"},{"location":"basic-usage/meta/ego-cokriging/egocokriging_examples/#try-each-acquisition-function","text":"for acq_func in acquisition_functions: print(f\"\\nOptimizing with {acq_func.upper()}","title":"Try each acquisition function"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/","text":"PyEGRO.meta.egogpr Usage Examples \u00b6 This document provides examples of how to use the PyEGRO.meta.egogpr module to build accurate surrogate models through efficient adaptive sampling. EGO is particularly valuable when function evaluations are expensive and metamodel accuracy is critical. Table of Contents \u00b6 Basic 1D Optimization 2D Function Optimization Custom Configuration Different Acquisition Functions Using Initial Data Saving and Loading Models Multi-Dimensional Optimization Custom Visualization Basic 1D Metamodel Building \u00b6 In this example, we build an accurate surrogate model of a 1D function using adaptive sampling to efficiently allocate evaluation points where they most improve model accuracy. import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.egogpr import EfficientGlobalOptimization # Define a 1D objective function to minimize def objective_function ( x ): \"\"\"Simple 1D function with multiple local minima\"\"\" return np . sin ( x ) + np . sin ( 10 * x / 3 ) # Set up the bounds and variable names bounds = np . array ([[ 0 , 10 ]]) # 1D bounds from 0 to 10 variable_names = [ 'x' ] # Create and run the optimizer optimizer = EfficientGlobalOptimization ( objective_func = objective_function , bounds = bounds , variable_names = variable_names ) # Run the optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x = { best_x [ 0 ] : .4f } , y = { best_y : .4f } \" ) 2D Function Optimization \u00b6 This example demonstrates optimizing a 2D function - the Branin function, which is a common benchmark. import numpy as np from PyEGRO.meta.egogpr import EfficientGlobalOptimization # Define the 2D Branin function def branin ( x ): \"\"\" Branin function with three global minima Global minima: f(\u2212\u03c0, 12.275) = f(\u03c0, 2.275) = f(9.42478, 2.475) \u2248 0.397887 \"\"\" a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) x1 = x [:, 0 ] x2 = x [:, 1 ] term1 = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 term2 = s * ( 1 - t ) * np . cos ( x1 ) return term1 + term2 + s # Set up the bounds and variable names bounds = np . array ([[ - 5 , 10 ], [ 0 , 15 ]]) # 2D bounds variable_names = [ 'x1' , 'x2' ] # Create and run the optimizer optimizer = EfficientGlobalOptimization ( objective_func = branin , bounds = bounds , variable_names = variable_names ) # Run the optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" ) print ( f \"Known global minima: f(\u2212\u03c0, 12.275) = f(\u03c0, 2.275) = f(9.42478, 2.475) \u2248 0.397887\" ) Custom Configuration \u00b6 This example shows how to customize the optimization process using the TrainingConfig class. import numpy as np from PyEGRO.meta.egogpr import EfficientGlobalOptimization , TrainingConfig # Define a simple objective function def rosenbrock ( x ): \"\"\" Rosenbrock function (a.k.a. banana function) Global minimum at (1, 1) with value 0 \"\"\" x1 = x [:, 0 ] x2 = x [:, 1 ] return ( 1 - x1 ) ** 2 + 100 * ( x2 - x1 ** 2 ) ** 2 # Set up bounds and variable names bounds = np . array ([[ - 2 , 2 ], [ - 2 , 2 ]]) variable_names = [ 'x1' , 'x2' ] # Create custom configuration config = TrainingConfig ( max_iterations = 50 , # Maximum optimization iterations rmse_threshold = 0.0005 , # RMSE threshold for early stopping rmse_patience = 15 , # Patience for RMSE improvement acquisition_name = \"lcb\" , # Use Lower Confidence Bound acquisition_params = { \"beta\" : 2.5 }, # Custom acquisition parameters training_iter = 150 , # GP model training iterations learning_rate = 0.008 , # Learning rate for model training kernel = \"rbf\" , # Use RBF kernel save_dir = \"RESULT_ROSENBROCK\" # Custom save directory ) # Create and run the optimizer with custom config optimizer = EfficientGlobalOptimization ( objective_func = rosenbrock , bounds = bounds , variable_names = variable_names , config = config ) # Run the optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" ) Different Acquisition Functions \u00b6 This example demonstrates how to use different acquisition functions. import numpy as np from egogpr import EfficientGlobalOptimization , TrainingConfig # Define a simple objective function def ackley ( x ): \"\"\" Ackley function - a multimodal function with many local minima Global minimum at (0, 0) with value 0 \"\"\" a = 20 b = 0.2 c = 2 * np . pi term1 = - a * np . exp ( - b * np . sqrt ( 0.5 * ( x [:, 0 ] ** 2 + x [:, 1 ] ** 2 ))) term2 = - np . exp ( 0.5 * ( np . cos ( c * x [:, 0 ]) + np . cos ( c * x [:, 1 ]))) return term1 + term2 + a + np . exp ( 1 ) # Set up bounds and variable names bounds = np . array ([[ - 5 , 5 ], [ - 5 , 5 ]]) variable_names = [ 'x1' , 'x2' ] # List of acquisition functions to try acquisition_functions = [ \"ei\" , # Expected Improvement \"pi\" , # Probability of Improvement \"lcb\" , # Lower Confidence Bound \"e3i\" , # Exploration Enhanced Expected Improvement \"eigf\" , # Expected Improvement for Global Fit \"cri3\" # Criterion 3 ] results = {} # Try each acquisition function for acq_func in acquisition_functions : print ( f \" \\n Optimizing with { acq_func . upper () } acquisition function\" ) # Create custom config with this acquisition function config = TrainingConfig ( max_iterations = 30 , acquisition_name = acq_func , save_dir = f \"RESULT_ACKLEY_ { acq_func . upper () } \" ) # Create and run optimizer optimizer = EfficientGlobalOptimization ( objective_func = ackley , bounds = bounds , variable_names = variable_names , config = config ) # Run optimization history = optimizer . run () # Store results best_y = np . min ( optimizer . y_train ) results [ acq_func ] = best_y print ( f \"Best value found with { acq_func . upper () } : { best_y : .6f } \" ) # Print summary of results print ( \" \\n --- Summary of Results ---\" ) for acq_func , best_y in results . items (): print ( f \" { acq_func . upper () } : { best_y : .6f } \" ) Using Initial Data \u00b6 This example shows how to start optimization with some initial data points. import numpy as np import pandas as pd from egogpr import EfficientGlobalOptimization # Define objective function def himmelblau ( x ): \"\"\" Himmelblau's function with four equal local minima \"\"\" x1 = x [:, 0 ] x2 = x [:, 1 ] return ( x1 ** 2 + x2 - 11 ) ** 2 + ( x1 + x2 ** 2 - 7 ) ** 2 # Create initial data with Latin Hypercube Sampling def lhs_samples ( bounds , n_samples ): from scipy.stats.qmc import LatinHypercube n_dims = bounds . shape [ 0 ] sampler = LatinHypercube ( d = n_dims ) samples = sampler . random ( n = n_samples ) # Scale samples to bounds for i in range ( n_dims ): samples [:, i ] = samples [:, i ] * ( bounds [ i , 1 ] - bounds [ i , 0 ]) + bounds [ i , 0 ] return samples # Set up bounds and variable names bounds = np . array ([[ - 5 , 5 ], [ - 5 , 5 ]]) variable_names = [ 'x1' , 'x2' ] # Generate initial samples X_initial = lhs_samples ( bounds , 10 ) y_initial = np . array ([ himmelblau ( x . reshape ( 1 , - 1 )) for x in X_initial ]) . flatten () # Create DataFrame with initial data initial_data = pd . DataFrame ({ 'x1' : X_initial [:, 0 ], 'x2' : X_initial [:, 1 ], 'y' : y_initial }) # Create and run optimizer with initial data optimizer = EfficientGlobalOptimization ( objective_func = himmelblau , bounds = bounds , variable_names = variable_names , initial_data = initial_data ) # Run optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" ) Saving and Loading Models \u00b6 This example demonstrates how to save and load models for later use. import numpy as np from PyEGRO.meta.egogpr import ( EfficientGlobalOptimization , TrainingConfig , load_model_data , GPRegressionModel ) import torch import gpytorch # Define objective function def levy ( x ): \"\"\"Levy function with multiple local minima and one global minimum at (1,1)\"\"\" x1 = x [:, 0 ] x2 = x [:, 1 ] w1 = 1 + ( x1 - 1 ) / 4 w2 = 1 + ( x2 - 1 ) / 4 term1 = np . sin ( np . pi * w1 ) ** 2 term2 = (( w1 - 1 ) ** 2 ) * ( 1 + 10 * np . sin ( np . pi * w1 + 1 ) ** 2 ) term3 = (( w2 - 1 ) ** 2 ) * ( 1 + np . sin ( 2 * np . pi * w2 ) ** 2 ) return term1 + term2 + term3 # Set up bounds and variable names bounds = np . array ([[ - 10 , 10 ], [ - 10 , 10 ]]) variable_names = [ 'x1' , 'x2' ] # Create config with custom save directory config = TrainingConfig ( max_iterations = 20 , save_dir = \"LEVY_MODEL\" ) # Create and run optimizer optimizer = EfficientGlobalOptimization ( objective_func = levy , bounds = bounds , variable_names = variable_names , config = config ) # Run optimization history = optimizer . run () # Load the saved model model_data = load_model_data ( \"LEVY_MODEL\" ) # Extract model components state_dict = model_data [ 'state_dict' ] scalers = model_data [ 'scalers' ] metadata = model_data [ 'metadata' ] # Recreate the model train_x = state_dict [ 'train_inputs' ][ 0 ] train_y = state_dict [ 'train_targets' ] kernel = state_dict [ 'kernel' ] likelihood = gpytorch . likelihoods . GaussianLikelihood () model = GPRegressionModel ( train_x , train_y , likelihood , kernel = kernel ) # Load state dictionaries model . load_state_dict ( state_dict [ 'model' ]) likelihood . load_state_dict ( state_dict [ 'likelihood' ]) print ( \"Model loaded successfully\" ) print ( f \"Kernel type: { kernel } \" ) print ( f \"Number of training points: { len ( train_y ) } \" ) print ( f \"Input dimensions: { metadata [ 'variable_names' ] } \" ) print ( f \"Best value found: { train_y . min () . item () : .6f } \" ) # Example: Make prediction at new points def predict_with_loaded_model ( x_new ): # Scale inputs x_scaled = scalers [ 'X' ] . transform ( x_new ) x_tensor = torch . tensor ( x_scaled , dtype = torch . float32 ) # Make prediction model . eval () likelihood . eval () with torch . no_grad (), gpytorch . settings . fast_pred_var (): predictions = likelihood ( model ( x_tensor )) mean = predictions . mean . numpy () std = predictions . variance . sqrt () . numpy () # Transform back to original scale mean = scalers [ 'y' ] . inverse_transform ( mean . reshape ( - 1 , 1 )) . flatten () std = std . reshape ( - 1 , 1 ) * scalers [ 'y' ] . scale_ return mean , std # Try a new point new_point = np . array ([[ 1.0 , 1.0 ]]) mean , std = predict_with_loaded_model ( new_point ) print ( f \" \\n Prediction at { new_point [ 0 ] } :\" ) print ( f \"Mean: { mean [ 0 ] : .6f } \" ) print ( f \"Std: { std [ 0 ] : .6f } \" ) Multi-Dimensional Optimization \u00b6 This example shows how to optimize a higher-dimensional function. import numpy as np from egogpr import EfficientGlobalOptimization , TrainingConfig # Define a multi-dimensional objective function (Sphere function) def sphere ( x ): \"\"\" Sphere function - simple quadratic function Global minimum at origin with value 0 \"\"\" return np . sum ( x ** 2 , axis = 1 ) # Set number of dimensions n_dims = 5 # Set up bounds and variable names bounds = np . array ([[ - 5 , 5 ]] * n_dims ) variable_names = [ f 'x { i + 1 } ' for i in range ( n_dims )] # Create custom config config = TrainingConfig ( max_iterations = 50 , acquisition_name = \"ei\" , save_dir = f \"RESULT_SPHERE_ { n_dims } D\" ) # Create and run optimizer optimizer = EfficientGlobalOptimization ( objective_func = sphere , bounds = bounds , variable_names = variable_names , config = config ) # Run optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best value found: { best_y : .6f } \" ) print ( \"Best point coordinates:\" ) for i , name in enumerate ( variable_names ): print ( f \" { name } = { best_x [ i ] : .6f } \" ) Custom Visualization \u00b6 This example demonstrates how to customize visualizations for optimization results using matplotlib and the trained model. ```python import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.egogpr import ( EfficientGlobalOptimization, ModelVisualizer, TrainingConfig ) Define a 1D function \u00b6 def sinusoidal(x): \"\"\"Sinusoidal function with multiple local minima\"\"\" return np.sin(5 * x) * np.exp(-0.5 * x) + 0.2 * x Set up bounds and variable names \u00b6 bounds = np.array([[-2, 2]]) variable_names = ['x'] Create and run optimizer \u00b6 config = TrainingConfig( max_iterations=15, save_dir=\"CUSTOM_VIZ\" ) optimizer = EfficientGlobalOptimization( objective_func=sinusoidal, bounds=bounds, variable_names=variable_names, config=config ) Run optimization \u00b6 history = optimizer.run() Create a custom visualization to show the optimization progress \u00b6 plt.figure(figsize=(10, 6)) Generate a fine grid of points for the true function \u00b6 x_fine = np.linspace(bounds[0, 0], bounds[0, 1], 200).reshape(-1, 1) y_fine = sinusoidal(x_fine) Plot the true function \u00b6 plt.plot(x_fine, y_fine, 'k--', label='True Function') Plot all sampled points \u00b6 plt.scatter(optimizer.X_train, optimizer.y_train, color='blue', s=50, label='Evaluated Points') Highlight the best point \u00b6 best_idx = np.argmin(optimizer.y_train) plt.scatter(optimizer.X_train[best_idx], optimizer.y_train[best_idx], color='red', s=100, marker='*', label='Best Point') Get model predictions \u00b6 model_cpu = optimizer.model.cpu() likelihood_cpu = optimizer.likelihood.cpu() x_tensor = torch.tensor(optimizer.scaler_x.transform(x_fine), dtype=torch.float32) model_cpu.eval() likelihood_cpu.eval() with torch.no_grad(), gpytorch.settings.fast_pred_var(): predictions = likelihood_cpu(model_cpu(x_tensor)) mean = predictions.mean.numpy() std = predictions.variance.sqrt().numpy() Convert predictions back to original scale \u00b6 mean = optimizer.scaler_y.inverse_transform(mean.reshape(-1, 1)).flatten() std = std.reshape(-1, 1) * optimizer.scaler_y.scale_ Plot the model prediction and uncertainty \u00b6 plt.plot(x_fine, mean, 'b-', label='GP Prediction') plt.fill_between(x_fine.flatten(), (mean - 2 * std.flatten()), (mean + 2 * std.flatten()), color='blue', alpha=0.2, label='95% Confidence') Customize plot \u00b6 plt.title('Optimization Progress with Custom Visualization', fontsize=14) plt.xlabel('x', fontsize=12) plt.ylabel('f(x)', fontsize=12) plt.legend(loc='best') plt.grid(True, alpha=0.3) plt.tight_layout() Save the custom visualization \u00b6 plt.savefig('custom_visualization.png', dpi=300, bbox_inches='tight') plt.show() print(f\"Best point found: x = {optimizer.X_train[best_idx][0]:.4f}, y = {optimizer.y_train[best_idx][0]:.4f}\") print(\"Custom visualization saved as 'custom_visualization.png'\")","title":"EGO-GPR"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#pyegrometaegogpr-usage-examples","text":"This document provides examples of how to use the PyEGRO.meta.egogpr module to build accurate surrogate models through efficient adaptive sampling. EGO is particularly valuable when function evaluations are expensive and metamodel accuracy is critical.","title":"PyEGRO.meta.egogpr Usage Examples"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#table-of-contents","text":"Basic 1D Optimization 2D Function Optimization Custom Configuration Different Acquisition Functions Using Initial Data Saving and Loading Models Multi-Dimensional Optimization Custom Visualization","title":"Table of Contents"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#basic-1d-metamodel-building","text":"In this example, we build an accurate surrogate model of a 1D function using adaptive sampling to efficiently allocate evaluation points where they most improve model accuracy. import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.egogpr import EfficientGlobalOptimization # Define a 1D objective function to minimize def objective_function ( x ): \"\"\"Simple 1D function with multiple local minima\"\"\" return np . sin ( x ) + np . sin ( 10 * x / 3 ) # Set up the bounds and variable names bounds = np . array ([[ 0 , 10 ]]) # 1D bounds from 0 to 10 variable_names = [ 'x' ] # Create and run the optimizer optimizer = EfficientGlobalOptimization ( objective_func = objective_function , bounds = bounds , variable_names = variable_names ) # Run the optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x = { best_x [ 0 ] : .4f } , y = { best_y : .4f } \" )","title":"Basic 1D Metamodel Building"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#2d-function-optimization","text":"This example demonstrates optimizing a 2D function - the Branin function, which is a common benchmark. import numpy as np from PyEGRO.meta.egogpr import EfficientGlobalOptimization # Define the 2D Branin function def branin ( x ): \"\"\" Branin function with three global minima Global minima: f(\u2212\u03c0, 12.275) = f(\u03c0, 2.275) = f(9.42478, 2.475) \u2248 0.397887 \"\"\" a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) x1 = x [:, 0 ] x2 = x [:, 1 ] term1 = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 term2 = s * ( 1 - t ) * np . cos ( x1 ) return term1 + term2 + s # Set up the bounds and variable names bounds = np . array ([[ - 5 , 10 ], [ 0 , 15 ]]) # 2D bounds variable_names = [ 'x1' , 'x2' ] # Create and run the optimizer optimizer = EfficientGlobalOptimization ( objective_func = branin , bounds = bounds , variable_names = variable_names ) # Run the optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" ) print ( f \"Known global minima: f(\u2212\u03c0, 12.275) = f(\u03c0, 2.275) = f(9.42478, 2.475) \u2248 0.397887\" )","title":"2D Function Optimization"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#custom-configuration","text":"This example shows how to customize the optimization process using the TrainingConfig class. import numpy as np from PyEGRO.meta.egogpr import EfficientGlobalOptimization , TrainingConfig # Define a simple objective function def rosenbrock ( x ): \"\"\" Rosenbrock function (a.k.a. banana function) Global minimum at (1, 1) with value 0 \"\"\" x1 = x [:, 0 ] x2 = x [:, 1 ] return ( 1 - x1 ) ** 2 + 100 * ( x2 - x1 ** 2 ) ** 2 # Set up bounds and variable names bounds = np . array ([[ - 2 , 2 ], [ - 2 , 2 ]]) variable_names = [ 'x1' , 'x2' ] # Create custom configuration config = TrainingConfig ( max_iterations = 50 , # Maximum optimization iterations rmse_threshold = 0.0005 , # RMSE threshold for early stopping rmse_patience = 15 , # Patience for RMSE improvement acquisition_name = \"lcb\" , # Use Lower Confidence Bound acquisition_params = { \"beta\" : 2.5 }, # Custom acquisition parameters training_iter = 150 , # GP model training iterations learning_rate = 0.008 , # Learning rate for model training kernel = \"rbf\" , # Use RBF kernel save_dir = \"RESULT_ROSENBROCK\" # Custom save directory ) # Create and run the optimizer with custom config optimizer = EfficientGlobalOptimization ( objective_func = rosenbrock , bounds = bounds , variable_names = variable_names , config = config ) # Run the optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" )","title":"Custom Configuration"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#different-acquisition-functions","text":"This example demonstrates how to use different acquisition functions. import numpy as np from egogpr import EfficientGlobalOptimization , TrainingConfig # Define a simple objective function def ackley ( x ): \"\"\" Ackley function - a multimodal function with many local minima Global minimum at (0, 0) with value 0 \"\"\" a = 20 b = 0.2 c = 2 * np . pi term1 = - a * np . exp ( - b * np . sqrt ( 0.5 * ( x [:, 0 ] ** 2 + x [:, 1 ] ** 2 ))) term2 = - np . exp ( 0.5 * ( np . cos ( c * x [:, 0 ]) + np . cos ( c * x [:, 1 ]))) return term1 + term2 + a + np . exp ( 1 ) # Set up bounds and variable names bounds = np . array ([[ - 5 , 5 ], [ - 5 , 5 ]]) variable_names = [ 'x1' , 'x2' ] # List of acquisition functions to try acquisition_functions = [ \"ei\" , # Expected Improvement \"pi\" , # Probability of Improvement \"lcb\" , # Lower Confidence Bound \"e3i\" , # Exploration Enhanced Expected Improvement \"eigf\" , # Expected Improvement for Global Fit \"cri3\" # Criterion 3 ] results = {} # Try each acquisition function for acq_func in acquisition_functions : print ( f \" \\n Optimizing with { acq_func . upper () } acquisition function\" ) # Create custom config with this acquisition function config = TrainingConfig ( max_iterations = 30 , acquisition_name = acq_func , save_dir = f \"RESULT_ACKLEY_ { acq_func . upper () } \" ) # Create and run optimizer optimizer = EfficientGlobalOptimization ( objective_func = ackley , bounds = bounds , variable_names = variable_names , config = config ) # Run optimization history = optimizer . run () # Store results best_y = np . min ( optimizer . y_train ) results [ acq_func ] = best_y print ( f \"Best value found with { acq_func . upper () } : { best_y : .6f } \" ) # Print summary of results print ( \" \\n --- Summary of Results ---\" ) for acq_func , best_y in results . items (): print ( f \" { acq_func . upper () } : { best_y : .6f } \" )","title":"Different Acquisition Functions"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#using-initial-data","text":"This example shows how to start optimization with some initial data points. import numpy as np import pandas as pd from egogpr import EfficientGlobalOptimization # Define objective function def himmelblau ( x ): \"\"\" Himmelblau's function with four equal local minima \"\"\" x1 = x [:, 0 ] x2 = x [:, 1 ] return ( x1 ** 2 + x2 - 11 ) ** 2 + ( x1 + x2 ** 2 - 7 ) ** 2 # Create initial data with Latin Hypercube Sampling def lhs_samples ( bounds , n_samples ): from scipy.stats.qmc import LatinHypercube n_dims = bounds . shape [ 0 ] sampler = LatinHypercube ( d = n_dims ) samples = sampler . random ( n = n_samples ) # Scale samples to bounds for i in range ( n_dims ): samples [:, i ] = samples [:, i ] * ( bounds [ i , 1 ] - bounds [ i , 0 ]) + bounds [ i , 0 ] return samples # Set up bounds and variable names bounds = np . array ([[ - 5 , 5 ], [ - 5 , 5 ]]) variable_names = [ 'x1' , 'x2' ] # Generate initial samples X_initial = lhs_samples ( bounds , 10 ) y_initial = np . array ([ himmelblau ( x . reshape ( 1 , - 1 )) for x in X_initial ]) . flatten () # Create DataFrame with initial data initial_data = pd . DataFrame ({ 'x1' : X_initial [:, 0 ], 'x2' : X_initial [:, 1 ], 'y' : y_initial }) # Create and run optimizer with initial data optimizer = EfficientGlobalOptimization ( objective_func = himmelblau , bounds = bounds , variable_names = variable_names , initial_data = initial_data ) # Run optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best point found: x1 = { best_x [ 0 ] : .4f } , x2 = { best_x [ 1 ] : .4f } , y = { best_y : .4f } \" )","title":"Using Initial Data"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#saving-and-loading-models","text":"This example demonstrates how to save and load models for later use. import numpy as np from PyEGRO.meta.egogpr import ( EfficientGlobalOptimization , TrainingConfig , load_model_data , GPRegressionModel ) import torch import gpytorch # Define objective function def levy ( x ): \"\"\"Levy function with multiple local minima and one global minimum at (1,1)\"\"\" x1 = x [:, 0 ] x2 = x [:, 1 ] w1 = 1 + ( x1 - 1 ) / 4 w2 = 1 + ( x2 - 1 ) / 4 term1 = np . sin ( np . pi * w1 ) ** 2 term2 = (( w1 - 1 ) ** 2 ) * ( 1 + 10 * np . sin ( np . pi * w1 + 1 ) ** 2 ) term3 = (( w2 - 1 ) ** 2 ) * ( 1 + np . sin ( 2 * np . pi * w2 ) ** 2 ) return term1 + term2 + term3 # Set up bounds and variable names bounds = np . array ([[ - 10 , 10 ], [ - 10 , 10 ]]) variable_names = [ 'x1' , 'x2' ] # Create config with custom save directory config = TrainingConfig ( max_iterations = 20 , save_dir = \"LEVY_MODEL\" ) # Create and run optimizer optimizer = EfficientGlobalOptimization ( objective_func = levy , bounds = bounds , variable_names = variable_names , config = config ) # Run optimization history = optimizer . run () # Load the saved model model_data = load_model_data ( \"LEVY_MODEL\" ) # Extract model components state_dict = model_data [ 'state_dict' ] scalers = model_data [ 'scalers' ] metadata = model_data [ 'metadata' ] # Recreate the model train_x = state_dict [ 'train_inputs' ][ 0 ] train_y = state_dict [ 'train_targets' ] kernel = state_dict [ 'kernel' ] likelihood = gpytorch . likelihoods . GaussianLikelihood () model = GPRegressionModel ( train_x , train_y , likelihood , kernel = kernel ) # Load state dictionaries model . load_state_dict ( state_dict [ 'model' ]) likelihood . load_state_dict ( state_dict [ 'likelihood' ]) print ( \"Model loaded successfully\" ) print ( f \"Kernel type: { kernel } \" ) print ( f \"Number of training points: { len ( train_y ) } \" ) print ( f \"Input dimensions: { metadata [ 'variable_names' ] } \" ) print ( f \"Best value found: { train_y . min () . item () : .6f } \" ) # Example: Make prediction at new points def predict_with_loaded_model ( x_new ): # Scale inputs x_scaled = scalers [ 'X' ] . transform ( x_new ) x_tensor = torch . tensor ( x_scaled , dtype = torch . float32 ) # Make prediction model . eval () likelihood . eval () with torch . no_grad (), gpytorch . settings . fast_pred_var (): predictions = likelihood ( model ( x_tensor )) mean = predictions . mean . numpy () std = predictions . variance . sqrt () . numpy () # Transform back to original scale mean = scalers [ 'y' ] . inverse_transform ( mean . reshape ( - 1 , 1 )) . flatten () std = std . reshape ( - 1 , 1 ) * scalers [ 'y' ] . scale_ return mean , std # Try a new point new_point = np . array ([[ 1.0 , 1.0 ]]) mean , std = predict_with_loaded_model ( new_point ) print ( f \" \\n Prediction at { new_point [ 0 ] } :\" ) print ( f \"Mean: { mean [ 0 ] : .6f } \" ) print ( f \"Std: { std [ 0 ] : .6f } \" )","title":"Saving and Loading Models"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#multi-dimensional-optimization","text":"This example shows how to optimize a higher-dimensional function. import numpy as np from egogpr import EfficientGlobalOptimization , TrainingConfig # Define a multi-dimensional objective function (Sphere function) def sphere ( x ): \"\"\" Sphere function - simple quadratic function Global minimum at origin with value 0 \"\"\" return np . sum ( x ** 2 , axis = 1 ) # Set number of dimensions n_dims = 5 # Set up bounds and variable names bounds = np . array ([[ - 5 , 5 ]] * n_dims ) variable_names = [ f 'x { i + 1 } ' for i in range ( n_dims )] # Create custom config config = TrainingConfig ( max_iterations = 50 , acquisition_name = \"ei\" , save_dir = f \"RESULT_SPHERE_ { n_dims } D\" ) # Create and run optimizer optimizer = EfficientGlobalOptimization ( objective_func = sphere , bounds = bounds , variable_names = variable_names , config = config ) # Run optimization history = optimizer . run () # Best point found best_x = optimizer . X_train [ np . argmin ( optimizer . y_train )] best_y = np . min ( optimizer . y_train ) print ( f \"Best value found: { best_y : .6f } \" ) print ( \"Best point coordinates:\" ) for i , name in enumerate ( variable_names ): print ( f \" { name } = { best_x [ i ] : .6f } \" )","title":"Multi-Dimensional Optimization"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#custom-visualization","text":"This example demonstrates how to customize visualizations for optimization results using matplotlib and the trained model. ```python import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.egogpr import ( EfficientGlobalOptimization, ModelVisualizer, TrainingConfig )","title":"Custom Visualization"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#define-a-1d-function","text":"def sinusoidal(x): \"\"\"Sinusoidal function with multiple local minima\"\"\" return np.sin(5 * x) * np.exp(-0.5 * x) + 0.2 * x","title":"Define a 1D function"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#set-up-bounds-and-variable-names","text":"bounds = np.array([[-2, 2]]) variable_names = ['x']","title":"Set up bounds and variable names"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#create-and-run-optimizer","text":"config = TrainingConfig( max_iterations=15, save_dir=\"CUSTOM_VIZ\" ) optimizer = EfficientGlobalOptimization( objective_func=sinusoidal, bounds=bounds, variable_names=variable_names, config=config )","title":"Create and run optimizer"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#run-optimization","text":"history = optimizer.run()","title":"Run optimization"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#create-a-custom-visualization-to-show-the-optimization-progress","text":"plt.figure(figsize=(10, 6))","title":"Create a custom visualization to show the optimization progress"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#generate-a-fine-grid-of-points-for-the-true-function","text":"x_fine = np.linspace(bounds[0, 0], bounds[0, 1], 200).reshape(-1, 1) y_fine = sinusoidal(x_fine)","title":"Generate a fine grid of points for the true function"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#plot-the-true-function","text":"plt.plot(x_fine, y_fine, 'k--', label='True Function')","title":"Plot the true function"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#plot-all-sampled-points","text":"plt.scatter(optimizer.X_train, optimizer.y_train, color='blue', s=50, label='Evaluated Points')","title":"Plot all sampled points"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#highlight-the-best-point","text":"best_idx = np.argmin(optimizer.y_train) plt.scatter(optimizer.X_train[best_idx], optimizer.y_train[best_idx], color='red', s=100, marker='*', label='Best Point')","title":"Highlight the best point"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#get-model-predictions","text":"model_cpu = optimizer.model.cpu() likelihood_cpu = optimizer.likelihood.cpu() x_tensor = torch.tensor(optimizer.scaler_x.transform(x_fine), dtype=torch.float32) model_cpu.eval() likelihood_cpu.eval() with torch.no_grad(), gpytorch.settings.fast_pred_var(): predictions = likelihood_cpu(model_cpu(x_tensor)) mean = predictions.mean.numpy() std = predictions.variance.sqrt().numpy()","title":"Get model predictions"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#convert-predictions-back-to-original-scale","text":"mean = optimizer.scaler_y.inverse_transform(mean.reshape(-1, 1)).flatten() std = std.reshape(-1, 1) * optimizer.scaler_y.scale_","title":"Convert predictions back to original scale"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#plot-the-model-prediction-and-uncertainty","text":"plt.plot(x_fine, mean, 'b-', label='GP Prediction') plt.fill_between(x_fine.flatten(), (mean - 2 * std.flatten()), (mean + 2 * std.flatten()), color='blue', alpha=0.2, label='95% Confidence')","title":"Plot the model prediction and uncertainty"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#customize-plot","text":"plt.title('Optimization Progress with Custom Visualization', fontsize=14) plt.xlabel('x', fontsize=12) plt.ylabel('f(x)', fontsize=12) plt.legend(loc='best') plt.grid(True, alpha=0.3) plt.tight_layout()","title":"Customize plot"},{"location":"basic-usage/meta/ego-gpr/egogpr_examples/#save-the-custom-visualization","text":"plt.savefig('custom_visualization.png', dpi=300, bbox_inches='tight') plt.show() print(f\"Best point found: x = {optimizer.X_train[best_idx][0]:.4f}, y = {optimizer.y_train[best_idx][0]:.4f}\") print(\"Custom visualization saved as 'custom_visualization.png'\")","title":"Save the custom visualization"},{"location":"basic-usage/meta/gpr/gpr_examples/","text":"PyEGRO GPR Examples \u00b6 This document provides practical examples of using the PyEGRO GPR module for Gaussian Process Regression modeling tasks. Table of Contents \u00b6 Basic Usage Working with Synthetic Data Working with CSV Data Custom Data Preparation Visualization Examples Model Loading and Prediction Advanced Usage Basic Usage \u00b6 Here's a minimal example of training a GPR model with the PyEGRO module: import numpy as np from PyEGRO.meta.gpr import MetaTraining # Generate some synthetic data X = np . random . rand ( 100 , 2 ) # 100 samples, 2 features y = np . sin ( X [:, 0 ]) + np . cos ( X [:, 1 ]) + 0.1 * np . random . randn ( 100 ) # Initialize meta-training meta = MetaTraining ( num_iterations = 500 , prefer_gpu = True , kernel = 'matern15' ) # Train the model model , scaler_X , scaler_y = meta . train ( X = X , y = y ) # Make predictions X_new = np . random . rand ( 10 , 2 ) y_pred , y_std = meta . predict ( X_new ) print ( \"Predictions:\" , y_pred . flatten ()) print ( \"Uncertainties:\" , y_std . flatten ()) Working with Synthetic Data \u00b6 The following example demonstrates how to use the GPR module with synthetic data and visualize the results: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.gpr import MetaTraining from PyEGRO.meta.gpr.visualization import visualize_gpr # Set random seed for reproducibility np . random . seed ( 42 ) # Define a true function to sample from def true_function ( x ): return x * np . sin ( x ) # Generate synthetic data # Training data n_train = 30 X_train = np . random . uniform ( 0 , 10 , n_train ) . reshape ( - 1 , 1 ) y_train = true_function ( X_train ) + 0.5 * np . random . randn ( n_train , 1 ) # Add noise # Testing data n_test = 50 X_test = np . linspace ( 0 , 12 , n_test ) . reshape ( - 1 , 1 ) y_test = true_function ( X_test ) + 0.25 * np . random . randn ( n_test , 1 ) # Add less noise # Define bounds for visualization bounds = np . array ([[ 0 , 12 ]]) variable_names = [ 'x' ] # Initialize and train GPR model print ( \"Training GPR model with synthetic data...\" ) meta = MetaTraining ( num_iterations = 500 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_GPR_SYNTHETIC' , kernel = 'matern05' , learning_rate = 0.01 , patience = 50 ) # Train model with synthetic data model , scaler_X , scaler_y = meta . train ( X = X_train , y = y_train , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_gpr ( meta = meta , X_train = X_train , y_train = y_train , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) # Display figures plt . show () Working with CSV Data \u00b6 This example demonstrates how to use the GPR module with data stored in CSV files: import numpy as np import pandas as pd import json import os from PyEGRO.meta.gpr import MetaTraining from PyEGRO.meta.gpr.visualization import visualize_gpr # Load initial data and problem configuration with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Load training data training_data = pd . read_csv ( 'DATA_PREPARATION/training_data.csv' ) # Load testing data (if available) test_data = pd . read_csv ( 'DATA_PREPARATION/testing_data.csv' ) # Get problem configuration bounds = np . array ( data_info [ 'input_bound' ]) variable_names = [ var [ 'name' ] for var in data_info [ 'variables' ]] # Get target column name (default to 'y' if not specified) target_column = data_info . get ( 'target_column' , 'y' ) # Extract features and targets X_train = training_data [ variable_names ] . values y_train = training_data [ target_column ] . values . reshape ( - 1 , 1 ) # Extract testing data X_test = test_data [ variable_names ] . values y_test = test_data [ target_column ] . values . reshape ( - 1 , 1 ) # Initialize and train GPR model print ( \"Training GPR model with CSV data...\" ) meta = MetaTraining ( num_iterations = 500 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_GPR' , kernel = 'matern05' , learning_rate = 0.01 , patience = 50 ) # Train model model , scaler_X , scaler_y = meta . train ( X = X_train , y = y_train , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_gpr ( meta = meta , X_train = X_train , y_train = y_train , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) Alternatively, you can let the MetaTraining class load the data automatically: # Initialize with data paths meta = MetaTraining ( data_dir = 'DATA_PREPARATION' , data_info_file = 'DATA_PREPARATION/data_info.json' , data_training_file = 'DATA_PREPARATION/training_data.csv' , num_iterations = 500 , prefer_gpu = True , kernel = 'matern05' ) # Train the model (it will load data from the specified files) model , scaler_X , scaler_y = meta . train () Custom Data Preparation \u00b6 Example of preparing a data_info.json file: import json import numpy as np import pandas as pd # Sample data generation np . random . seed ( 42 ) n_samples = 100 X1 = np . random . uniform ( 0 , 10 , n_samples ) X2 = np . random . uniform ( - 5 , 5 , n_samples ) y = 2 * X1 + 3 * X2 + np . sin ( X1 ) * np . cos ( X2 ) + np . random . randn ( n_samples ) * 0.5 # Create a DataFrame data = pd . DataFrame ({ 'x1' : X1 , 'x2' : X2 , 'output' : y }) # Split into training (80%) and testing (20%) from sklearn.model_selection import train_test_split train_data , test_data = train_test_split ( data , test_size = 0.2 , random_state = 42 ) # Calculate bounds x1_min , x1_max = data [ 'x1' ] . min (), data [ 'x1' ] . max () x2_min , x2_max = data [ 'x2' ] . min (), data [ 'x2' ] . max () # Create data_info.json data_info = { \"variables\" : [ { \"name\" : \"x1\" , \"type\" : \"continuous\" }, { \"name\" : \"x2\" , \"type\" : \"continuous\" } ], \"input_bound\" : [ [ x1_min , x1_max ], [ x2_min , x2_max ] ], \"target_column\" : \"output\" } # Create DATA_PREPARATION directory import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) # Save files with open ( \"DATA_PREPARATION/data_info.json\" , \"w\" ) as f : json . dump ( data_info , f , indent = 4 ) train_data . to_csv ( \"DATA_PREPARATION/training_data.csv\" , index = False ) test_data . to_csv ( \"DATA_PREPARATION/testing_data.csv\" , index = False ) print ( \"Data preparation complete.\" ) Visualization Examples \u00b6 Creating visualizations for 1D and 2D models: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.gpr import MetaTraining from PyEGRO.meta.gpr.visualization import visualize_gpr # 1D Example (using previous synthetic data example) # ... # Generate visualizations figures = visualize_gpr ( meta = meta , X_train = X_train , y_train = y_train , X_test = X_test , y_test = y_test , variable_names = [ 'x' ], bounds = np . array ([[ 0 , 12 ]]), savefig = True , output_dir = 'visualizations/1d_model' ) # 2D Example n_samples = 100 X_train = np . random . rand ( n_samples , 2 ) * np . array ([ 10 , 8 ]) y_train = np . sin ( X_train [:, 0 ]) * np . cos ( X_train [:, 1 ]) + 0.1 * np . random . randn ( n_samples ) y_train = y_train . reshape ( - 1 , 1 ) meta_2d = MetaTraining ( num_iterations = 300 , kernel = 'rbf' , output_dir = 'RESULT_MODEL_GPR_2D' ) model_2d , _ , _ = meta_2d . train ( X = X_train , y = y_train ) figures_2d = visualize_gpr ( meta = meta_2d , X_train = X_train , y_train = y_train , variable_names = [ 'x1' , 'x2' ], bounds = np . array ([[ 0 , 10 ], [ 0 , 8 ]]), savefig = True , output_dir = 'visualizations/2d_model' ) plt . show () Model Loading and Prediction \u00b6 Example of loading a trained model and making predictions: import numpy as np from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Initialize device-agnostic loader gpr_loader = DeviceAgnosticGPR ( prefer_gpu = True ) # Load the trained model loaded = gpr_loader . load_model ( model_dir = 'RESULT_MODEL_GPR' ) if loaded : # Generate new input data for prediction X_new = np . random . rand ( 10 , 2 ) * np . array ([ 10 , 8 ]) # Make predictions y_pred , y_std = gpr_loader . predict ( X_new ) # Print results for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred [ i ][ 0 ] : .4f } \u00b1 { y_std [ i ][ 0 ] : .4f } \" ) else : print ( \"Failed to load model\" ) Alternatively, use the MetaTraining class to load and use the model: from PyEGRO.meta.gpr import MetaTraining # Initialize meta meta = MetaTraining () # Load model meta . load_model ( 'RESULT_MODEL_GPR/gpr_model.pth' ) # Make predictions X_new = np . random . rand ( 10 , 2 ) * np . array ([ 10 , 8 ]) y_pred , y_std = meta . predict ( X_new ) # Print results for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred [ i ][ 0 ] : .4f } \u00b1 { y_std [ i ][ 0 ] : .4f } \" ) # Print model hyperparameters meta . print_hyperparameters () Advanced Usage \u00b6 Customizing Kernels \u00b6 The GPR module supports different kernel types for different kinds of data: # For smooth functions meta_smooth = MetaTraining ( kernel = 'rbf' ) # For less smooth functions with continuous derivatives meta_matern25 = MetaTraining ( kernel = 'matern25' ) # Mat\u00e9rn 5/2 # For functions with continuous first derivatives meta_matern15 = MetaTraining ( kernel = 'matern15' ) # Mat\u00e9rn 3/2 # For continuous but non-differentiable functions meta_matern05 = MetaTraining ( kernel = 'matern05' ) # Mat\u00e9rn 1/2 Batch Processing for Large Datasets \u00b6 For larger datasets, you can use the DeviceAgnosticGPR class with batch processing: import numpy as np from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Generate a large dataset n_samples = 10000 X_large = np . random . rand ( n_samples , 5 ) # 10,000 samples with 5 features # Load a trained model gpr = DeviceAgnosticGPR ( prefer_gpu = True ) gpr . load_model ( 'RESULT_MODEL_GPR' ) # Make predictions with batch processing y_pred , y_std = gpr . predict ( X_large , batch_size = 500 ) # Process 500 samples at a time print ( f \"Processed { n_samples } samples with shapes: { y_pred . shape } , { y_std . shape } \" ) Early Stopping and Learning Rate Scheduling \u00b6 The MetaTraining class includes built-in early stopping and learning rate scheduling: meta = MetaTraining ( num_iterations = 1000 , # Maximum iterations learning_rate = 0.01 , # Initial learning rate patience = 50 # Patience for early stopping ) # The optimizer will reduce the learning rate when progress plateaus # Early stopping will trigger if no improvement after 'patience' iterations model , _ , _ = meta . train ( X = X_train , y = y_train )","title":"GPR"},{"location":"basic-usage/meta/gpr/gpr_examples/#pyegro-gpr-examples","text":"This document provides practical examples of using the PyEGRO GPR module for Gaussian Process Regression modeling tasks.","title":"PyEGRO GPR Examples"},{"location":"basic-usage/meta/gpr/gpr_examples/#table-of-contents","text":"Basic Usage Working with Synthetic Data Working with CSV Data Custom Data Preparation Visualization Examples Model Loading and Prediction Advanced Usage","title":"Table of Contents"},{"location":"basic-usage/meta/gpr/gpr_examples/#basic-usage","text":"Here's a minimal example of training a GPR model with the PyEGRO module: import numpy as np from PyEGRO.meta.gpr import MetaTraining # Generate some synthetic data X = np . random . rand ( 100 , 2 ) # 100 samples, 2 features y = np . sin ( X [:, 0 ]) + np . cos ( X [:, 1 ]) + 0.1 * np . random . randn ( 100 ) # Initialize meta-training meta = MetaTraining ( num_iterations = 500 , prefer_gpu = True , kernel = 'matern15' ) # Train the model model , scaler_X , scaler_y = meta . train ( X = X , y = y ) # Make predictions X_new = np . random . rand ( 10 , 2 ) y_pred , y_std = meta . predict ( X_new ) print ( \"Predictions:\" , y_pred . flatten ()) print ( \"Uncertainties:\" , y_std . flatten ())","title":"Basic Usage"},{"location":"basic-usage/meta/gpr/gpr_examples/#working-with-synthetic-data","text":"The following example demonstrates how to use the GPR module with synthetic data and visualize the results: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.gpr import MetaTraining from PyEGRO.meta.gpr.visualization import visualize_gpr # Set random seed for reproducibility np . random . seed ( 42 ) # Define a true function to sample from def true_function ( x ): return x * np . sin ( x ) # Generate synthetic data # Training data n_train = 30 X_train = np . random . uniform ( 0 , 10 , n_train ) . reshape ( - 1 , 1 ) y_train = true_function ( X_train ) + 0.5 * np . random . randn ( n_train , 1 ) # Add noise # Testing data n_test = 50 X_test = np . linspace ( 0 , 12 , n_test ) . reshape ( - 1 , 1 ) y_test = true_function ( X_test ) + 0.25 * np . random . randn ( n_test , 1 ) # Add less noise # Define bounds for visualization bounds = np . array ([[ 0 , 12 ]]) variable_names = [ 'x' ] # Initialize and train GPR model print ( \"Training GPR model with synthetic data...\" ) meta = MetaTraining ( num_iterations = 500 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_GPR_SYNTHETIC' , kernel = 'matern05' , learning_rate = 0.01 , patience = 50 ) # Train model with synthetic data model , scaler_X , scaler_y = meta . train ( X = X_train , y = y_train , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_gpr ( meta = meta , X_train = X_train , y_train = y_train , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) # Display figures plt . show ()","title":"Working with Synthetic Data"},{"location":"basic-usage/meta/gpr/gpr_examples/#working-with-csv-data","text":"This example demonstrates how to use the GPR module with data stored in CSV files: import numpy as np import pandas as pd import json import os from PyEGRO.meta.gpr import MetaTraining from PyEGRO.meta.gpr.visualization import visualize_gpr # Load initial data and problem configuration with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Load training data training_data = pd . read_csv ( 'DATA_PREPARATION/training_data.csv' ) # Load testing data (if available) test_data = pd . read_csv ( 'DATA_PREPARATION/testing_data.csv' ) # Get problem configuration bounds = np . array ( data_info [ 'input_bound' ]) variable_names = [ var [ 'name' ] for var in data_info [ 'variables' ]] # Get target column name (default to 'y' if not specified) target_column = data_info . get ( 'target_column' , 'y' ) # Extract features and targets X_train = training_data [ variable_names ] . values y_train = training_data [ target_column ] . values . reshape ( - 1 , 1 ) # Extract testing data X_test = test_data [ variable_names ] . values y_test = test_data [ target_column ] . values . reshape ( - 1 , 1 ) # Initialize and train GPR model print ( \"Training GPR model with CSV data...\" ) meta = MetaTraining ( num_iterations = 500 , prefer_gpu = True , show_progress = True , output_dir = 'RESULT_MODEL_GPR' , kernel = 'matern05' , learning_rate = 0.01 , patience = 50 ) # Train model model , scaler_X , scaler_y = meta . train ( X = X_train , y = y_train , X_test = X_test , y_test = y_test , feature_names = variable_names ) # Generate visualization figures = visualize_gpr ( meta = meta , X_train = X_train , y_train = y_train , X_test = X_test , y_test = y_test , variable_names = variable_names , bounds = bounds , savefig = True ) Alternatively, you can let the MetaTraining class load the data automatically: # Initialize with data paths meta = MetaTraining ( data_dir = 'DATA_PREPARATION' , data_info_file = 'DATA_PREPARATION/data_info.json' , data_training_file = 'DATA_PREPARATION/training_data.csv' , num_iterations = 500 , prefer_gpu = True , kernel = 'matern05' ) # Train the model (it will load data from the specified files) model , scaler_X , scaler_y = meta . train ()","title":"Working with CSV Data"},{"location":"basic-usage/meta/gpr/gpr_examples/#custom-data-preparation","text":"Example of preparing a data_info.json file: import json import numpy as np import pandas as pd # Sample data generation np . random . seed ( 42 ) n_samples = 100 X1 = np . random . uniform ( 0 , 10 , n_samples ) X2 = np . random . uniform ( - 5 , 5 , n_samples ) y = 2 * X1 + 3 * X2 + np . sin ( X1 ) * np . cos ( X2 ) + np . random . randn ( n_samples ) * 0.5 # Create a DataFrame data = pd . DataFrame ({ 'x1' : X1 , 'x2' : X2 , 'output' : y }) # Split into training (80%) and testing (20%) from sklearn.model_selection import train_test_split train_data , test_data = train_test_split ( data , test_size = 0.2 , random_state = 42 ) # Calculate bounds x1_min , x1_max = data [ 'x1' ] . min (), data [ 'x1' ] . max () x2_min , x2_max = data [ 'x2' ] . min (), data [ 'x2' ] . max () # Create data_info.json data_info = { \"variables\" : [ { \"name\" : \"x1\" , \"type\" : \"continuous\" }, { \"name\" : \"x2\" , \"type\" : \"continuous\" } ], \"input_bound\" : [ [ x1_min , x1_max ], [ x2_min , x2_max ] ], \"target_column\" : \"output\" } # Create DATA_PREPARATION directory import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) # Save files with open ( \"DATA_PREPARATION/data_info.json\" , \"w\" ) as f : json . dump ( data_info , f , indent = 4 ) train_data . to_csv ( \"DATA_PREPARATION/training_data.csv\" , index = False ) test_data . to_csv ( \"DATA_PREPARATION/testing_data.csv\" , index = False ) print ( \"Data preparation complete.\" )","title":"Custom Data Preparation"},{"location":"basic-usage/meta/gpr/gpr_examples/#visualization-examples","text":"Creating visualizations for 1D and 2D models: import numpy as np import matplotlib.pyplot as plt from PyEGRO.meta.gpr import MetaTraining from PyEGRO.meta.gpr.visualization import visualize_gpr # 1D Example (using previous synthetic data example) # ... # Generate visualizations figures = visualize_gpr ( meta = meta , X_train = X_train , y_train = y_train , X_test = X_test , y_test = y_test , variable_names = [ 'x' ], bounds = np . array ([[ 0 , 12 ]]), savefig = True , output_dir = 'visualizations/1d_model' ) # 2D Example n_samples = 100 X_train = np . random . rand ( n_samples , 2 ) * np . array ([ 10 , 8 ]) y_train = np . sin ( X_train [:, 0 ]) * np . cos ( X_train [:, 1 ]) + 0.1 * np . random . randn ( n_samples ) y_train = y_train . reshape ( - 1 , 1 ) meta_2d = MetaTraining ( num_iterations = 300 , kernel = 'rbf' , output_dir = 'RESULT_MODEL_GPR_2D' ) model_2d , _ , _ = meta_2d . train ( X = X_train , y = y_train ) figures_2d = visualize_gpr ( meta = meta_2d , X_train = X_train , y_train = y_train , variable_names = [ 'x1' , 'x2' ], bounds = np . array ([[ 0 , 10 ], [ 0 , 8 ]]), savefig = True , output_dir = 'visualizations/2d_model' ) plt . show ()","title":"Visualization Examples"},{"location":"basic-usage/meta/gpr/gpr_examples/#model-loading-and-prediction","text":"Example of loading a trained model and making predictions: import numpy as np from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Initialize device-agnostic loader gpr_loader = DeviceAgnosticGPR ( prefer_gpu = True ) # Load the trained model loaded = gpr_loader . load_model ( model_dir = 'RESULT_MODEL_GPR' ) if loaded : # Generate new input data for prediction X_new = np . random . rand ( 10 , 2 ) * np . array ([ 10 , 8 ]) # Make predictions y_pred , y_std = gpr_loader . predict ( X_new ) # Print results for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred [ i ][ 0 ] : .4f } \u00b1 { y_std [ i ][ 0 ] : .4f } \" ) else : print ( \"Failed to load model\" ) Alternatively, use the MetaTraining class to load and use the model: from PyEGRO.meta.gpr import MetaTraining # Initialize meta meta = MetaTraining () # Load model meta . load_model ( 'RESULT_MODEL_GPR/gpr_model.pth' ) # Make predictions X_new = np . random . rand ( 10 , 2 ) * np . array ([ 10 , 8 ]) y_pred , y_std = meta . predict ( X_new ) # Print results for i in range ( len ( X_new )): print ( f \"Input: { X_new [ i ] } , Prediction: { y_pred [ i ][ 0 ] : .4f } \u00b1 { y_std [ i ][ 0 ] : .4f } \" ) # Print model hyperparameters meta . print_hyperparameters ()","title":"Model Loading and Prediction"},{"location":"basic-usage/meta/gpr/gpr_examples/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"basic-usage/meta/gpr/gpr_examples/#customizing-kernels","text":"The GPR module supports different kernel types for different kinds of data: # For smooth functions meta_smooth = MetaTraining ( kernel = 'rbf' ) # For less smooth functions with continuous derivatives meta_matern25 = MetaTraining ( kernel = 'matern25' ) # Mat\u00e9rn 5/2 # For functions with continuous first derivatives meta_matern15 = MetaTraining ( kernel = 'matern15' ) # Mat\u00e9rn 3/2 # For continuous but non-differentiable functions meta_matern05 = MetaTraining ( kernel = 'matern05' ) # Mat\u00e9rn 1/2","title":"Customizing Kernels"},{"location":"basic-usage/meta/gpr/gpr_examples/#batch-processing-for-large-datasets","text":"For larger datasets, you can use the DeviceAgnosticGPR class with batch processing: import numpy as np from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Generate a large dataset n_samples = 10000 X_large = np . random . rand ( n_samples , 5 ) # 10,000 samples with 5 features # Load a trained model gpr = DeviceAgnosticGPR ( prefer_gpu = True ) gpr . load_model ( 'RESULT_MODEL_GPR' ) # Make predictions with batch processing y_pred , y_std = gpr . predict ( X_large , batch_size = 500 ) # Process 500 samples at a time print ( f \"Processed { n_samples } samples with shapes: { y_pred . shape } , { y_std . shape } \" )","title":"Batch Processing for Large Datasets"},{"location":"basic-usage/meta/gpr/gpr_examples/#early-stopping-and-learning-rate-scheduling","text":"The MetaTraining class includes built-in early stopping and learning rate scheduling: meta = MetaTraining ( num_iterations = 1000 , # Maximum iterations learning_rate = 0.01 , # Initial learning rate patience = 50 # Patience for early stopping ) # The optimizer will reduce the learning rate when progress plateaus # Early stopping will trigger if no improvement after 'patience' iterations model , _ , _ = meta . train ( X = X_train , y = y_train )","title":"Early Stopping and Learning Rate Scheduling"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/","text":"PyEGRO ModelTesting Examples \u00b6 This document provides practical examples of using the PyEGRO ModelTesting module to evaluate trained GPR and Co-Kriging models on unseen data. Table of Contents \u00b6 Introduction Installation Basic Usage Testing GPR Models Testing Co-Kriging Models Working with Custom Test Data Customizing Plots Command Line Interface Integration with Logging Advanced Usage Introduction \u00b6 The ModelTesting module provides tools to evaluate trained surrogate models (GPR and Co-Kriging) on unseen data. It offers functionality for: Loading trained models saved by the PyEGRO training modules Loading or generating test data Computing performance metrics (R\u00b2, RMSE, MAE) Visualizing model predictions and uncertainty Saving evaluation results and plots Installation \u00b6 The PyEGRO ModelTesting module depends on the following packages: - numpy - pandas - scikit-learn - matplotlib - scipy - torch - gpytorch - joblib These should already be installed if you're using the PyEGRO GPR or Co-Kriging modules. Basic Usage \u00b6 Here's a minimal example of testing a model using the ModelTesting module: from PyEGRO.meta.modeltesting import ModelTester # Initialize the tester with the path to your trained model tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR' ) # Load the model tester . load_model () # Load or generate test data X_test , y_test = tester . load_test_data ( data_path = 'test_data.csv' ) # Evaluate the model tester . evaluate ( X_test , y_test ) # Save results and generate plots tester . save_results ( output_dir = 'test_results' ) tester . plot_results ( output_dir = 'test_results' ) # Print metrics results = tester . test_results print ( f \"R\u00b2 Score: { results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { results [ 'rmse' ] : .4f } \" ) print ( f \"MAE: { results [ 'mae' ] : .4f } \" ) Testing GPR Models \u00b6 Example of testing a GPR model with synthetic data: from PyEGRO.meta.modeltesting import ModelTester import numpy as np import matplotlib.pyplot as plt # Create a tester for a GPR model tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR_SYNTHETIC' , model_name = 'gpr_model' # Optional, will be inferred if not provided ) # Load the model tester . load_model () # Create synthetic test data for 1D function n_samples = 50 X_test = np . linspace ( 0 , 10 , n_samples ) . reshape ( - 1 , 1 ) y_true = X_test * np . sin ( X_test ) + 0.1 * np . random . randn ( n_samples , 1 ) # Evaluate the model tester . evaluate ( X_test , y_true ) # Generate and display plots figures = tester . plot_results ( output_dir = 'test_results' , show_plots = True , save_plots = True ) # Access and customize the uncertainty plot fig = figures [ 'uncertainty' ] ax = fig . axes [ 0 ] ax . set_title ( 'GPR Prediction Uncertainty' ) ax . set_xlabel ( 'Sample Index (ordered by input value)' ) # Save the customized figure fig . savefig ( 'custom_uncertainty.png' , dpi = 300 ) # Print summary metrics print ( f \"Performance Summary:\" ) print ( f \"R\u00b2 Score: { tester . test_results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { tester . test_results [ 'rmse' ] : .4f } \" ) Testing Co-Kriging Models \u00b6 Example of testing a Co-Kriging model: from PyEGRO.meta.modeltesting import ModelTester # Create a tester for a Co-Kriging model tester = ModelTester ( model_dir = 'RESULT_MODEL_COKRIGING' , model_name = 'cokriging_model' # Optional, will be inferred if not provided ) # Load the model tester . load_model () # Generate test data (if no actual test data is available) X_test , y_test = tester . load_test_data ( n_samples = 100 , n_features = 2 ) # Evaluate the model tester . evaluate ( X_test , y_test ) # Generate and save plots tester . plot_results ( output_dir = 'cokriging_test_results' , show_plots = True ) # Print summary metrics print ( f \"Co-Kriging Model Performance:\" ) print ( f \"R\u00b2 Score: { tester . test_results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { tester . test_results [ 'rmse' ] : .4f } \" ) print ( f \"MAE: { tester . test_results [ 'mae' ] : .4f } \" ) Working with Custom Test Data \u00b6 Example of loading and using custom test data from a CSV file: from PyEGRO.meta.modeltesting import ModelTester # Create the tester tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR' ) # Load the model tester . load_model () # Load test data from CSV with custom column names X_test , y_test = tester . load_test_data ( data_path = 'my_test_data.csv' , feature_cols = [ 'x1' , 'x2' , 'x3' ], # Specific feature columns to use target_col = 'output' # Target column name ) # Evaluate and save results tester . evaluate ( X_test , y_test ) tester . save_results ( output_dir = 'custom_data_results' ) tester . plot_results () Using the Convenience Function \u00b6 For quick testing, use the load_and_test_model convenience function: from PyEGRO.meta.modeltesting import load_and_test_model # Test a model in one call results = load_and_test_model ( data_path = 'test_data.csv' , model_dir = 'RESULT_MODEL_GPR' , output_dir = 'quick_test_results' , target_col = 'y' , show_plots = True ) # Print metrics print ( f \"Quick Test Results:\" ) print ( f \"R\u00b2 Score: { results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { results [ 'rmse' ] : .4f } \" ) Customizing Plots \u00b6 Example of customizing the generated plots: from PyEGRO.meta.modeltesting import ModelTester import matplotlib.pyplot as plt # Create and evaluate as before tester = ModelTester ( model_path = 'RESULT_MODEL_GPR/gpr_model.pth' ) tester . load_model () X_test , y_test = tester . load_test_data () tester . evaluate ( X_test , y_test ) # Get plot figures without saving them figures = tester . plot_results ( save_plots = False , show_plots = False ) # Customize the prediction vs actual plot pred_fig = figures [ 'predictions' ] ax = pred_fig . axes [ 0 ] ax . set_title ( 'Custom Prediction Plot' ) ax . set_facecolor ( '#f5f5f5' ) # Light gray background ax . grid ( True , linestyle = '--' , alpha = 0.7 ) # Customize the uncertainty plot if 'uncertainty' in figures : uncert_fig = figures [ 'uncertainty' ] ax = uncert_fig . axes [ 0 ] ax . set_title ( 'Custom Uncertainty Visualization' ) ax . set_facecolor ( '#f0f8ff' ) # Light blue background # Add a text annotation r2 = tester . test_results [ 'r2' ] ax . annotate ( f 'R\u00b2 = { r2 : .4f } ' , xy = ( 0.05 , 0.95 ), xycoords = 'axes fraction' , fontsize = 12 , ha = 'left' , va = 'top' , bbox = dict ( boxstyle = 'round,pad=0.5' , fc = 'yellow' , alpha = 0.3 )) # Save customized figures pred_fig . savefig ( 'custom_predictions.png' , dpi = 300 , bbox_inches = 'tight' ) if 'uncertainty' in figures : uncert_fig . savefig ( 'custom_uncertainty.png' , dpi = 300 , bbox_inches = 'tight' ) # Show all figures plt . show () Command Line Interface \u00b6 The ModelTesting module can also be used from the command line: # Test a model from a specific directory python -m PyEGRO.meta.modeltesting --model-dir RESULT_MODEL_GPR --data test_data.csv --output test_results # Directly specify the model file python -m PyEGRO.meta.modeltesting --model-path RESULT_MODEL_GPR/gpr_model.pth --data test_data.csv # Specify a different target column python -m PyEGRO.meta.modeltesting --model-dir RESULT_MODEL_GPR --data test_data.csv --target output_value # Don't show plots (only save them) python -m PyEGRO.meta.modeltesting --model-dir RESULT_MODEL_GPR --data test_data.csv --no-plot Integration with Logging \u00b6 Example of integrating with Python's logging system: import logging from PyEGRO.meta.modeltesting import ModelTester # Configure logging logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' , handlers = [ logging . FileHandler ( \"model_testing.log\" ), logging . StreamHandler () ] ) logger = logging . getLogger ( \"model_tester\" ) # Create tester with logger tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR' , logger = logger ) # Now all operations will be logged tester . load_model () X_test , y_test = tester . load_test_data () tester . evaluate ( X_test , y_test ) tester . save_results () Advanced Usage \u00b6 Comparing Multiple Models \u00b6 Example of comparing multiple models on the same test data: import numpy as np import pandas as pd import matplotlib.pyplot as plt from PyEGRO.meta.modeltesting import ModelTester # Generate test data n_samples = 100 n_features = 2 X_test = np . random . rand ( n_samples , n_features ) * 10 y_test = np . sin ( X_test [:, 0 ]) * np . cos ( X_test [:, 1 ]) + 0.1 * np . random . randn ( n_samples ) y_test = y_test . reshape ( - 1 , 1 ) # Models to compare model_dirs = [ 'RESULT_MODEL_GPR_RBF' , 'RESULT_MODEL_GPR_MATERN' , 'RESULT_MODEL_COKRIGING' ] # Store results results = {} # Test each model for model_dir in model_dirs : # Extract model name from directory model_name = model_dir . split ( '_' )[ - 1 ] . lower () # Create tester tester = ModelTester ( model_dir = model_dir ) # Load and evaluate tester . load_model () tester . evaluate ( X_test , y_test ) # Store results results [ model_name ] = { 'r2' : tester . test_results [ 'r2' ], 'rmse' : tester . test_results [ 'rmse' ], 'mae' : tester . test_results [ 'mae' ] } # Save individual results tester . save_results ( output_dir = 'comparison_results' ) # Create comparison bar chart metrics = [ 'r2' , 'rmse' , 'mae' ] fig , axes = plt . subplots ( 1 , 3 , figsize = ( 15 , 5 )) for i , metric in enumerate ( metrics ): ax = axes [ i ] # Extract values for this metric values = [ results [ model ][ metric ] for model in results ] # Create bar chart ax . bar ( list ( results . keys ()), values ) ax . set_title ( f ' { metric . upper () } Comparison' ) ax . set_ylim ( 0 , max ( values ) * 1.2 ) # Add value labels on bars for j , v in enumerate ( values ): ax . text ( j , v + 0.01 , f ' { v : .4f } ' , ha = 'center' ) plt . tight_layout () plt . savefig ( 'model_comparison.png' , dpi = 300 ) plt . show () # Create comparison table comparison_df = pd . DataFrame ( results ) . T comparison_df . columns = [ 'R\u00b2' , 'RMSE' , 'MAE' ] print ( comparison_df ) comparison_df . to_csv ( 'model_comparison.csv' ) Custom Model Evaluation Workflow \u00b6 Example of a custom evaluation workflow using the ModelTester: from PyEGRO.meta.modeltesting import ModelTester import numpy as np import pandas as pd from sklearn.model_selection import KFold import matplotlib.pyplot as plt # Load your dataset data = pd . read_csv ( 'full_dataset.csv' ) X = data . drop ( 'target' , axis = 1 ) . values y = data [ 'target' ] . values . reshape ( - 1 , 1 ) # Setup cross-validation kf = KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) fold_results = [] # Path to model model_path = 'RESULT_MODEL_GPR/gpr_model.pth' # Loop through folds for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( f \"Evaluating fold { fold + 1 } /5\" ) # Split data X_test_fold = X [ test_idx ] y_test_fold = y [ test_idx ] # Create tester tester = ModelTester ( model_path = model_path ) tester . load_model () # Evaluate on this fold tester . evaluate ( X_test_fold , y_test_fold ) # Store results for this fold fold_results . append ({ 'fold' : fold + 1 , 'r2' : tester . test_results [ 'r2' ], 'rmse' : tester . test_results [ 'rmse' ], 'mae' : tester . test_results [ 'mae' ], 'n_samples' : len ( test_idx ) }) # Save plots for this fold tester . plot_results ( output_dir = f 'cv_results/fold_ { fold + 1 } ' ) # Create summary DataFrame results_df = pd . DataFrame ( fold_results ) print ( results_df ) # Calculate average metrics avg_metrics = { 'r2_mean' : results_df [ 'r2' ] . mean (), 'r2_std' : results_df [ 'r2' ] . std (), 'rmse_mean' : results_df [ 'rmse' ] . mean (), 'rmse_std' : results_df [ 'rmse' ] . std (), 'mae_mean' : results_df [ 'mae' ] . mean (), 'mae_std' : results_df [ 'mae' ] . std () } # Print summary print ( \" \\n Cross-Validation Results:\" ) print ( f \"R\u00b2 Score: { avg_metrics [ 'r2_mean' ] : .4f } \u00b1 { avg_metrics [ 'r2_std' ] : .4f } \" ) print ( f \"RMSE: { avg_metrics [ 'rmse_mean' ] : .4f } \u00b1 { avg_metrics [ 'rmse_std' ] : .4f } \" ) print ( f \"MAE: { avg_metrics [ 'mae_mean' ] : .4f } \u00b1 { avg_metrics [ 'mae_std' ] : .4f } \" ) # Save summary with open ( 'cv_results/summary.txt' , 'w' ) as f : f . write ( \"Cross-Validation Results: \\n \" ) f . write ( f \"R\u00b2 Score: { avg_metrics [ 'r2_mean' ] : .4f } \u00b1 { avg_metrics [ 'r2_std' ] : .4f } \\n \" ) f . write ( f \"RMSE: { avg_metrics [ 'rmse_mean' ] : .4f } \u00b1 { avg_metrics [ 'rmse_std' ] : .4f } \\n \" ) f . write ( f \"MAE: { avg_metrics [ 'mae_mean' ] : .4f } \u00b1 { avg_metrics [ 'mae_std' ] : .4f } \\n \" )","title":"Model Evaluation"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#pyegro-modeltesting-examples","text":"This document provides practical examples of using the PyEGRO ModelTesting module to evaluate trained GPR and Co-Kriging models on unseen data.","title":"PyEGRO ModelTesting Examples"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#table-of-contents","text":"Introduction Installation Basic Usage Testing GPR Models Testing Co-Kriging Models Working with Custom Test Data Customizing Plots Command Line Interface Integration with Logging Advanced Usage","title":"Table of Contents"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#introduction","text":"The ModelTesting module provides tools to evaluate trained surrogate models (GPR and Co-Kriging) on unseen data. It offers functionality for: Loading trained models saved by the PyEGRO training modules Loading or generating test data Computing performance metrics (R\u00b2, RMSE, MAE) Visualizing model predictions and uncertainty Saving evaluation results and plots","title":"Introduction"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#installation","text":"The PyEGRO ModelTesting module depends on the following packages: - numpy - pandas - scikit-learn - matplotlib - scipy - torch - gpytorch - joblib These should already be installed if you're using the PyEGRO GPR or Co-Kriging modules.","title":"Installation"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#basic-usage","text":"Here's a minimal example of testing a model using the ModelTesting module: from PyEGRO.meta.modeltesting import ModelTester # Initialize the tester with the path to your trained model tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR' ) # Load the model tester . load_model () # Load or generate test data X_test , y_test = tester . load_test_data ( data_path = 'test_data.csv' ) # Evaluate the model tester . evaluate ( X_test , y_test ) # Save results and generate plots tester . save_results ( output_dir = 'test_results' ) tester . plot_results ( output_dir = 'test_results' ) # Print metrics results = tester . test_results print ( f \"R\u00b2 Score: { results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { results [ 'rmse' ] : .4f } \" ) print ( f \"MAE: { results [ 'mae' ] : .4f } \" )","title":"Basic Usage"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#testing-gpr-models","text":"Example of testing a GPR model with synthetic data: from PyEGRO.meta.modeltesting import ModelTester import numpy as np import matplotlib.pyplot as plt # Create a tester for a GPR model tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR_SYNTHETIC' , model_name = 'gpr_model' # Optional, will be inferred if not provided ) # Load the model tester . load_model () # Create synthetic test data for 1D function n_samples = 50 X_test = np . linspace ( 0 , 10 , n_samples ) . reshape ( - 1 , 1 ) y_true = X_test * np . sin ( X_test ) + 0.1 * np . random . randn ( n_samples , 1 ) # Evaluate the model tester . evaluate ( X_test , y_true ) # Generate and display plots figures = tester . plot_results ( output_dir = 'test_results' , show_plots = True , save_plots = True ) # Access and customize the uncertainty plot fig = figures [ 'uncertainty' ] ax = fig . axes [ 0 ] ax . set_title ( 'GPR Prediction Uncertainty' ) ax . set_xlabel ( 'Sample Index (ordered by input value)' ) # Save the customized figure fig . savefig ( 'custom_uncertainty.png' , dpi = 300 ) # Print summary metrics print ( f \"Performance Summary:\" ) print ( f \"R\u00b2 Score: { tester . test_results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { tester . test_results [ 'rmse' ] : .4f } \" )","title":"Testing GPR Models"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#testing-co-kriging-models","text":"Example of testing a Co-Kriging model: from PyEGRO.meta.modeltesting import ModelTester # Create a tester for a Co-Kriging model tester = ModelTester ( model_dir = 'RESULT_MODEL_COKRIGING' , model_name = 'cokriging_model' # Optional, will be inferred if not provided ) # Load the model tester . load_model () # Generate test data (if no actual test data is available) X_test , y_test = tester . load_test_data ( n_samples = 100 , n_features = 2 ) # Evaluate the model tester . evaluate ( X_test , y_test ) # Generate and save plots tester . plot_results ( output_dir = 'cokriging_test_results' , show_plots = True ) # Print summary metrics print ( f \"Co-Kriging Model Performance:\" ) print ( f \"R\u00b2 Score: { tester . test_results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { tester . test_results [ 'rmse' ] : .4f } \" ) print ( f \"MAE: { tester . test_results [ 'mae' ] : .4f } \" )","title":"Testing Co-Kriging Models"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#working-with-custom-test-data","text":"Example of loading and using custom test data from a CSV file: from PyEGRO.meta.modeltesting import ModelTester # Create the tester tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR' ) # Load the model tester . load_model () # Load test data from CSV with custom column names X_test , y_test = tester . load_test_data ( data_path = 'my_test_data.csv' , feature_cols = [ 'x1' , 'x2' , 'x3' ], # Specific feature columns to use target_col = 'output' # Target column name ) # Evaluate and save results tester . evaluate ( X_test , y_test ) tester . save_results ( output_dir = 'custom_data_results' ) tester . plot_results ()","title":"Working with Custom Test Data"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#using-the-convenience-function","text":"For quick testing, use the load_and_test_model convenience function: from PyEGRO.meta.modeltesting import load_and_test_model # Test a model in one call results = load_and_test_model ( data_path = 'test_data.csv' , model_dir = 'RESULT_MODEL_GPR' , output_dir = 'quick_test_results' , target_col = 'y' , show_plots = True ) # Print metrics print ( f \"Quick Test Results:\" ) print ( f \"R\u00b2 Score: { results [ 'r2' ] : .4f } \" ) print ( f \"RMSE: { results [ 'rmse' ] : .4f } \" )","title":"Using the Convenience Function"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#customizing-plots","text":"Example of customizing the generated plots: from PyEGRO.meta.modeltesting import ModelTester import matplotlib.pyplot as plt # Create and evaluate as before tester = ModelTester ( model_path = 'RESULT_MODEL_GPR/gpr_model.pth' ) tester . load_model () X_test , y_test = tester . load_test_data () tester . evaluate ( X_test , y_test ) # Get plot figures without saving them figures = tester . plot_results ( save_plots = False , show_plots = False ) # Customize the prediction vs actual plot pred_fig = figures [ 'predictions' ] ax = pred_fig . axes [ 0 ] ax . set_title ( 'Custom Prediction Plot' ) ax . set_facecolor ( '#f5f5f5' ) # Light gray background ax . grid ( True , linestyle = '--' , alpha = 0.7 ) # Customize the uncertainty plot if 'uncertainty' in figures : uncert_fig = figures [ 'uncertainty' ] ax = uncert_fig . axes [ 0 ] ax . set_title ( 'Custom Uncertainty Visualization' ) ax . set_facecolor ( '#f0f8ff' ) # Light blue background # Add a text annotation r2 = tester . test_results [ 'r2' ] ax . annotate ( f 'R\u00b2 = { r2 : .4f } ' , xy = ( 0.05 , 0.95 ), xycoords = 'axes fraction' , fontsize = 12 , ha = 'left' , va = 'top' , bbox = dict ( boxstyle = 'round,pad=0.5' , fc = 'yellow' , alpha = 0.3 )) # Save customized figures pred_fig . savefig ( 'custom_predictions.png' , dpi = 300 , bbox_inches = 'tight' ) if 'uncertainty' in figures : uncert_fig . savefig ( 'custom_uncertainty.png' , dpi = 300 , bbox_inches = 'tight' ) # Show all figures plt . show ()","title":"Customizing Plots"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#command-line-interface","text":"The ModelTesting module can also be used from the command line: # Test a model from a specific directory python -m PyEGRO.meta.modeltesting --model-dir RESULT_MODEL_GPR --data test_data.csv --output test_results # Directly specify the model file python -m PyEGRO.meta.modeltesting --model-path RESULT_MODEL_GPR/gpr_model.pth --data test_data.csv # Specify a different target column python -m PyEGRO.meta.modeltesting --model-dir RESULT_MODEL_GPR --data test_data.csv --target output_value # Don't show plots (only save them) python -m PyEGRO.meta.modeltesting --model-dir RESULT_MODEL_GPR --data test_data.csv --no-plot","title":"Command Line Interface"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#integration-with-logging","text":"Example of integrating with Python's logging system: import logging from PyEGRO.meta.modeltesting import ModelTester # Configure logging logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' , handlers = [ logging . FileHandler ( \"model_testing.log\" ), logging . StreamHandler () ] ) logger = logging . getLogger ( \"model_tester\" ) # Create tester with logger tester = ModelTester ( model_dir = 'RESULT_MODEL_GPR' , logger = logger ) # Now all operations will be logged tester . load_model () X_test , y_test = tester . load_test_data () tester . evaluate ( X_test , y_test ) tester . save_results ()","title":"Integration with Logging"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#comparing-multiple-models","text":"Example of comparing multiple models on the same test data: import numpy as np import pandas as pd import matplotlib.pyplot as plt from PyEGRO.meta.modeltesting import ModelTester # Generate test data n_samples = 100 n_features = 2 X_test = np . random . rand ( n_samples , n_features ) * 10 y_test = np . sin ( X_test [:, 0 ]) * np . cos ( X_test [:, 1 ]) + 0.1 * np . random . randn ( n_samples ) y_test = y_test . reshape ( - 1 , 1 ) # Models to compare model_dirs = [ 'RESULT_MODEL_GPR_RBF' , 'RESULT_MODEL_GPR_MATERN' , 'RESULT_MODEL_COKRIGING' ] # Store results results = {} # Test each model for model_dir in model_dirs : # Extract model name from directory model_name = model_dir . split ( '_' )[ - 1 ] . lower () # Create tester tester = ModelTester ( model_dir = model_dir ) # Load and evaluate tester . load_model () tester . evaluate ( X_test , y_test ) # Store results results [ model_name ] = { 'r2' : tester . test_results [ 'r2' ], 'rmse' : tester . test_results [ 'rmse' ], 'mae' : tester . test_results [ 'mae' ] } # Save individual results tester . save_results ( output_dir = 'comparison_results' ) # Create comparison bar chart metrics = [ 'r2' , 'rmse' , 'mae' ] fig , axes = plt . subplots ( 1 , 3 , figsize = ( 15 , 5 )) for i , metric in enumerate ( metrics ): ax = axes [ i ] # Extract values for this metric values = [ results [ model ][ metric ] for model in results ] # Create bar chart ax . bar ( list ( results . keys ()), values ) ax . set_title ( f ' { metric . upper () } Comparison' ) ax . set_ylim ( 0 , max ( values ) * 1.2 ) # Add value labels on bars for j , v in enumerate ( values ): ax . text ( j , v + 0.01 , f ' { v : .4f } ' , ha = 'center' ) plt . tight_layout () plt . savefig ( 'model_comparison.png' , dpi = 300 ) plt . show () # Create comparison table comparison_df = pd . DataFrame ( results ) . T comparison_df . columns = [ 'R\u00b2' , 'RMSE' , 'MAE' ] print ( comparison_df ) comparison_df . to_csv ( 'model_comparison.csv' )","title":"Comparing Multiple Models"},{"location":"basic-usage/meta/model-evalulation/modeltesting_examples/#custom-model-evaluation-workflow","text":"Example of a custom evaluation workflow using the ModelTester: from PyEGRO.meta.modeltesting import ModelTester import numpy as np import pandas as pd from sklearn.model_selection import KFold import matplotlib.pyplot as plt # Load your dataset data = pd . read_csv ( 'full_dataset.csv' ) X = data . drop ( 'target' , axis = 1 ) . values y = data [ 'target' ] . values . reshape ( - 1 , 1 ) # Setup cross-validation kf = KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) fold_results = [] # Path to model model_path = 'RESULT_MODEL_GPR/gpr_model.pth' # Loop through folds for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( f \"Evaluating fold { fold + 1 } /5\" ) # Split data X_test_fold = X [ test_idx ] y_test_fold = y [ test_idx ] # Create tester tester = ModelTester ( model_path = model_path ) tester . load_model () # Evaluate on this fold tester . evaluate ( X_test_fold , y_test_fold ) # Store results for this fold fold_results . append ({ 'fold' : fold + 1 , 'r2' : tester . test_results [ 'r2' ], 'rmse' : tester . test_results [ 'rmse' ], 'mae' : tester . test_results [ 'mae' ], 'n_samples' : len ( test_idx ) }) # Save plots for this fold tester . plot_results ( output_dir = f 'cv_results/fold_ { fold + 1 } ' ) # Create summary DataFrame results_df = pd . DataFrame ( fold_results ) print ( results_df ) # Calculate average metrics avg_metrics = { 'r2_mean' : results_df [ 'r2' ] . mean (), 'r2_std' : results_df [ 'r2' ] . std (), 'rmse_mean' : results_df [ 'rmse' ] . mean (), 'rmse_std' : results_df [ 'rmse' ] . std (), 'mae_mean' : results_df [ 'mae' ] . mean (), 'mae_std' : results_df [ 'mae' ] . std () } # Print summary print ( \" \\n Cross-Validation Results:\" ) print ( f \"R\u00b2 Score: { avg_metrics [ 'r2_mean' ] : .4f } \u00b1 { avg_metrics [ 'r2_std' ] : .4f } \" ) print ( f \"RMSE: { avg_metrics [ 'rmse_mean' ] : .4f } \u00b1 { avg_metrics [ 'rmse_std' ] : .4f } \" ) print ( f \"MAE: { avg_metrics [ 'mae_mean' ] : .4f } \u00b1 { avg_metrics [ 'mae_std' ] : .4f } \" ) # Save summary with open ( 'cv_results/summary.txt' , 'w' ) as f : f . write ( \"Cross-Validation Results: \\n \" ) f . write ( f \"R\u00b2 Score: { avg_metrics [ 'r2_mean' ] : .4f } \u00b1 { avg_metrics [ 'r2_std' ] : .4f } \\n \" ) f . write ( f \"RMSE: { avg_metrics [ 'rmse_mean' ] : .4f } \u00b1 { avg_metrics [ 'rmse_std' ] : .4f } \\n \" ) f . write ( f \"MAE: { avg_metrics [ 'mae_mean' ] : .4f } \u00b1 { avg_metrics [ 'mae_std' ] : .4f } \\n \" )","title":"Custom Model Evaluation Workflow"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/","text":"PyEGRO.robustopt.method_mcs Usage Examples \u00b6 This document provides examples of how to use the PyEGRO.robustopt.method_mcs module for robust optimization using Monte Carlo Simulation (MCS) for uncertainty quantification. Table of Contents \u00b6 Quick Start Variable Definition Methods Basic Robust Optimization Custom Reference Point for HV Example Functions Gaussian Peaks Function Aerospace Wing Design Example Manufacturing Process Optimization Problem with Mixed Variables Using Metamodels Quick Start \u00b6 Variable Definition Methods \u00b6 PyEGRO supports two different methods for defining variables in robust optimization problems: 1. Dictionary Format \u00b6 data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] } 2. Loading from JSON File \u00b6 import json # Load existing problem definition with open ( 'data_info.json' , 'r' ) as f : data_info = json . load ( f ) Basic Robust Optimization \u00b6 In this example, we perform robust optimization on a simple test function with uncertain design variables. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a test function for demonstration def branin ( X ): \"\"\"Simple branin function with interaction terms.\"\"\" x1 , x2 = X [:, 0 ], X [:, 1 ] a = 1.0 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) return a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # Define the problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 10 ], 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' # Normal distribution around design point }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 15 ], 'cov' : 0.05 , # 5% coefficient of variation 'distribution' : 'normal' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = branin , mcs_samples = 5000 , # Number of Monte Carlo samples pop_size = 50 , # Population size for NSGA-II n_gen = 30 , # Number of generations show_info = True # Display progress ) # Save and visualize results save_optimization_results ( results , data_info , save_dir = 'RESULT_TEST_FUNCTION' ) # Print sample robust solution pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Get a balanced solution (mid-point of Pareto front) idx = len ( pareto_front ) // 2 balanced_solution = pareto_set [ idx ] mean_perf = pareto_front [ idx , 0 ] stddev = pareto_front [ idx , 1 ] print ( \" \\n Balanced Robust Solution:\" ) print ( f \"x1 = { balanced_solution [ 0 ] : .4f } , x2 = { balanced_solution [ 1 ] : .4f } \" ) print ( f \"Mean Performance = { mean_perf : .4f } , StdDev = { stddev : .4f } \" ) Custom Reference Point for HV \u00b6 This example demonstrates using a custom reference point for the hypervolume (HV) indicator. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define test function def test_function ( X ): \"\"\"Sphere function with environmental noise.\"\"\" x = X [:, 0 : 2 ] # Design variables e = X [:, 2 ] # Environmental variable # Calculate base function f_base = np . sum ( x ** 2 , axis = 1 ) # Add effect of environmental variable f = f_base + 0.2 * e * ( x [:, 0 ] + x [:, 1 ]) return f # Define problem using dictionary format data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] } # Set custom reference point based on preliminary analysis # This should be worse than the worst expected solution in both objectives reference_point = np . array ([ 50.0 , 10.0 ]) # Run optimization with custom reference point results = run_robust_optimization ( data_info = data_info , true_func = test_function , mcs_samples = 5000 , pop_size = 50 , n_gen = 30 , metric = 'hv' , reference_point = reference_point ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_CUSTOM_REFERENCE' ) # Print hypervolume convergence print ( \" \\n Hypervolume Convergence:\" ) print ( \"-\" * 30 ) hv_values = results [ 'convergence_history' ][ 'metric_values' ] iterations = range ( 1 , len ( hv_values ) + 1 ) print ( f \" { 'Iteration' : <10 } | { 'Hypervolume' : <15 } \" ) print ( \"-\" * 30 ) for i , hv in zip ( iterations [:: 5 ], hv_values [:: 5 ]): # Print every 5th value print ( f \" { i : <10 } | { hv : <15.6f } \" ) print ( f \" \\n Final hypervolume: { hv_values [ - 1 ] : .6f } \" ) print ( f \"Used reference point: { results [ 'convergence_history' ][ 'reference_point' ] } \" ) Example Functions \u00b6 Gaussian Peaks Function \u00b6 This example uses a 2D function with two Gaussian peaks, where one peak is higher but more sensitive to parameter variations. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define 2D test function with two Gaussian peaks def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] # Parameters for two Gaussian peaks a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 # Lower and more sensitive a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 # Higher and more robust # Calculate negative sum of two Gaussian peaks f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f # Define problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , # Higher uncertainty to highlight robust solutions 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , 'description' : 'Second design variable' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = objective_func_2D , mcs_samples = 8000 , pop_size = 100 , n_gen = 50 , show_info = True ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_GAUSSIAN_PEAKS' ) # Analyze results - find best performing and most robust solutions pareto_front = results [ 'pareto_front' ] pareto_set = results [ 'pareto_set' ] idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( \" \\n Gaussian Peaks Optimization Results:\" ) print ( \"-\" * 50 ) print ( f \"Best Performance Solution: x1= { pareto_set [ idx_best_perf , 0 ] : .4f } , x2= { pareto_set [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_best_perf , 0 ] : .4f } , StdDev= { pareto_front [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Most Robust Solution: x1= { pareto_set [ idx_most_robust , 0 ] : .4f } , x2= { pareto_set [ idx_most_robust , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_most_robust , 0 ] : .4f } , StdDev= { pareto_front [ idx_most_robust , 1 ] : .4f } \" ) Aerospace Wing Design Example \u00b6 This example demonstrates a more complex application to aerospace wing design optimization. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a simplified wing performance model def wing_model ( X ): \"\"\" Simplified model for wing performance calculation. Returns a combined metric of drag and weight (lower is better). \"\"\" # Design variables aspect_ratio = X [:, 0 ] # Wing aspect ratio sweep_angle = X [:, 1 ] # Sweep angle (degrees) thickness_ratio = X [:, 2 ] # Thickness-to-chord ratio # Environmental variables mach_number = X [:, 3 ] # Cruise Mach number altitude = X [:, 4 ] # Cruise altitude (m) # Simplified physics calculations # Wave drag increases with Mach and thickness, decreases with sweep wave_drag = 0.01 * mach_number ** 2 * thickness_ratio / np . cos ( np . radians ( sweep_angle )) # Induced drag decreases with aspect ratio induced_drag = 1.0 / ( np . pi * aspect_ratio ) # Profile drag increases with thickness profile_drag = 0.005 + 0.01 * thickness_ratio # Total drag total_drag = wave_drag + induced_drag + profile_drag # Structural weight increases with aspect ratio and decreases with sweep and thickness structural_weight = ( 1.5 * aspect_ratio ** 1.5 ) / ( 1 + 0.5 * np . sin ( np . radians ( sweep_angle ))) / thickness_ratio ** 0.5 # Total performance metric (weighted sum of drag and weight) performance = 10 * total_drag + structural_weight return performance # Define the robust wing design problem data_info = { 'variables' : [ # Design variables { 'name' : 'aspect_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 6 , 12 ], 'cov' : 0.02 , # Manufacturing tolerance 'description' : 'Wing aspect ratio' }, { 'name' : 'sweep_angle' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 20 , 40 ], # degrees 'cov' : 0.01 , # Manufacturing tolerance 'description' : 'Wing sweep angle' }, { 'name' : 'thickness_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.08 , 0.16 ], 'cov' : 0.03 , # Manufacturing tolerance 'description' : 'Thickness-to-chord ratio' }, # Environmental variables (operating conditions) { 'name' : 'mach_number' , 'vars_type' : 'env_vars' , 'distribution' : 'triangular' , 'low' : 0.75 , # Minimum Mach 'high' : 0.85 , # Maximum Mach 'mode' : 0.78 , # Most common Mach 'description' : 'Cruise Mach number' }, { 'name' : 'altitude' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 11000 , # 11,000 m (typical cruise altitude) 'cov' : 0.1 , # CoV 10% 'description' : 'Cruise altitude' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = wing_model , mcs_samples = 8000 , pop_size = 100 , n_gen = 80 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_WING_DESIGN' ) # Print results summary pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] print ( \" \\n Wing Design Optimization Results:\" ) print ( \"-\" * 50 ) # Extract key solutions idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( f \" { 'Solution' : <15 } | { 'Aspect Ratio' : <12 } | { 'Sweep Angle' : <12 } | { 'Thickness' : <10 } | { 'Performance' : <12 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 85 ) print ( f \" { 'Best Performance' : <15 } | { pareto_set [ idx_best_perf , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_best_perf , 1 ] : <12.2f } | { pareto_set [ idx_best_perf , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_best_perf , 0 ] : <12.4f } | { pareto_front [ idx_best_perf , 1 ] : <10.4f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <12.2f } | { pareto_set [ idx_most_robust , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <12.4f } | { pareto_front [ idx_most_robust , 1 ] : <10.4f } \" ) Manufacturing Process Optimization \u00b6 This example demonstrates optimizing a manufacturing process with multiple sources of uncertainty. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a manufacturing process model def manufacturing_process ( X ): \"\"\" Model of a manufacturing process with multiple parameters. Returns the total cost (to be minimized). \"\"\" # Process parameters (design variables) temp = X [:, 0 ] # Process temperature time = X [:, 1 ] # Process time pressure = X [:, 2 ] # Process pressure # Material properties (environmental variables) material_density = X [:, 3 ] material_purity = X [:, 4 ] # Machine variations (environmental variables) machine_bias = X [:, 5 ] # Quality calculation base_quality = ( - 0.1 * ( temp - 350 ) ** 2 - # Temperature effect 0.05 * ( time - 60 ) ** 2 - # Time effect 0.02 * ( pressure - 50 ) ** 2 # Pressure effect ) # Material effects on quality material_effect = 20 * material_purity - 0.001 * material_density # Machine bias effect machine_effect = - 5 * machine_bias # Final quality (higher is better) quality = base_quality + material_effect + machine_effect # Cost calculation energy_cost = 0.01 * temp * time material_cost = 0.1 * material_density # Time-based costs labor_cost = 0.5 * time # Total cost total_cost = energy_cost + material_cost + labor_cost - 0.2 * quality return total_cost # Define the robust optimization problem data_info = { 'variables' : [ # Process parameters (design variables) { 'name' : 'temperature' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 300 , 400 ], # degrees C 'cov' : 0.02 , # 2% control uncertainty 'description' : 'Process temperature' }, { 'name' : 'time' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 90 ], # minutes 'cov' : 0.03 , # 3% control uncertainty 'description' : 'Process time' }, { 'name' : 'pressure' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 70 ], # units 'cov' : 0.01 , # 1% control uncertainty 'description' : 'Process pressure' }, # Material properties (environmental variables) { 'name' : 'material_density' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2000 , 'std' : 50 , 'description' : 'Material density' }, { 'name' : 'material_purity' , 'vars_type' : 'env_vars' , 'distribution' : 'beta' , 'alpha' : 9 , # Shape parameter 'beta' : 1 , # Shape parameter 'low' : 0.9 , 'high' : 1.0 , # Range for scaling 'description' : 'Material purity' }, # Machine variation (environmental variable) { 'name' : 'machine_bias' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.1 , 'description' : 'Machine bias' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = manufacturing_process , mcs_samples = 5000 , pop_size = 100 , n_gen = 50 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_MANUFACTURING' ) # Print interesting solutions print ( \" \\n Manufacturing Process Optimization Results:\" ) print ( \"-\" * 60 ) pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Find solutions idx_best_cost = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) idx_balanced = np . argmin ( pareto_front [:, 0 ] * 0.7 + pareto_front [:, 1 ] * 0.3 ) # Weighted balance print ( f \" { 'Solution' : <15 } | { 'Temp (\u00b0C)' : <10 } | { 'Time (min)' : <10 } | { 'Pressure' : <10 } | { 'Cost' : <10 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 72 ) print ( f \" { 'Lowest Cost' : <15 } | { pareto_set [ idx_best_cost , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_best_cost , 1 ] : <10.1f } | { pareto_set [ idx_best_cost , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_best_cost , 0 ] : <10.2f } | { pareto_front [ idx_best_cost , 1 ] : <10.2f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <10.1f } | { pareto_set [ idx_most_robust , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <10.2f } | { pareto_front [ idx_most_robust , 1 ] : <10.2f } \" ) print ( f \" { 'Balanced' : <15 } | { pareto_set [ idx_balanced , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_balanced , 1 ] : <10.1f } | { pareto_set [ idx_balanced , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_balanced , 0 ] : <10.2f } | { pareto_front [ idx_balanced , 1 ] : <10.2f } \" ) Problem with Mixed Variables \u00b6 This example demonstrates a problem with both continuous and discrete variables. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a mixed-variable optimization problem def portfolio_problem ( X ): \"\"\" Portfolio optimization problem with mixed variables. Returns the negative expected return (to be minimized). \"\"\" # Investment allocations (continuous design variables) stock_allocation = X [:, 0 ] # Allocation to stocks bond_allocation = X [:, 1 ] # Allocation to bonds cash_allocation = X [:, 2 ] # Allocation to cash # Asset selection (discrete design variables, but handled as continuous) # In real implementation, these would be rounded to integers stock_selection = X [:, 3 ] # Stock selection (1-5) bond_selection = X [:, 4 ] # Bond selection (1-3) # Market conditions (environmental variables) market_return = X [:, 5 ] # Overall market return interest_rate = X [:, 6 ] # Interest rate inflation = X [:, 7 ] # Inflation rate # Expected returns based on selections (simplified model) stock_returns = [ 0.08 , 0.09 , 0.10 , 0.12 , 0.15 ] # Expected returns for different stocks bond_returns = [ 0.03 , 0.04 , 0.05 ] # Expected returns for different bonds # Get expected returns based on selections # Linear interpolation for continuous representation of discrete choices stock_idx = np . clip ( stock_selection - 1 , 0 , 3.999 ) stock_idx_low = np . floor ( stock_idx ) . astype ( int ) stock_idx_high = np . ceil ( stock_idx ) . astype ( int ) stock_frac = stock_idx - stock_idx_low stock_return = ( 1 - stock_frac ) * np . array ([ stock_returns [ i ] for i in stock_idx_low ]) + \\ stock_frac * np . array ([ stock_returns [ i ] for i in stock_idx_high ]) bond_idx = np . clip ( bond_selection - 1 , 0 , 1.999 ) bond_idx_low = np . floor ( bond_idx ) . astype ( int ) bond_idx_high = np . ceil ( bond_idx ) . astype ( int ) bond_frac = bond_idx - bond_idx_low bond_return = ( 1 - bond_frac ) * np . array ([ bond_returns [ i ] for i in bond_idx_low ]) + \\ bond_frac * np . array ([ bond_returns [ i ] for i in bond_idx_high ]) # Base return calculation cash_return = 0.01 # Fixed cash return # Market effects on returns stock_return = stock_return * ( 1 + 2 * ( market_return - 0.05 )) # Stocks highly affected by market bond_return = bond_return - 5 * ( interest_rate - 0.03 ) # Bonds affected by interest rates cash_return = cash_return - inflation # Cash affected by inflation # Total portfolio return portfolio_return = ( stock_allocation * stock_return + bond_allocation * bond_return + cash_allocation * cash_return ) # Return negative expected return (for minimization) return - portfolio_return # Define the robust optimization problem data_info = { 'variables' : [ # Continuous allocation variables { 'name' : 'stock_allocation' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.0 , 0.8 ], 'cov' : 0.01 , # Small rebalancing drift 'description' : 'Stock allocation' }, { 'name' : 'bond_allocation' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.0 , 0.8 ], 'cov' : 0.01 , 'description' : 'Bond allocation' }, { 'name' : 'cash_allocation' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.0 , 0.5 ], 'cov' : 0.005 , 'description' : 'Cash allocation' }, # Discrete selection variables (handled as continuous) { 'name' : 'stock_selection' , 'vars_type' : 'design_vars' , 'distribution' : 'fixed' , 'range_bounds' : [ 1 , 5 ], 'cov' : 0.0 , # No uncertainty in selection 'description' : 'Stock selection' }, { 'name' : 'bond_selection' , 'vars_type' : 'design_vars' , 'distribution' : 'fixed' , 'range_bounds' : [ 1 , 3 ], 'cov' : 0.0 , # No uncertainty in selection 'description' : 'Bond selection' }, # Market condition variables { 'name' : 'market_return' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0.05 , # 5% average market return 'std' : 0.10 , # 10% standard deviation 'description' : 'Market return' }, { 'name' : 'interest_rate' , 'vars_type' : 'env_vars' , 'distribution' : 'triangular' , 'low' : 0.01 , # Minimum interest rate 'high' : 0.07 , # Maximum interest rate 'mode' : 0.03 , # Most likely interest rate 'description' : 'Interest rate' }, { 'name' : 'inflation' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0.025 , # 2.5% average inflation 'std' : 0.01 , # 1% standard deviation 'description' : 'Inflation rate' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = portfolio_problem , mcs_samples = 10000 , pop_size = 80 , n_gen = 50 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PORTFOLIO' ) # Print key solutions pareto_front = results [ 'pareto_front' ] pareto_set = results [ 'pareto_set' ] # Find key solutions idx_best_return = np . argmin ( pareto_front [:, 0 ]) idx_lowest_risk = np . argmin ( pareto_front [:, 1 ]) # Post-process solutions for the best return and lowest risk portfolio print ( \" \\n Portfolio Optimization Results:\" ) print ( \"-\" * 50 ) print ( \"Best Return Portfolio:\" ) print ( f \" Stock Allocation: { pareto_set [ idx_best_return , 0 ] : .2f } , Stock Type: { pareto_set [ idx_best_return , 3 ] : .0f } \" ) print ( f \" Bond Allocation: { pareto_set [ idx_best_return , 1 ] : .2f } , Bond Type: { pareto_set [ idx_best_return , 4 ] : .0f } \" ) print ( f \" Cash Allocation: { pareto_set [ idx_best_return , 2 ] : .2f } \" ) print ( f \" Expected Return: { - pareto_front [ idx_best_return , 0 ] : .2% } \" ) print ( f \" Risk (StdDev): { pareto_front [ idx_best_return , 1 ] : .2% } \" ) print ( \" \\n Lowest Risk Portfolio:\" ) print ( f \" Stock Allocation: { pareto_set [ idx_lowest_risk , 0 ] : .2f } , Stock Type: { pareto_set [ idx_lowest_risk , 3 ] : .0f } \" ) print ( f \" Bond Allocation: { pareto_set [ idx_lowest_risk , 1 ] : .2f } , Bond Type: { pareto_set [ idx_lowest_risk , 4 ] : .0f } \" ) print ( f \" Cash Allocation: { pareto_set [ idx_lowest_risk , 2 ] : .2f } \" ) print ( f \" Expected Return: { - pareto_front [ idx_lowest_risk , 0 ] : .2% } \" ) print ( f \" Risk (StdDev): { pareto_front [ idx_lowest_risk , 1 ] : .2% } \" ) Using Metamodels \u00b6 When evaluating the original function is computationally expensive, you can use pre-trained surrogate models for efficient robust optimization. PyEGRO supports multiple surrogate model types including Gaussian Process Regression (GPR) and Cokriging. Using GPR Models \u00b6 #========================================================================== # Using Surrogate Model - GPR #========================================================================== from PyEGRO.meta.gpr import gpr_utils from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results import json print ( \" \\n Using GPR Surrogate Model\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize GPR handler (**GPR need to be trained) gpr_handler = gpr_utils . DeviceAgnosticGPR ( prefer_gpu = True ) gpr_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = gpr_handler , data_info = data_info , mcs_samples = 10000 , pop_size = 25 , n_gen = 100 , metric = 'hv' ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'MCS_RESULT_GPR' ) Using Cokriging Models \u00b6 Cokriging models are particularly useful when you have multi-fidelity data or multiple correlated responses. #========================================================================== # Using Surrogate Model - Cokriging #========================================================================== from PyEGRO.meta.cokriging import cokriging_utils from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results import json print ( \" \\n Using Cokriging Surrogate Model\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize Cokriging handler (**Cokriging model need to be trained) cokriging_handler = cokriging_utils . DeviceAgnosticCoKriging ( prefer_gpu = True ) cokriging_handler . load_model ( 'RESULT_MODEL_COKRIGING' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = cokriging_handler , data_info = data_info , mcs_samples = 10000 , pop_size = 25 , n_gen = 100 , metric = 'hv' , reference_point = [ 5 , 5 ] # Custom reference point for HV calculation ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'MCS_RESULT_CK' ) The metamodel approach is particularly useful when: The original function is computationally expensive You need to run multiple robust optimizations with different settings You already have a trained surrogate model from previous analyses You want to leverage multi-fidelity data (especially for Cokriging) First, the surrogate model needs to be trained separately using an appropriate DOE (Design of Experiments) and training algorithm. Once trained and saved, it can be loaded for robust optimization as shown in the examples above. This approach significantly reduces computational time while still providing good approximations of the robustness metrics.","title":"MCS Approach"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#pyegrorobustoptmethod_mcs-usage-examples","text":"This document provides examples of how to use the PyEGRO.robustopt.method_mcs module for robust optimization using Monte Carlo Simulation (MCS) for uncertainty quantification.","title":"PyEGRO.robustopt.method_mcs Usage Examples"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#table-of-contents","text":"Quick Start Variable Definition Methods Basic Robust Optimization Custom Reference Point for HV Example Functions Gaussian Peaks Function Aerospace Wing Design Example Manufacturing Process Optimization Problem with Mixed Variables Using Metamodels","title":"Table of Contents"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#quick-start","text":"","title":"Quick Start"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#variable-definition-methods","text":"PyEGRO supports two different methods for defining variables in robust optimization problems:","title":"Variable Definition Methods"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#basic-robust-optimization","text":"In this example, we perform robust optimization on a simple test function with uncertain design variables. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a test function for demonstration def branin ( X ): \"\"\"Simple branin function with interaction terms.\"\"\" x1 , x2 = X [:, 0 ], X [:, 1 ] a = 1.0 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) return a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # Define the problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 10 ], 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' # Normal distribution around design point }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 15 ], 'cov' : 0.05 , # 5% coefficient of variation 'distribution' : 'normal' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = branin , mcs_samples = 5000 , # Number of Monte Carlo samples pop_size = 50 , # Population size for NSGA-II n_gen = 30 , # Number of generations show_info = True # Display progress ) # Save and visualize results save_optimization_results ( results , data_info , save_dir = 'RESULT_TEST_FUNCTION' ) # Print sample robust solution pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Get a balanced solution (mid-point of Pareto front) idx = len ( pareto_front ) // 2 balanced_solution = pareto_set [ idx ] mean_perf = pareto_front [ idx , 0 ] stddev = pareto_front [ idx , 1 ] print ( \" \\n Balanced Robust Solution:\" ) print ( f \"x1 = { balanced_solution [ 0 ] : .4f } , x2 = { balanced_solution [ 1 ] : .4f } \" ) print ( f \"Mean Performance = { mean_perf : .4f } , StdDev = { stddev : .4f } \" )","title":"Basic Robust Optimization"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#custom-reference-point-for-hv","text":"This example demonstrates using a custom reference point for the hypervolume (HV) indicator. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define test function def test_function ( X ): \"\"\"Sphere function with environmental noise.\"\"\" x = X [:, 0 : 2 ] # Design variables e = X [:, 2 ] # Environmental variable # Calculate base function f_base = np . sum ( x ** 2 , axis = 1 ) # Add effect of environmental variable f = f_base + 0.2 * e * ( x [:, 0 ] + x [:, 1 ]) return f # Define problem using dictionary format data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] } # Set custom reference point based on preliminary analysis # This should be worse than the worst expected solution in both objectives reference_point = np . array ([ 50.0 , 10.0 ]) # Run optimization with custom reference point results = run_robust_optimization ( data_info = data_info , true_func = test_function , mcs_samples = 5000 , pop_size = 50 , n_gen = 30 , metric = 'hv' , reference_point = reference_point ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_CUSTOM_REFERENCE' ) # Print hypervolume convergence print ( \" \\n Hypervolume Convergence:\" ) print ( \"-\" * 30 ) hv_values = results [ 'convergence_history' ][ 'metric_values' ] iterations = range ( 1 , len ( hv_values ) + 1 ) print ( f \" { 'Iteration' : <10 } | { 'Hypervolume' : <15 } \" ) print ( \"-\" * 30 ) for i , hv in zip ( iterations [:: 5 ], hv_values [:: 5 ]): # Print every 5th value print ( f \" { i : <10 } | { hv : <15.6f } \" ) print ( f \" \\n Final hypervolume: { hv_values [ - 1 ] : .6f } \" ) print ( f \"Used reference point: { results [ 'convergence_history' ][ 'reference_point' ] } \" )","title":"Custom Reference Point for HV"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#example-functions","text":"","title":"Example Functions"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#gaussian-peaks-function","text":"This example uses a 2D function with two Gaussian peaks, where one peak is higher but more sensitive to parameter variations. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define 2D test function with two Gaussian peaks def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] # Parameters for two Gaussian peaks a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 # Lower and more sensitive a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 # Higher and more robust # Calculate negative sum of two Gaussian peaks f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f # Define problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , # Higher uncertainty to highlight robust solutions 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , 'description' : 'Second design variable' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = objective_func_2D , mcs_samples = 8000 , pop_size = 100 , n_gen = 50 , show_info = True ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_GAUSSIAN_PEAKS' ) # Analyze results - find best performing and most robust solutions pareto_front = results [ 'pareto_front' ] pareto_set = results [ 'pareto_set' ] idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( \" \\n Gaussian Peaks Optimization Results:\" ) print ( \"-\" * 50 ) print ( f \"Best Performance Solution: x1= { pareto_set [ idx_best_perf , 0 ] : .4f } , x2= { pareto_set [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_best_perf , 0 ] : .4f } , StdDev= { pareto_front [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Most Robust Solution: x1= { pareto_set [ idx_most_robust , 0 ] : .4f } , x2= { pareto_set [ idx_most_robust , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_most_robust , 0 ] : .4f } , StdDev= { pareto_front [ idx_most_robust , 1 ] : .4f } \" )","title":"Gaussian Peaks Function"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#aerospace-wing-design-example","text":"This example demonstrates a more complex application to aerospace wing design optimization. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a simplified wing performance model def wing_model ( X ): \"\"\" Simplified model for wing performance calculation. Returns a combined metric of drag and weight (lower is better). \"\"\" # Design variables aspect_ratio = X [:, 0 ] # Wing aspect ratio sweep_angle = X [:, 1 ] # Sweep angle (degrees) thickness_ratio = X [:, 2 ] # Thickness-to-chord ratio # Environmental variables mach_number = X [:, 3 ] # Cruise Mach number altitude = X [:, 4 ] # Cruise altitude (m) # Simplified physics calculations # Wave drag increases with Mach and thickness, decreases with sweep wave_drag = 0.01 * mach_number ** 2 * thickness_ratio / np . cos ( np . radians ( sweep_angle )) # Induced drag decreases with aspect ratio induced_drag = 1.0 / ( np . pi * aspect_ratio ) # Profile drag increases with thickness profile_drag = 0.005 + 0.01 * thickness_ratio # Total drag total_drag = wave_drag + induced_drag + profile_drag # Structural weight increases with aspect ratio and decreases with sweep and thickness structural_weight = ( 1.5 * aspect_ratio ** 1.5 ) / ( 1 + 0.5 * np . sin ( np . radians ( sweep_angle ))) / thickness_ratio ** 0.5 # Total performance metric (weighted sum of drag and weight) performance = 10 * total_drag + structural_weight return performance # Define the robust wing design problem data_info = { 'variables' : [ # Design variables { 'name' : 'aspect_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 6 , 12 ], 'cov' : 0.02 , # Manufacturing tolerance 'description' : 'Wing aspect ratio' }, { 'name' : 'sweep_angle' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 20 , 40 ], # degrees 'cov' : 0.01 , # Manufacturing tolerance 'description' : 'Wing sweep angle' }, { 'name' : 'thickness_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.08 , 0.16 ], 'cov' : 0.03 , # Manufacturing tolerance 'description' : 'Thickness-to-chord ratio' }, # Environmental variables (operating conditions) { 'name' : 'mach_number' , 'vars_type' : 'env_vars' , 'distribution' : 'triangular' , 'low' : 0.75 , # Minimum Mach 'high' : 0.85 , # Maximum Mach 'mode' : 0.78 , # Most common Mach 'description' : 'Cruise Mach number' }, { 'name' : 'altitude' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 11000 , # 11,000 m (typical cruise altitude) 'cov' : 0.1 , # CoV 10% 'description' : 'Cruise altitude' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = wing_model , mcs_samples = 8000 , pop_size = 100 , n_gen = 80 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_WING_DESIGN' ) # Print results summary pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] print ( \" \\n Wing Design Optimization Results:\" ) print ( \"-\" * 50 ) # Extract key solutions idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( f \" { 'Solution' : <15 } | { 'Aspect Ratio' : <12 } | { 'Sweep Angle' : <12 } | { 'Thickness' : <10 } | { 'Performance' : <12 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 85 ) print ( f \" { 'Best Performance' : <15 } | { pareto_set [ idx_best_perf , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_best_perf , 1 ] : <12.2f } | { pareto_set [ idx_best_perf , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_best_perf , 0 ] : <12.4f } | { pareto_front [ idx_best_perf , 1 ] : <10.4f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <12.2f } | { pareto_set [ idx_most_robust , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <12.4f } | { pareto_front [ idx_most_robust , 1 ] : <10.4f } \" )","title":"Aerospace Wing Design Example"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#manufacturing-process-optimization","text":"This example demonstrates optimizing a manufacturing process with multiple sources of uncertainty. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a manufacturing process model def manufacturing_process ( X ): \"\"\" Model of a manufacturing process with multiple parameters. Returns the total cost (to be minimized). \"\"\" # Process parameters (design variables) temp = X [:, 0 ] # Process temperature time = X [:, 1 ] # Process time pressure = X [:, 2 ] # Process pressure # Material properties (environmental variables) material_density = X [:, 3 ] material_purity = X [:, 4 ] # Machine variations (environmental variables) machine_bias = X [:, 5 ] # Quality calculation base_quality = ( - 0.1 * ( temp - 350 ) ** 2 - # Temperature effect 0.05 * ( time - 60 ) ** 2 - # Time effect 0.02 * ( pressure - 50 ) ** 2 # Pressure effect ) # Material effects on quality material_effect = 20 * material_purity - 0.001 * material_density # Machine bias effect machine_effect = - 5 * machine_bias # Final quality (higher is better) quality = base_quality + material_effect + machine_effect # Cost calculation energy_cost = 0.01 * temp * time material_cost = 0.1 * material_density # Time-based costs labor_cost = 0.5 * time # Total cost total_cost = energy_cost + material_cost + labor_cost - 0.2 * quality return total_cost # Define the robust optimization problem data_info = { 'variables' : [ # Process parameters (design variables) { 'name' : 'temperature' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 300 , 400 ], # degrees C 'cov' : 0.02 , # 2% control uncertainty 'description' : 'Process temperature' }, { 'name' : 'time' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 90 ], # minutes 'cov' : 0.03 , # 3% control uncertainty 'description' : 'Process time' }, { 'name' : 'pressure' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 70 ], # units 'cov' : 0.01 , # 1% control uncertainty 'description' : 'Process pressure' }, # Material properties (environmental variables) { 'name' : 'material_density' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2000 , 'std' : 50 , 'description' : 'Material density' }, { 'name' : 'material_purity' , 'vars_type' : 'env_vars' , 'distribution' : 'beta' , 'alpha' : 9 , # Shape parameter 'beta' : 1 , # Shape parameter 'low' : 0.9 , 'high' : 1.0 , # Range for scaling 'description' : 'Material purity' }, # Machine variation (environmental variable) { 'name' : 'machine_bias' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.1 , 'description' : 'Machine bias' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = manufacturing_process , mcs_samples = 5000 , pop_size = 100 , n_gen = 50 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_MANUFACTURING' ) # Print interesting solutions print ( \" \\n Manufacturing Process Optimization Results:\" ) print ( \"-\" * 60 ) pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Find solutions idx_best_cost = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) idx_balanced = np . argmin ( pareto_front [:, 0 ] * 0.7 + pareto_front [:, 1 ] * 0.3 ) # Weighted balance print ( f \" { 'Solution' : <15 } | { 'Temp (\u00b0C)' : <10 } | { 'Time (min)' : <10 } | { 'Pressure' : <10 } | { 'Cost' : <10 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 72 ) print ( f \" { 'Lowest Cost' : <15 } | { pareto_set [ idx_best_cost , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_best_cost , 1 ] : <10.1f } | { pareto_set [ idx_best_cost , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_best_cost , 0 ] : <10.2f } | { pareto_front [ idx_best_cost , 1 ] : <10.2f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <10.1f } | { pareto_set [ idx_most_robust , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <10.2f } | { pareto_front [ idx_most_robust , 1 ] : <10.2f } \" ) print ( f \" { 'Balanced' : <15 } | { pareto_set [ idx_balanced , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_balanced , 1 ] : <10.1f } | { pareto_set [ idx_balanced , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_balanced , 0 ] : <10.2f } | { pareto_front [ idx_balanced , 1 ] : <10.2f } \" )","title":"Manufacturing Process Optimization"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#problem-with-mixed-variables","text":"This example demonstrates a problem with both continuous and discrete variables. import numpy as np from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results # Define a mixed-variable optimization problem def portfolio_problem ( X ): \"\"\" Portfolio optimization problem with mixed variables. Returns the negative expected return (to be minimized). \"\"\" # Investment allocations (continuous design variables) stock_allocation = X [:, 0 ] # Allocation to stocks bond_allocation = X [:, 1 ] # Allocation to bonds cash_allocation = X [:, 2 ] # Allocation to cash # Asset selection (discrete design variables, but handled as continuous) # In real implementation, these would be rounded to integers stock_selection = X [:, 3 ] # Stock selection (1-5) bond_selection = X [:, 4 ] # Bond selection (1-3) # Market conditions (environmental variables) market_return = X [:, 5 ] # Overall market return interest_rate = X [:, 6 ] # Interest rate inflation = X [:, 7 ] # Inflation rate # Expected returns based on selections (simplified model) stock_returns = [ 0.08 , 0.09 , 0.10 , 0.12 , 0.15 ] # Expected returns for different stocks bond_returns = [ 0.03 , 0.04 , 0.05 ] # Expected returns for different bonds # Get expected returns based on selections # Linear interpolation for continuous representation of discrete choices stock_idx = np . clip ( stock_selection - 1 , 0 , 3.999 ) stock_idx_low = np . floor ( stock_idx ) . astype ( int ) stock_idx_high = np . ceil ( stock_idx ) . astype ( int ) stock_frac = stock_idx - stock_idx_low stock_return = ( 1 - stock_frac ) * np . array ([ stock_returns [ i ] for i in stock_idx_low ]) + \\ stock_frac * np . array ([ stock_returns [ i ] for i in stock_idx_high ]) bond_idx = np . clip ( bond_selection - 1 , 0 , 1.999 ) bond_idx_low = np . floor ( bond_idx ) . astype ( int ) bond_idx_high = np . ceil ( bond_idx ) . astype ( int ) bond_frac = bond_idx - bond_idx_low bond_return = ( 1 - bond_frac ) * np . array ([ bond_returns [ i ] for i in bond_idx_low ]) + \\ bond_frac * np . array ([ bond_returns [ i ] for i in bond_idx_high ]) # Base return calculation cash_return = 0.01 # Fixed cash return # Market effects on returns stock_return = stock_return * ( 1 + 2 * ( market_return - 0.05 )) # Stocks highly affected by market bond_return = bond_return - 5 * ( interest_rate - 0.03 ) # Bonds affected by interest rates cash_return = cash_return - inflation # Cash affected by inflation # Total portfolio return portfolio_return = ( stock_allocation * stock_return + bond_allocation * bond_return + cash_allocation * cash_return ) # Return negative expected return (for minimization) return - portfolio_return # Define the robust optimization problem data_info = { 'variables' : [ # Continuous allocation variables { 'name' : 'stock_allocation' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.0 , 0.8 ], 'cov' : 0.01 , # Small rebalancing drift 'description' : 'Stock allocation' }, { 'name' : 'bond_allocation' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.0 , 0.8 ], 'cov' : 0.01 , 'description' : 'Bond allocation' }, { 'name' : 'cash_allocation' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.0 , 0.5 ], 'cov' : 0.005 , 'description' : 'Cash allocation' }, # Discrete selection variables (handled as continuous) { 'name' : 'stock_selection' , 'vars_type' : 'design_vars' , 'distribution' : 'fixed' , 'range_bounds' : [ 1 , 5 ], 'cov' : 0.0 , # No uncertainty in selection 'description' : 'Stock selection' }, { 'name' : 'bond_selection' , 'vars_type' : 'design_vars' , 'distribution' : 'fixed' , 'range_bounds' : [ 1 , 3 ], 'cov' : 0.0 , # No uncertainty in selection 'description' : 'Bond selection' }, # Market condition variables { 'name' : 'market_return' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0.05 , # 5% average market return 'std' : 0.10 , # 10% standard deviation 'description' : 'Market return' }, { 'name' : 'interest_rate' , 'vars_type' : 'env_vars' , 'distribution' : 'triangular' , 'low' : 0.01 , # Minimum interest rate 'high' : 0.07 , # Maximum interest rate 'mode' : 0.03 , # Most likely interest rate 'description' : 'Interest rate' }, { 'name' : 'inflation' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0.025 , # 2.5% average inflation 'std' : 0.01 , # 1% standard deviation 'description' : 'Inflation rate' } ] } # Run robust optimization results = run_robust_optimization ( data_info = data_info , true_func = portfolio_problem , mcs_samples = 10000 , pop_size = 80 , n_gen = 50 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PORTFOLIO' ) # Print key solutions pareto_front = results [ 'pareto_front' ] pareto_set = results [ 'pareto_set' ] # Find key solutions idx_best_return = np . argmin ( pareto_front [:, 0 ]) idx_lowest_risk = np . argmin ( pareto_front [:, 1 ]) # Post-process solutions for the best return and lowest risk portfolio print ( \" \\n Portfolio Optimization Results:\" ) print ( \"-\" * 50 ) print ( \"Best Return Portfolio:\" ) print ( f \" Stock Allocation: { pareto_set [ idx_best_return , 0 ] : .2f } , Stock Type: { pareto_set [ idx_best_return , 3 ] : .0f } \" ) print ( f \" Bond Allocation: { pareto_set [ idx_best_return , 1 ] : .2f } , Bond Type: { pareto_set [ idx_best_return , 4 ] : .0f } \" ) print ( f \" Cash Allocation: { pareto_set [ idx_best_return , 2 ] : .2f } \" ) print ( f \" Expected Return: { - pareto_front [ idx_best_return , 0 ] : .2% } \" ) print ( f \" Risk (StdDev): { pareto_front [ idx_best_return , 1 ] : .2% } \" ) print ( \" \\n Lowest Risk Portfolio:\" ) print ( f \" Stock Allocation: { pareto_set [ idx_lowest_risk , 0 ] : .2f } , Stock Type: { pareto_set [ idx_lowest_risk , 3 ] : .0f } \" ) print ( f \" Bond Allocation: { pareto_set [ idx_lowest_risk , 1 ] : .2f } , Bond Type: { pareto_set [ idx_lowest_risk , 4 ] : .0f } \" ) print ( f \" Cash Allocation: { pareto_set [ idx_lowest_risk , 2 ] : .2f } \" ) print ( f \" Expected Return: { - pareto_front [ idx_lowest_risk , 0 ] : .2% } \" ) print ( f \" Risk (StdDev): { pareto_front [ idx_lowest_risk , 1 ] : .2% } \" )","title":"Problem with Mixed Variables"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#using-metamodels","text":"When evaluating the original function is computationally expensive, you can use pre-trained surrogate models for efficient robust optimization. PyEGRO supports multiple surrogate model types including Gaussian Process Regression (GPR) and Cokriging.","title":"Using Metamodels"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#using-gpr-models","text":"#========================================================================== # Using Surrogate Model - GPR #========================================================================== from PyEGRO.meta.gpr import gpr_utils from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results import json print ( \" \\n Using GPR Surrogate Model\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize GPR handler (**GPR need to be trained) gpr_handler = gpr_utils . DeviceAgnosticGPR ( prefer_gpu = True ) gpr_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = gpr_handler , data_info = data_info , mcs_samples = 10000 , pop_size = 25 , n_gen = 100 , metric = 'hv' ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'MCS_RESULT_GPR' )","title":"Using GPR Models"},{"location":"basic-usage/robustopt/approach-mcs/robustopt_mcs_examples/#using-cokriging-models","text":"Cokriging models are particularly useful when you have multi-fidelity data or multiple correlated responses. #========================================================================== # Using Surrogate Model - Cokriging #========================================================================== from PyEGRO.meta.cokriging import cokriging_utils from PyEGRO.robustopt.method_mcs import run_robust_optimization , save_optimization_results import json print ( \" \\n Using Cokriging Surrogate Model\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize Cokriging handler (**Cokriging model need to be trained) cokriging_handler = cokriging_utils . DeviceAgnosticCoKriging ( prefer_gpu = True ) cokriging_handler . load_model ( 'RESULT_MODEL_COKRIGING' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = cokriging_handler , data_info = data_info , mcs_samples = 10000 , pop_size = 25 , n_gen = 100 , metric = 'hv' , reference_point = [ 5 , 5 ] # Custom reference point for HV calculation ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'MCS_RESULT_CK' ) The metamodel approach is particularly useful when: The original function is computationally expensive You need to run multiple robust optimizations with different settings You already have a trained surrogate model from previous analyses You want to leverage multi-fidelity data (especially for Cokriging) First, the surrogate model needs to be trained separately using an appropriate DOE (Design of Experiments) and training algorithm. Once trained and saved, it can be loaded for robust optimization as shown in the examples above. This approach significantly reduces computational time while still providing good approximations of the robustness metrics.","title":"Using Cokriging Models"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/","text":"PyEGRO.robustopt.method_nnmcs Usage Examples \u00b6 This document provides examples of how to use the PyEGRO.robustopt.method_nnmcs module for robust optimization using a two-stage approach that combines Neural Networks (NN) with Monte Carlo Simulation (MCS). Table of Contents \u00b6 Quick Start Basic Configuration Running the Pipeline Customizing Neural Network Training Hyperparameter Ranges Adjusting Training Settings Sampling Strategy Balancing Accuracy and Speed Using Different Surrogate Models With GPR Models With Cokriging Models Full Pipeline Example Quick Start \u00b6 Basic Configuration \u00b6 The first step is to set up the configuration objects for the pipeline: import numpy as np from PyEGRO.robustopt.method_nnmcs import ( RobustOptimization , ANNConfig , SamplingConfig , OptimizationConfig , PathConfig ) # Basic configurations ann_config = ANNConfig () # Default values sampling_config = SamplingConfig () # Default values opt_config = OptimizationConfig () # Default values path_config = PathConfig () # Default values # Initialize the optimizer optimizer = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config ) Running the Pipeline \u00b6 Once configured, you can run the complete pipeline: # Run the full pipeline pareto_set , pareto_front = optimizer . run () # Access optimization results print ( f \"Number of Pareto-optimal solutions: { len ( pareto_set ) } \" ) print ( \" \\n Sample Solutions:\" ) for i in range ( min ( 3 , len ( pareto_set ))): print ( f \"Solution { i + 1 } :\" ) print ( f \" Design variables: { pareto_set [ i ] } \" ) print ( f \" Objectives (Mean, StdDev): { pareto_front [ i ] } \" ) Customizing Neural Network Training \u00b6 Hyperparameter Ranges \u00b6 You can customize the neural network architecture search space: # Custom ANN configuration ann_config = ANNConfig ( n_trials = 100 , # More trials for better hyperparameter optimization n_jobs =- 1 , # Use all available cores hyperparameter_ranges = { 'n_layers' : ( 2 , 5 ), # Network depth 'n_units' : ( 64 , 512 ), # Units per layer 'learning_rate' : ( 1e-5 , 1e-2 ), # Learning rate range 'batch_size' : ( 16 , 128 ), # Batch size range 'max_epochs' : ( 100 , 500 ), # Maximum training epochs 'activations' : [ 'ReLU' , 'LeakyReLU' , 'Tanh' , 'SiLU' ], # Activation functions 'optimizers' : [ 'Adam' , 'AdamW' , 'RMSprop' ], # Optimizers 'loss_functions' : [ 'MSELoss' , 'HuberLoss' , 'L1Loss' ] # Loss functions } ) Adjusting Training Settings \u00b6 You can control the training process: # ANN configuration focused on training stability ann_config = ANNConfig ( n_trials = 50 , n_jobs = 4 , # Limit to 4 parallel jobs hyperparameter_ranges = { 'n_layers' : ( 1 , 3 ), # Simpler networks 'n_units' : ( 32 , 128 ), # Fewer units 'learning_rate' : ( 1e-4 , 1e-3 ), # More conservative learning rates 'batch_size' : ( 32 , 64 ), # Medium batch sizes 'max_epochs' : ( 200 , 300 ), # Moderate training duration 'activations' : [ 'ReLU' ], # Stick to ReLU 'optimizers' : [ 'Adam' ], # Use only Adam 'loss_functions' : [ 'MSELoss' ] # Use only MSE loss } ) Sampling Strategy \u00b6 Balancing Accuracy and Speed \u00b6 You can adjust the sampling configuration to balance computational cost and accuracy: # Fast exploration configuration fast_sampling = SamplingConfig ( n_lhs_samples = 500 , # Fewer design samples n_mcs_samples_low = 1000 , # Low-fidelity MCS for training n_mcs_samples_high = 10000 # Medium-fidelity verification ) # High accuracy configuration accurate_sampling = SamplingConfig ( n_lhs_samples = 5000 , # More design samples n_mcs_samples_low = 10000 , # Higher-fidelity training n_mcs_samples_high = 1000000 # Very high-fidelity verification ) # Balanced configuration balanced_sampling = SamplingConfig ( n_lhs_samples = 2000 , n_mcs_samples_low = 5000 , n_mcs_samples_high = 100000 ) Using Different Metamodels \u00b6 With GPR Models \u00b6 # Configuration for GPR models path_config_gpr = PathConfig ( model_type = 'gpr' , model_dir = 'RESULT_MODEL_GPR' , ann_model_dir = 'RESULT_MODEL_ANN_GPR' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE_GPR' ) # Initialize optimizer with GPR optimizer_gpr = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config_gpr ) # Run optimization with GPR pareto_set_gpr , pareto_front_gpr = optimizer_gpr . run () With Cokriging Models \u00b6 # Configuration for Cokriging models path_config_ck = PathConfig ( model_type = 'cokriging' , model_dir = 'RESULT_MODEL_COKRIGING' , ann_model_dir = 'RESULT_MODEL_ANN_CK' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE_CK' ) # Initialize optimizer with Cokriging optimizer_ck = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config_ck ) # Run optimization with Cokriging pareto_set_ck , pareto_front_ck = optimizer_ck . run () Full Pipeline Example \u00b6 Here's a complete example that demonstrates the entire pipeline with custom configurations: import numpy as np from PyEGRO.robustopt.method_nnmcs import ( RobustOptimization , ANNConfig , SamplingConfig , OptimizationConfig , PathConfig ) # 1. Define Neural Network Configuration ann_config = ANNConfig ( n_trials = 50 , n_jobs =- 1 , hyperparameter_ranges = { 'n_layers' : ( 2 , 4 ), 'n_units' : ( 64 , 256 ), 'learning_rate' : ( 1e-4 , 1e-2 ), 'batch_size' : ( 32 , 64 ), 'max_epochs' : ( 100 , 300 ), 'activations' : [ 'ReLU' , 'LeakyReLU' ], 'optimizers' : [ 'Adam' ], 'loss_functions' : [ 'MSELoss' ] } ) # 2. Define Sampling Configuration sampling_config = SamplingConfig ( n_lhs_samples = 2000 , n_mcs_samples_low = 10000 , n_mcs_samples_high = 500000 , random_seed = 42 ) # 3. Define Optimization Configuration opt_config = OptimizationConfig ( pop_size = 50 , n_gen = 100 , prefer_gpu = True , verbose = True , random_seed = 42 , metric = 'hv' , reference_point = np . array ([ 10.0 , 5.0 ]) ) # 4. Define Paths Configuration path_config = PathConfig ( model_type = 'gpr' , model_dir = 'RESULT_MODEL_GPR' , ann_model_dir = 'RESULT_MODEL_ANN' , data_info_path = 'DATA_PREPARATION/data_info.json' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE' , qoi_dir = 'RESULT_QOI' ) # 5. Initialize the Optimizer optimizer = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config ) # 6. Run the Optimization print ( \"Starting Two-Stage Robust Optimization Pipeline...\" ) pareto_set , pareto_front = optimizer . run () # 7. Process and Display Results print ( \" \\n Optimization Results:\" ) print ( \"-\" * 50 ) print ( f \"Number of Pareto-optimal solutions: { len ( pareto_set ) } \" ) # Get the best mean solution idx_best_mean = np . argmin ( pareto_front [:, 0 ]) print ( f \" \\n Best Mean Solution:\" ) print ( f \" Design Variables: { pareto_set [ idx_best_mean ] } \" ) print ( f \" Mean: { pareto_front [ idx_best_mean , 0 ] : .4f } \" ) print ( f \" StdDev: { pareto_front [ idx_best_mean , 1 ] : .4f } \" ) # Get the most robust solution idx_best_std = np . argmin ( pareto_front [:, 1 ]) print ( f \" \\n Most Robust Solution:\" ) print ( f \" Design Variables: { pareto_set [ idx_best_std ] } \" ) print ( f \" Mean: { pareto_front [ idx_best_std , 0 ] : .4f } \" ) print ( f \" StdDev: { pareto_front [ idx_best_std , 1 ] : .4f } \" ) # Get a balanced solution idx_balanced = np . argmin ( 0.5 * pareto_front [:, 0 ] + 0.5 * pareto_front [:, 1 ]) print ( f \" \\n Balanced Solution:\" ) print ( f \" Design Variables: { pareto_set [ idx_balanced ] } \" ) print ( f \" Mean: { pareto_front [ idx_balanced , 0 ] : .4f } \" ) print ( f \" StdDev: { pareto_front [ idx_balanced , 1 ] : .4f } \" ) This example demonstrates the complete workflow for the two-stage robust optimization approach, from configuring the pipeline to analyzing the final results. The approach significantly reduces computational time compared to traditional MCS-based robust optimization while maintaining high accuracy in the final solutions.","title":"NNMCS Approach"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#pyegrorobustoptmethod_nnmcs-usage-examples","text":"This document provides examples of how to use the PyEGRO.robustopt.method_nnmcs module for robust optimization using a two-stage approach that combines Neural Networks (NN) with Monte Carlo Simulation (MCS).","title":"PyEGRO.robustopt.method_nnmcs Usage Examples"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#table-of-contents","text":"Quick Start Basic Configuration Running the Pipeline Customizing Neural Network Training Hyperparameter Ranges Adjusting Training Settings Sampling Strategy Balancing Accuracy and Speed Using Different Surrogate Models With GPR Models With Cokriging Models Full Pipeline Example","title":"Table of Contents"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#quick-start","text":"","title":"Quick Start"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#basic-configuration","text":"The first step is to set up the configuration objects for the pipeline: import numpy as np from PyEGRO.robustopt.method_nnmcs import ( RobustOptimization , ANNConfig , SamplingConfig , OptimizationConfig , PathConfig ) # Basic configurations ann_config = ANNConfig () # Default values sampling_config = SamplingConfig () # Default values opt_config = OptimizationConfig () # Default values path_config = PathConfig () # Default values # Initialize the optimizer optimizer = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config )","title":"Basic Configuration"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#running-the-pipeline","text":"Once configured, you can run the complete pipeline: # Run the full pipeline pareto_set , pareto_front = optimizer . run () # Access optimization results print ( f \"Number of Pareto-optimal solutions: { len ( pareto_set ) } \" ) print ( \" \\n Sample Solutions:\" ) for i in range ( min ( 3 , len ( pareto_set ))): print ( f \"Solution { i + 1 } :\" ) print ( f \" Design variables: { pareto_set [ i ] } \" ) print ( f \" Objectives (Mean, StdDev): { pareto_front [ i ] } \" )","title":"Running the Pipeline"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#customizing-neural-network-training","text":"","title":"Customizing Neural Network Training"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#hyperparameter-ranges","text":"You can customize the neural network architecture search space: # Custom ANN configuration ann_config = ANNConfig ( n_trials = 100 , # More trials for better hyperparameter optimization n_jobs =- 1 , # Use all available cores hyperparameter_ranges = { 'n_layers' : ( 2 , 5 ), # Network depth 'n_units' : ( 64 , 512 ), # Units per layer 'learning_rate' : ( 1e-5 , 1e-2 ), # Learning rate range 'batch_size' : ( 16 , 128 ), # Batch size range 'max_epochs' : ( 100 , 500 ), # Maximum training epochs 'activations' : [ 'ReLU' , 'LeakyReLU' , 'Tanh' , 'SiLU' ], # Activation functions 'optimizers' : [ 'Adam' , 'AdamW' , 'RMSprop' ], # Optimizers 'loss_functions' : [ 'MSELoss' , 'HuberLoss' , 'L1Loss' ] # Loss functions } )","title":"Hyperparameter Ranges"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#adjusting-training-settings","text":"You can control the training process: # ANN configuration focused on training stability ann_config = ANNConfig ( n_trials = 50 , n_jobs = 4 , # Limit to 4 parallel jobs hyperparameter_ranges = { 'n_layers' : ( 1 , 3 ), # Simpler networks 'n_units' : ( 32 , 128 ), # Fewer units 'learning_rate' : ( 1e-4 , 1e-3 ), # More conservative learning rates 'batch_size' : ( 32 , 64 ), # Medium batch sizes 'max_epochs' : ( 200 , 300 ), # Moderate training duration 'activations' : [ 'ReLU' ], # Stick to ReLU 'optimizers' : [ 'Adam' ], # Use only Adam 'loss_functions' : [ 'MSELoss' ] # Use only MSE loss } )","title":"Adjusting Training Settings"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#sampling-strategy","text":"","title":"Sampling Strategy"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#balancing-accuracy-and-speed","text":"You can adjust the sampling configuration to balance computational cost and accuracy: # Fast exploration configuration fast_sampling = SamplingConfig ( n_lhs_samples = 500 , # Fewer design samples n_mcs_samples_low = 1000 , # Low-fidelity MCS for training n_mcs_samples_high = 10000 # Medium-fidelity verification ) # High accuracy configuration accurate_sampling = SamplingConfig ( n_lhs_samples = 5000 , # More design samples n_mcs_samples_low = 10000 , # Higher-fidelity training n_mcs_samples_high = 1000000 # Very high-fidelity verification ) # Balanced configuration balanced_sampling = SamplingConfig ( n_lhs_samples = 2000 , n_mcs_samples_low = 5000 , n_mcs_samples_high = 100000 )","title":"Balancing Accuracy and Speed"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#using-different-metamodels","text":"","title":"Using Different Metamodels"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#with-gpr-models","text":"# Configuration for GPR models path_config_gpr = PathConfig ( model_type = 'gpr' , model_dir = 'RESULT_MODEL_GPR' , ann_model_dir = 'RESULT_MODEL_ANN_GPR' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE_GPR' ) # Initialize optimizer with GPR optimizer_gpr = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config_gpr ) # Run optimization with GPR pareto_set_gpr , pareto_front_gpr = optimizer_gpr . run ()","title":"With GPR Models"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#with-cokriging-models","text":"# Configuration for Cokriging models path_config_ck = PathConfig ( model_type = 'cokriging' , model_dir = 'RESULT_MODEL_COKRIGING' , ann_model_dir = 'RESULT_MODEL_ANN_CK' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE_CK' ) # Initialize optimizer with Cokriging optimizer_ck = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config_ck ) # Run optimization with Cokriging pareto_set_ck , pareto_front_ck = optimizer_ck . run ()","title":"With Cokriging Models"},{"location":"basic-usage/robustopt/approach-nnmcs/robustopt_nnmcs_examples/#full-pipeline-example","text":"Here's a complete example that demonstrates the entire pipeline with custom configurations: import numpy as np from PyEGRO.robustopt.method_nnmcs import ( RobustOptimization , ANNConfig , SamplingConfig , OptimizationConfig , PathConfig ) # 1. Define Neural Network Configuration ann_config = ANNConfig ( n_trials = 50 , n_jobs =- 1 , hyperparameter_ranges = { 'n_layers' : ( 2 , 4 ), 'n_units' : ( 64 , 256 ), 'learning_rate' : ( 1e-4 , 1e-2 ), 'batch_size' : ( 32 , 64 ), 'max_epochs' : ( 100 , 300 ), 'activations' : [ 'ReLU' , 'LeakyReLU' ], 'optimizers' : [ 'Adam' ], 'loss_functions' : [ 'MSELoss' ] } ) # 2. Define Sampling Configuration sampling_config = SamplingConfig ( n_lhs_samples = 2000 , n_mcs_samples_low = 10000 , n_mcs_samples_high = 500000 , random_seed = 42 ) # 3. Define Optimization Configuration opt_config = OptimizationConfig ( pop_size = 50 , n_gen = 100 , prefer_gpu = True , verbose = True , random_seed = 42 , metric = 'hv' , reference_point = np . array ([ 10.0 , 5.0 ]) ) # 4. Define Paths Configuration path_config = PathConfig ( model_type = 'gpr' , model_dir = 'RESULT_MODEL_GPR' , ann_model_dir = 'RESULT_MODEL_ANN' , data_info_path = 'DATA_PREPARATION/data_info.json' , results_dir = 'RESULT_PARETO_FRONT_TWOSTAGE' , qoi_dir = 'RESULT_QOI' ) # 5. Initialize the Optimizer optimizer = RobustOptimization ( ann_config = ann_config , sampling_config = sampling_config , optimization_config = opt_config , path_config = path_config ) # 6. Run the Optimization print ( \"Starting Two-Stage Robust Optimization Pipeline...\" ) pareto_set , pareto_front = optimizer . run () # 7. Process and Display Results print ( \" \\n Optimization Results:\" ) print ( \"-\" * 50 ) print ( f \"Number of Pareto-optimal solutions: { len ( pareto_set ) } \" ) # Get the best mean solution idx_best_mean = np . argmin ( pareto_front [:, 0 ]) print ( f \" \\n Best Mean Solution:\" ) print ( f \" Design Variables: { pareto_set [ idx_best_mean ] } \" ) print ( f \" Mean: { pareto_front [ idx_best_mean , 0 ] : .4f } \" ) print ( f \" StdDev: { pareto_front [ idx_best_mean , 1 ] : .4f } \" ) # Get the most robust solution idx_best_std = np . argmin ( pareto_front [:, 1 ]) print ( f \" \\n Most Robust Solution:\" ) print ( f \" Design Variables: { pareto_set [ idx_best_std ] } \" ) print ( f \" Mean: { pareto_front [ idx_best_std , 0 ] : .4f } \" ) print ( f \" StdDev: { pareto_front [ idx_best_std , 1 ] : .4f } \" ) # Get a balanced solution idx_balanced = np . argmin ( 0.5 * pareto_front [:, 0 ] + 0.5 * pareto_front [:, 1 ]) print ( f \" \\n Balanced Solution:\" ) print ( f \" Design Variables: { pareto_set [ idx_balanced ] } \" ) print ( f \" Mean: { pareto_front [ idx_balanced , 0 ] : .4f } \" ) print ( f \" StdDev: { pareto_front [ idx_balanced , 1 ] : .4f } \" ) This example demonstrates the complete workflow for the two-stage robust optimization approach, from configuring the pipeline to analyzing the final results. The approach significantly reduces computational time compared to traditional MCS-based robust optimization while maintaining high accuracy in the final solutions.","title":"Full Pipeline Example"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/","text":"PyEGRO.robustopt.method_pce Usage Examples \u00b6 This document provides examples of how to use the PyEGRO.robustopt.method_pce module for robust optimization using Polynomial Chaos Expansion (PCE) for uncertainty quantification. Table of Contents \u00b6 Quick Start Variable Definition Methods Basic Robust Optimization with PCE Custom PCE Order and Sample Size Example Functions Gaussian Peaks Function Aerospace Wing Design Example Manufacturing Process Optimization Using Metamodels Using GPR Models Using Cokriging Models Quick Start \u00b6 Variable Definition Methods \u00b6 PCE supports two different methods for defining variables in robust optimization problems: 1. Dictionary Format \u00b6 data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] } 2. Loading from JSON File \u00b6 import json # Load existing problem definition with open ( 'data_info.json' , 'r' ) as f : data_info = json . load ( f ) Basic Robust Optimization with PCE \u00b6 This example demonstrates basic robust optimization using PCE for uncertainty quantification. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a test function for demonstration def branin ( X ): \"\"\"Simple branin function with interaction terms.\"\"\" x1 , x2 = X [:, 0 ], X [:, 1 ] a = 1.0 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) return a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # Define the problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 10 ], 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' # Normal distribution around design point }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 15 ], 'cov' : 0.05 , # 5% coefficient of variation 'distribution' : 'normal' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = branin , pce_samples = 200 , # Number of PCE samples pce_order = 3 , # Order of PCE expansion pop_size = 50 , # Population size for NSGA-II n_gen = 30 , # Number of generations show_info = True # Display progress ) # Save and visualize results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_TEST' ) # Print sample robust solution pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Get a balanced solution (mid-point of Pareto front) idx = len ( pareto_front ) // 2 balanced_solution = pareto_set [ idx ] mean_perf = pareto_front [ idx , 0 ] stddev = pareto_front [ idx , 1 ] print ( \" \\n Balanced Robust Solution:\" ) print ( f \"x1 = { balanced_solution [ 0 ] : .4f } , x2 = { balanced_solution [ 1 ] : .4f } \" ) print ( f \"Mean Performance = { mean_perf : .4f } , StdDev = { stddev : .4f } \" ) Custom PCE Order and Sample Size \u00b6 This example demonstrates how to customize the PCE order and sample size for different accuracy/performance tradeoffs. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a simple test function def test_function ( X ): \"\"\"Sphere function with environmental noise.\"\"\" x = X [:, 0 : 2 ] # Design variables e = X [:, 2 ] # Environmental variable # Calculate base function f_base = np . sum ( x ** 2 , axis = 1 ) # Add effect of environmental variable f = f_base + 0.2 * e * ( x [:, 0 ] + x [:, 1 ]) return f # Define problem using dictionary format data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] } # Higher order PCE with more samples for better accuracy results_high_accuracy = run_robust_optimization ( data_info = data_info , true_func = test_function , pce_samples = 500 , # More samples pce_order = 5 , # Higher order expansion pop_size = 50 , n_gen = 30 , metric = 'hv' ) save_optimization_results ( results_high_accuracy , data_info , save_dir = 'RESULT_PCE_HIGH_ACCURACY' ) # Lower order PCE with fewer samples for faster execution results_fast = run_robust_optimization ( data_info = data_info , true_func = test_function , pce_samples = 100 , # Fewer samples pce_order = 2 , # Lower order expansion pop_size = 50 , n_gen = 30 , metric = 'hv' ) save_optimization_results ( results_fast , data_info , save_dir = 'RESULT_PCE_FAST' ) # Compare convergence and accuracy print ( \" \\n PCE Configuration Comparison:\" ) print ( \"-\" * 40 ) print ( f \"High Accuracy PCE (Order 5, 500 samples):\" ) print ( f \" Final HV: { results_high_accuracy [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Runtime: { results_high_accuracy [ 'runtime' ] : .2f } seconds\" ) print ( f \" \\n Fast PCE (Order 2, 100 samples):\" ) print ( f \" Final HV: { results_fast [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Runtime: { results_fast [ 'runtime' ] : .2f } seconds\" ) Example Functions \u00b6 Gaussian Peaks Function \u00b6 This example uses a 2D function with two Gaussian peaks, where one peak is higher but more sensitive to parameter variations. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define 2D test function with two Gaussian peaks def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] # Parameters for two Gaussian peaks a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 # Lower and more sensitive a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 # Higher and more robust # Calculate negative sum of two Gaussian peaks f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f # Define problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , # Higher uncertainty to highlight robust solutions 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , 'description' : 'Second design variable' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = objective_func_2D , pce_samples = 300 , pce_order = 4 , pop_size = 100 , n_gen = 50 , show_info = True ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_GAUSSIAN_PEAKS' ) # Analyze results - find best performing and most robust solutions pareto_front = results [ 'pareto_front' ] pareto_set = results [ 'pareto_set' ] idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( \" \\n Gaussian Peaks Optimization Results:\" ) print ( \"-\" * 50 ) print ( f \"Best Performance Solution: x1= { pareto_set [ idx_best_perf , 0 ] : .4f } , x2= { pareto_set [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_best_perf , 0 ] : .4f } , StdDev= { pareto_front [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Most Robust Solution: x1= { pareto_set [ idx_most_robust , 0 ] : .4f } , x2= { pareto_set [ idx_most_robust , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_most_robust , 0 ] : .4f } , StdDev= { pareto_front [ idx_most_robust , 1 ] : .4f } \" ) Aerospace Wing Design Example \u00b6 This example demonstrates a more complex application to aerospace wing design optimization using PCE. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a simplified wing performance model def wing_model ( X ): \"\"\" Simplified model for wing performance calculation. Returns a combined metric of drag and weight (lower is better). \"\"\" # Design variables aspect_ratio = X [:, 0 ] # Wing aspect ratio sweep_angle = X [:, 1 ] # Sweep angle (degrees) thickness_ratio = X [:, 2 ] # Thickness-to-chord ratio # Environmental variables mach_number = X [:, 3 ] # Cruise Mach number altitude = X [:, 4 ] # Cruise altitude (m) # Simplified physics calculations # Wave drag increases with Mach and thickness, decreases with sweep wave_drag = 0.01 * mach_number ** 2 * thickness_ratio / np . cos ( np . radians ( sweep_angle )) # Induced drag decreases with aspect ratio induced_drag = 1.0 / ( np . pi * aspect_ratio ) # Profile drag increases with thickness profile_drag = 0.005 + 0.01 * thickness_ratio # Total drag total_drag = wave_drag + induced_drag + profile_drag # Structural weight increases with aspect ratio and decreases with sweep and thickness structural_weight = ( 1.5 * aspect_ratio ** 1.5 ) / ( 1 + 0.5 * np . sin ( np . radians ( sweep_angle ))) / thickness_ratio ** 0.5 # Total performance metric (weighted sum of drag and weight) performance = 10 * total_drag + structural_weight return performance # Define the robust wing design problem data_info = { 'variables' : [ # Design variables { 'name' : 'aspect_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 6 , 12 ], 'cov' : 0.02 , # Manufacturing tolerance 'description' : 'Wing aspect ratio' }, { 'name' : 'sweep_angle' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 20 , 40 ], # degrees 'cov' : 0.01 , # Manufacturing tolerance 'description' : 'Wing sweep angle' }, { 'name' : 'thickness_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.08 , 0.16 ], 'cov' : 0.03 , # Manufacturing tolerance 'description' : 'Thickness-to-chord ratio' }, # Environmental variables (operating conditions) { 'name' : 'mach_number' , 'vars_type' : 'env_vars' , 'distribution' : 'triangular' , 'low' : 0.75 , # Minimum Mach 'high' : 0.85 , # Maximum Mach 'mode' : 0.78 , # Most common Mach 'description' : 'Cruise Mach number' }, { 'name' : 'altitude' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 11000 , # 11,000 m (typical cruise altitude) 'cov' : 0.1 , # CoV 10% 'description' : 'Cruise altitude' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = wing_model , pce_samples = 400 , pce_order = 4 , pop_size = 100 , n_gen = 80 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_WING_DESIGN' ) # Print results summary pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] print ( \" \\n Wing Design Optimization Results:\" ) print ( \"-\" * 50 ) # Extract key solutions idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( f \" { 'Solution' : <15 } | { 'Aspect Ratio' : <12 } | { 'Sweep Angle' : <12 } | { 'Thickness' : <10 } | { 'Performance' : <12 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 85 ) print ( f \" { 'Best Performance' : <15 } | { pareto_set [ idx_best_perf , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_best_perf , 1 ] : <12.2f } | { pareto_set [ idx_best_perf , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_best_perf , 0 ] : <12.4f } | { pareto_front [ idx_best_perf , 1 ] : <10.4f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <12.2f } | { pareto_set [ idx_most_robust , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <12.4f } | { pareto_front [ idx_most_robust , 1 ] : <10.4f } \" ) Manufacturing Process Optimization \u00b6 This example demonstrates optimizing a manufacturing process with multiple sources of uncertainty using PCE. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a manufacturing process model def manufacturing_process ( X ): \"\"\" Model of a manufacturing process with multiple parameters. Returns the total cost (to be minimized). \"\"\" # Process parameters (design variables) temp = X [:, 0 ] # Process temperature time = X [:, 1 ] # Process time pressure = X [:, 2 ] # Process pressure # Material properties (environmental variables) material_density = X [:, 3 ] material_purity = X [:, 4 ] # Machine variations (environmental variables) machine_bias = X [:, 5 ] # Quality calculation base_quality = ( - 0.1 * ( temp - 350 ) ** 2 - # Temperature effect 0.05 * ( time - 60 ) ** 2 - # Time effect 0.02 * ( pressure - 50 ) ** 2 # Pressure effect ) # Material effects on quality material_effect = 20 * material_purity - 0.001 * material_density # Machine bias effect machine_effect = - 5 * machine_bias # Final quality (higher is better) quality = base_quality + material_effect + machine_effect # Cost calculation energy_cost = 0.01 * temp * time material_cost = 0.1 * material_density # Time-based costs labor_cost = 0.5 * time # Total cost total_cost = energy_cost + material_cost + labor_cost - 0.2 * quality return total_cost # Define the robust optimization problem data_info = { 'variables' : [ # Process parameters (design variables) { 'name' : 'temperature' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 300 , 400 ], # degrees C 'cov' : 0.02 , # 2% control uncertainty 'description' : 'Process temperature' }, { 'name' : 'time' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 90 ], # minutes 'cov' : 0.03 , # 3% control uncertainty 'description' : 'Process time' }, { 'name' : 'pressure' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 70 ], # units 'cov' : 0.01 , # 1% control uncertainty 'description' : 'Process pressure' }, # Material properties (environmental variables) { 'name' : 'material_density' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2000 , 'std' : 50 , 'description' : 'Material density' }, { 'name' : 'material_purity' , 'vars_type' : 'env_vars' , 'distribution' : 'beta' , 'alpha' : 9 , # Shape parameter 'beta' : 1 , # Shape parameter 'low' : 0.9 , 'high' : 1.0 , # Range for scaling 'description' : 'Material purity' }, # Machine variation (environmental variable) { 'name' : 'machine_bias' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.1 , 'description' : 'Machine bias' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = manufacturing_process , pce_samples = 300 , pce_order = 3 , pop_size = 100 , n_gen = 50 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_MANUFACTURING' ) # Print interesting solutions print ( \" \\n Manufacturing Process Optimization Results:\" ) print ( \"-\" * 60 ) pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Find solutions idx_best_cost = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) idx_balanced = np . argmin ( pareto_front [:, 0 ] * 0.7 + pareto_front [:, 1 ] * 0.3 ) # Weighted balance print ( f \" { 'Solution' : <15 } | { 'Temp (\u00b0C)' : <10 } | { 'Time (min)' : <10 } | { 'Pressure' : <10 } | { 'Cost' : <10 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 72 ) print ( f \" { 'Lowest Cost' : <15 } | { pareto_set [ idx_best_cost , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_best_cost , 1 ] : <10.1f } | { pareto_set [ idx_best_cost , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_best_cost , 0 ] : <10.2f } | { pareto_front [ idx_best_cost , 1 ] : <10.2f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <10.1f } | { pareto_set [ idx_most_robust , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <10.2f } | { pareto_front [ idx_most_robust , 1 ] : <10.2f } \" ) print ( f \" { 'Balanced' : <15 } | { pareto_set [ idx_balanced , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_balanced , 1 ] : <10.1f } | { pareto_set [ idx_balanced , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_balanced , 0 ] : <10.2f } | { pareto_front [ idx_balanced , 1 ] : <10.2f } \" ) Using Metamodels \u00b6 When evaluating the original function is computationally expensive, you can use pre-trained surrogate models for efficient robust optimization with PCE. PyEGRO supports multiple surrogate model types including Gaussian Process Regression (GPR) and Cokriging. Using GPR Models \u00b6 #========================================================================== # Using Surrogate Model - GPR with PCE #========================================================================== from PyEGRO.meta.gpr import gpr_utils from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results import json print ( \" \\n Using GPR Surrogate Model with PCE\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize GPR handler (**GPR need to be trained) gpr_handler = gpr_utils . DeviceAgnosticGPR ( prefer_gpu = True ) gpr_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = gpr_handler , data_info = data_info , pce_samples = 200 , # PCE samples per evaluation pce_order = 3 , # PCE polynomial order pop_size = 25 , n_gen = 100 , metric = 'hv' ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'PCE_RESULT_GPR' ) Using Cokriging Models \u00b6 Cokriging models are particularly useful when you have multi-fidelity data or multiple correlated responses. #========================================================================== # Using Surrogate Model - Cokriging with PCE #========================================================================== from PyEGRO.meta.cokriging import cokriging_utils from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results import json print ( \" \\n Using Cokriging Surrogate Model with PCE\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize Cokriging handler (**Cokriging model need to be trained) cokriging_handler = cokriging_utils . DeviceAgnosticCoKriging ( prefer_gpu = True ) cokriging_handler . load_model ( 'RESULT_MODEL_COKRIGING' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = cokriging_handler , data_info = data_info , pce_samples = 200 , # PCE samples per evaluation pce_order = 3 , # PCE polynomial order pop_size = 25 , n_gen = 100 , metric = 'hv' , reference_point = [ 5 , 5 ] # Custom reference point for HV calculation ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'PCE_RESULT_CK' ) Comparing PCE with MCS \u00b6 This example demonstrates a comparison between PCE and MCS approaches for the same problem. import numpy as np import time from PyEGRO.robustopt.method_pce import run_robust_optimization as pce_optimization from PyEGRO.robustopt.method_mcs import run_robust_optimization as mcs_optimization from PyEGRO.robustopt.method_pce import save_optimization_results # Define a test function def test_function ( X ): \"\"\"Test function for uncertainty quantification comparison.\"\"\" x1 , x2 = X [:, 0 ], X [:, 1 ] e1 = X [:, 2 ] # Environmental variable # Base function with non-linearity f_base = ( x1 ** 2 + x2 - 11 ) ** 2 + ( x1 + x2 ** 2 - 7 ) ** 2 # Add environmental impact f = f_base + 0.5 * e1 * ( x1 + x2 ) return f # Define problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.1 , 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.1 , 'description' : 'Second design variable' }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 1 , 'description' : 'Environmental variable' } ] } # Run PCE-based optimization print ( \" \\n Running PCE-based optimization...\" ) start_time_pce = time . time () pce_results = pce_optimization ( data_info = data_info , true_func = test_function , pce_samples = 200 , pce_order = 3 , pop_size = 50 , n_gen = 30 ) pce_time = time . time () - start_time_pce # Run MCS-based optimization print ( \" \\n Running MCS-based optimization...\" ) start_time_mcs = time . time () mcs_results = mcs_optimization ( data_info = data_info , true_func = test_function , mcs_samples = 5000 , # Typically MCS needs more samples than PCE pop_size = 50 , n_gen = 30 ) mcs_time = time . time () - start_time_mcs # Save results save_optimization_results ( pce_results , data_info , save_dir = 'RESULT_PCE_COMPARISON' ) save_optimization_results ( mcs_results , data_info , save_dir = 'RESULT_MCS_COMPARISON' ) # Compare results print ( \" \\n Method Comparison:\" ) print ( \"-\" * 50 ) print ( f \"PCE approach (Order=3, Samples=200):\" ) print ( f \" Runtime: { pce_time : .2f } seconds\" ) print ( f \" Final HV: { pce_results [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Number of total evaluations: { pce_results [ 'convergence_history' ][ 'n_evals' ][ - 1 ] } \" ) print ( f \" \\n MCS approach (Samples=5000):\" ) print ( f \" Runtime: { mcs_time : .2f } seconds\" ) print ( f \" Final HV: { mcs_results [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Number of total evaluations: { mcs_results [ 'convergence_history' ][ 'n_evals' ][ - 1 ] } \" ) # Extract best solutions from each approach pce_idx_best = np . argmin ( pce_results [ 'pareto_front' ][:, 0 ]) mcs_idx_best = np . argmin ( mcs_results [ 'pareto_front' ][:, 0 ]) print ( \" \\n Best Solutions Found:\" ) print ( f \"PCE Best: x1= { pce_results [ 'pareto_set' ][ pce_idx_best , 0 ] : .4f } , \" f \"x2= { pce_results [ 'pareto_set' ][ pce_idx_best , 1 ] : .4f } \" ) print ( f \"MCS Best: x1= { mcs_results [ 'pareto_set' ][ mcs_idx_best , 0 ] : .4f } , \" f \"x2= { mcs_results [ 'pareto_set' ][ mcs_idx_best , 1 ] : .4f } \" ) # Compare performance vs. robustness tradeoffs print ( \" \\n Tradeoff Analysis:\" ) print ( f \"PCE Mean Range: [ { np . min ( pce_results [ 'pareto_front' ][:, 0 ]) : .4f } , \" f \" { np . max ( pce_results [ 'pareto_front' ][:, 0 ]) : .4f } ]\" ) print ( f \"PCE StdDev Range: [ { np . min ( pce_results [ 'pareto_front' ][:, 1 ]) : .4f } , \" f \" { np . max ( pce_results [ 'pareto_front' ][:, 1 ]) : .4f } ]\" ) print ( f \"MCS Mean Range: [ { np . min ( mcs_results [ 'pareto_front' ][:, 0 ]) : .4f } , \" f \" { np . max ( mcs_results [ 'pareto_front' ][:, 0 ]) : .4f } ]\" ) print ( f \"MCS StdDev Range: [ { np . min ( mcs_results [ 'pareto_front' ][:, 1 ]) : .4f } , \" f \" { np . max ( mcs_results [ 'pareto_front' ][:, 1 ]) : .4f } ]\" ) PCE Parameter Study \u00b6 This example demonstrates how different PCE parameters affect the accuracy and computational cost of the uncertainty quantification. import numpy as np import time from PyEGRO.robustopt.method_pce import PCESampler # Define test function for parameter study def nonlinear_function ( x ): \"\"\"Non-linear test function for uncertainty analysis.\"\"\" return x ** 3 + 2 * x ** 2 - 5 * x + 3 # Define range of PCE settings to test pce_orders = [ 2 , 3 , 4 , 5 , 6 ] pce_sample_sizes = [ 50 , 100 , 200 , 400 , 800 ] # Analytical solution for normal with std=1 true_mean = 3 # For standard normal, the mean of x^3 + 2x^2 - 5x + 3 true_std = np . sqrt ( 15 + 4 ) # Std of x^3 + 2x^2 - 5x + 3 for standard normal # Track results results = [] print ( f \" { 'Order' : <6 } | { 'Samples' : <8 } | { 'Mean Error %' : <12 } | { 'StdDev Error %' : <14 } | { 'Time (ms)' : <10 } \" ) print ( \"-\" * 60 ) for order in pce_orders : for samples in pce_sample_sizes : # Create PCE sampler sampler = PCESampler ( n_samples = samples , order = order ) # Time the evaluation start_time = time . time () # Generate samples x_samples = sampler . generate_samples ( mean = 0 , std = 1 ) # Evaluate function y_samples = nonlinear_function ( x_samples ) # Calculate statistics mean = np . mean ( y_samples ) std = np . std ( y_samples ) elapsed = ( time . time () - start_time ) * 1000 # Convert to milliseconds # Calculate errors mean_error = abs ( mean - true_mean ) / true_mean * 100 std_error = abs ( std - true_std ) / true_std * 100 # Print result print ( f \" { order : <6 } | { samples : <8 } | { mean_error : <12.4f } | { std_error : <14.4f } | { elapsed : <10.2f } \" ) # Store result results . append ({ 'order' : order , 'samples' : samples , 'mean_error' : mean_error , 'std_error' : std_error , 'time_ms' : elapsed }) # Summarize findings print ( \" \\n Recommended Settings:\" ) print ( \"-\" * 30 ) # Find best accuracy per computational budget for samples in [ 50 , 200 , 800 ]: best_order = min ([ r for r in results if r [ 'samples' ] == samples ], key = lambda x : x [ 'mean_error' ] + x [ 'std_error' ]) print ( f \"For { samples } samples: Order { best_order [ 'order' ] } \" f \"(Mean Error: { best_order [ 'mean_error' ] : .4f } %, \" f \"StdDev Error: { best_order [ 'std_error' ] : .4f } %)\" ) # Find minimum sample size for given accuracy target target_accuracy = 1.0 # 1% error for order in pce_orders : min_samples = min ([ r for r in results if r [ 'order' ] == order and ( r [ 'mean_error' ] + r [ 'std_error' ]) / 2 < target_accuracy ], key = lambda x : x [ 'samples' ], default = None ) if min_samples : print ( f \"For Order { order } : Minimum { min_samples [ 'samples' ] } samples needed \" f \"for < { target_accuracy } % error\" ) This comprehensive guide demonstrates the various ways to use PCE for robust optimization in PyEGRO, from basic examples to more complex applications and comparisons with other methods.","title":"PCE Approach"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#pyegrorobustoptmethod_pce-usage-examples","text":"This document provides examples of how to use the PyEGRO.robustopt.method_pce module for robust optimization using Polynomial Chaos Expansion (PCE) for uncertainty quantification.","title":"PyEGRO.robustopt.method_pce Usage Examples"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#table-of-contents","text":"Quick Start Variable Definition Methods Basic Robust Optimization with PCE Custom PCE Order and Sample Size Example Functions Gaussian Peaks Function Aerospace Wing Design Example Manufacturing Process Optimization Using Metamodels Using GPR Models Using Cokriging Models","title":"Table of Contents"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#quick-start","text":"","title":"Quick Start"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#variable-definition-methods","text":"PCE supports two different methods for defining variables in robust optimization problems:","title":"Variable Definition Methods"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#basic-robust-optimization-with-pce","text":"This example demonstrates basic robust optimization using PCE for uncertainty quantification. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a test function for demonstration def branin ( X ): \"\"\"Simple branin function with interaction terms.\"\"\" x1 , x2 = X [:, 0 ], X [:, 1 ] a = 1.0 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) return a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # Define the problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 10 ], 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' # Normal distribution around design point }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 15 ], 'cov' : 0.05 , # 5% coefficient of variation 'distribution' : 'normal' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = branin , pce_samples = 200 , # Number of PCE samples pce_order = 3 , # Order of PCE expansion pop_size = 50 , # Population size for NSGA-II n_gen = 30 , # Number of generations show_info = True # Display progress ) # Save and visualize results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_TEST' ) # Print sample robust solution pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Get a balanced solution (mid-point of Pareto front) idx = len ( pareto_front ) // 2 balanced_solution = pareto_set [ idx ] mean_perf = pareto_front [ idx , 0 ] stddev = pareto_front [ idx , 1 ] print ( \" \\n Balanced Robust Solution:\" ) print ( f \"x1 = { balanced_solution [ 0 ] : .4f } , x2 = { balanced_solution [ 1 ] : .4f } \" ) print ( f \"Mean Performance = { mean_perf : .4f } , StdDev = { stddev : .4f } \" )","title":"Basic Robust Optimization with PCE"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#custom-pce-order-and-sample-size","text":"This example demonstrates how to customize the PCE order and sample size for different accuracy/performance tradeoffs. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a simple test function def test_function ( X ): \"\"\"Sphere function with environmental noise.\"\"\" x = X [:, 0 : 2 ] # Design variables e = X [:, 2 ] # Environmental variable # Calculate base function f_base = np . sum ( x ** 2 , axis = 1 ) # Add effect of environmental variable f = f_base + 0.2 * e * ( x [:, 0 ] + x [:, 1 ]) return f # Define problem using dictionary format data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.05 }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 1 , 'high' : 1 } ] } # Higher order PCE with more samples for better accuracy results_high_accuracy = run_robust_optimization ( data_info = data_info , true_func = test_function , pce_samples = 500 , # More samples pce_order = 5 , # Higher order expansion pop_size = 50 , n_gen = 30 , metric = 'hv' ) save_optimization_results ( results_high_accuracy , data_info , save_dir = 'RESULT_PCE_HIGH_ACCURACY' ) # Lower order PCE with fewer samples for faster execution results_fast = run_robust_optimization ( data_info = data_info , true_func = test_function , pce_samples = 100 , # Fewer samples pce_order = 2 , # Lower order expansion pop_size = 50 , n_gen = 30 , metric = 'hv' ) save_optimization_results ( results_fast , data_info , save_dir = 'RESULT_PCE_FAST' ) # Compare convergence and accuracy print ( \" \\n PCE Configuration Comparison:\" ) print ( \"-\" * 40 ) print ( f \"High Accuracy PCE (Order 5, 500 samples):\" ) print ( f \" Final HV: { results_high_accuracy [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Runtime: { results_high_accuracy [ 'runtime' ] : .2f } seconds\" ) print ( f \" \\n Fast PCE (Order 2, 100 samples):\" ) print ( f \" Final HV: { results_fast [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Runtime: { results_fast [ 'runtime' ] : .2f } seconds\" )","title":"Custom PCE Order and Sample Size"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#example-functions","text":"","title":"Example Functions"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#gaussian-peaks-function","text":"This example uses a 2D function with two Gaussian peaks, where one peak is higher but more sensitive to parameter variations. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define 2D test function with two Gaussian peaks def objective_func_2D ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] # Parameters for two Gaussian peaks a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 # Lower and more sensitive a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 # Higher and more robust # Calculate negative sum of two Gaussian peaks f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f # Define problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , # Higher uncertainty to highlight robust solutions 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.2 , 'description' : 'Second design variable' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = objective_func_2D , pce_samples = 300 , pce_order = 4 , pop_size = 100 , n_gen = 50 , show_info = True ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_GAUSSIAN_PEAKS' ) # Analyze results - find best performing and most robust solutions pareto_front = results [ 'pareto_front' ] pareto_set = results [ 'pareto_set' ] idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( \" \\n Gaussian Peaks Optimization Results:\" ) print ( \"-\" * 50 ) print ( f \"Best Performance Solution: x1= { pareto_set [ idx_best_perf , 0 ] : .4f } , x2= { pareto_set [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_best_perf , 0 ] : .4f } , StdDev= { pareto_front [ idx_best_perf , 1 ] : .4f } \" ) print ( f \"Most Robust Solution: x1= { pareto_set [ idx_most_robust , 0 ] : .4f } , x2= { pareto_set [ idx_most_robust , 1 ] : .4f } \" ) print ( f \"Mean= { pareto_front [ idx_most_robust , 0 ] : .4f } , StdDev= { pareto_front [ idx_most_robust , 1 ] : .4f } \" )","title":"Gaussian Peaks Function"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#aerospace-wing-design-example","text":"This example demonstrates a more complex application to aerospace wing design optimization using PCE. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a simplified wing performance model def wing_model ( X ): \"\"\" Simplified model for wing performance calculation. Returns a combined metric of drag and weight (lower is better). \"\"\" # Design variables aspect_ratio = X [:, 0 ] # Wing aspect ratio sweep_angle = X [:, 1 ] # Sweep angle (degrees) thickness_ratio = X [:, 2 ] # Thickness-to-chord ratio # Environmental variables mach_number = X [:, 3 ] # Cruise Mach number altitude = X [:, 4 ] # Cruise altitude (m) # Simplified physics calculations # Wave drag increases with Mach and thickness, decreases with sweep wave_drag = 0.01 * mach_number ** 2 * thickness_ratio / np . cos ( np . radians ( sweep_angle )) # Induced drag decreases with aspect ratio induced_drag = 1.0 / ( np . pi * aspect_ratio ) # Profile drag increases with thickness profile_drag = 0.005 + 0.01 * thickness_ratio # Total drag total_drag = wave_drag + induced_drag + profile_drag # Structural weight increases with aspect ratio and decreases with sweep and thickness structural_weight = ( 1.5 * aspect_ratio ** 1.5 ) / ( 1 + 0.5 * np . sin ( np . radians ( sweep_angle ))) / thickness_ratio ** 0.5 # Total performance metric (weighted sum of drag and weight) performance = 10 * total_drag + structural_weight return performance # Define the robust wing design problem data_info = { 'variables' : [ # Design variables { 'name' : 'aspect_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 6 , 12 ], 'cov' : 0.02 , # Manufacturing tolerance 'description' : 'Wing aspect ratio' }, { 'name' : 'sweep_angle' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 20 , 40 ], # degrees 'cov' : 0.01 , # Manufacturing tolerance 'description' : 'Wing sweep angle' }, { 'name' : 'thickness_ratio' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.08 , 0.16 ], 'cov' : 0.03 , # Manufacturing tolerance 'description' : 'Thickness-to-chord ratio' }, # Environmental variables (operating conditions) { 'name' : 'mach_number' , 'vars_type' : 'env_vars' , 'distribution' : 'triangular' , 'low' : 0.75 , # Minimum Mach 'high' : 0.85 , # Maximum Mach 'mode' : 0.78 , # Most common Mach 'description' : 'Cruise Mach number' }, { 'name' : 'altitude' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 11000 , # 11,000 m (typical cruise altitude) 'cov' : 0.1 , # CoV 10% 'description' : 'Cruise altitude' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = wing_model , pce_samples = 400 , pce_order = 4 , pop_size = 100 , n_gen = 80 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_WING_DESIGN' ) # Print results summary pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] print ( \" \\n Wing Design Optimization Results:\" ) print ( \"-\" * 50 ) # Extract key solutions idx_best_perf = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) print ( f \" { 'Solution' : <15 } | { 'Aspect Ratio' : <12 } | { 'Sweep Angle' : <12 } | { 'Thickness' : <10 } | { 'Performance' : <12 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 85 ) print ( f \" { 'Best Performance' : <15 } | { pareto_set [ idx_best_perf , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_best_perf , 1 ] : <12.2f } | { pareto_set [ idx_best_perf , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_best_perf , 0 ] : <12.4f } | { pareto_front [ idx_best_perf , 1 ] : <10.4f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <12.2f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <12.2f } | { pareto_set [ idx_most_robust , 2 ] : <10.4f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <12.4f } | { pareto_front [ idx_most_robust , 1 ] : <10.4f } \" )","title":"Aerospace Wing Design Example"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#manufacturing-process-optimization","text":"This example demonstrates optimizing a manufacturing process with multiple sources of uncertainty using PCE. import numpy as np from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results # Define a manufacturing process model def manufacturing_process ( X ): \"\"\" Model of a manufacturing process with multiple parameters. Returns the total cost (to be minimized). \"\"\" # Process parameters (design variables) temp = X [:, 0 ] # Process temperature time = X [:, 1 ] # Process time pressure = X [:, 2 ] # Process pressure # Material properties (environmental variables) material_density = X [:, 3 ] material_purity = X [:, 4 ] # Machine variations (environmental variables) machine_bias = X [:, 5 ] # Quality calculation base_quality = ( - 0.1 * ( temp - 350 ) ** 2 - # Temperature effect 0.05 * ( time - 60 ) ** 2 - # Time effect 0.02 * ( pressure - 50 ) ** 2 # Pressure effect ) # Material effects on quality material_effect = 20 * material_purity - 0.001 * material_density # Machine bias effect machine_effect = - 5 * machine_bias # Final quality (higher is better) quality = base_quality + material_effect + machine_effect # Cost calculation energy_cost = 0.01 * temp * time material_cost = 0.1 * material_density # Time-based costs labor_cost = 0.5 * time # Total cost total_cost = energy_cost + material_cost + labor_cost - 0.2 * quality return total_cost # Define the robust optimization problem data_info = { 'variables' : [ # Process parameters (design variables) { 'name' : 'temperature' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 300 , 400 ], # degrees C 'cov' : 0.02 , # 2% control uncertainty 'description' : 'Process temperature' }, { 'name' : 'time' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 90 ], # minutes 'cov' : 0.03 , # 3% control uncertainty 'description' : 'Process time' }, { 'name' : 'pressure' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 30 , 70 ], # units 'cov' : 0.01 , # 1% control uncertainty 'description' : 'Process pressure' }, # Material properties (environmental variables) { 'name' : 'material_density' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2000 , 'std' : 50 , 'description' : 'Material density' }, { 'name' : 'material_purity' , 'vars_type' : 'env_vars' , 'distribution' : 'beta' , 'alpha' : 9 , # Shape parameter 'beta' : 1 , # Shape parameter 'low' : 0.9 , 'high' : 1.0 , # Range for scaling 'description' : 'Material purity' }, # Machine variation (environmental variable) { 'name' : 'machine_bias' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.1 , 'description' : 'Machine bias' } ] } # Run robust optimization with PCE results = run_robust_optimization ( data_info = data_info , true_func = manufacturing_process , pce_samples = 300 , pce_order = 3 , pop_size = 100 , n_gen = 50 ) # Save results save_optimization_results ( results , data_info , save_dir = 'RESULT_PCE_MANUFACTURING' ) # Print interesting solutions print ( \" \\n Manufacturing Process Optimization Results:\" ) print ( \"-\" * 60 ) pareto_set = results [ 'pareto_set' ] pareto_front = results [ 'pareto_front' ] # Find solutions idx_best_cost = np . argmin ( pareto_front [:, 0 ]) idx_most_robust = np . argmin ( pareto_front [:, 1 ]) idx_balanced = np . argmin ( pareto_front [:, 0 ] * 0.7 + pareto_front [:, 1 ] * 0.3 ) # Weighted balance print ( f \" { 'Solution' : <15 } | { 'Temp (\u00b0C)' : <10 } | { 'Time (min)' : <10 } | { 'Pressure' : <10 } | { 'Cost' : <10 } | { 'StdDev' : <10 } \" ) print ( \"-\" * 72 ) print ( f \" { 'Lowest Cost' : <15 } | { pareto_set [ idx_best_cost , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_best_cost , 1 ] : <10.1f } | { pareto_set [ idx_best_cost , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_best_cost , 0 ] : <10.2f } | { pareto_front [ idx_best_cost , 1 ] : <10.2f } \" ) print ( f \" { 'Most Robust' : <15 } | { pareto_set [ idx_most_robust , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_most_robust , 1 ] : <10.1f } | { pareto_set [ idx_most_robust , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_most_robust , 0 ] : <10.2f } | { pareto_front [ idx_most_robust , 1 ] : <10.2f } \" ) print ( f \" { 'Balanced' : <15 } | { pareto_set [ idx_balanced , 0 ] : <10.1f } | \" f \" { pareto_set [ idx_balanced , 1 ] : <10.1f } | { pareto_set [ idx_balanced , 2 ] : <10.1f } | \" f \" { pareto_front [ idx_balanced , 0 ] : <10.2f } | { pareto_front [ idx_balanced , 1 ] : <10.2f } \" )","title":"Manufacturing Process Optimization"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#using-metamodels","text":"When evaluating the original function is computationally expensive, you can use pre-trained surrogate models for efficient robust optimization with PCE. PyEGRO supports multiple surrogate model types including Gaussian Process Regression (GPR) and Cokriging.","title":"Using Metamodels"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#using-gpr-models","text":"#========================================================================== # Using Surrogate Model - GPR with PCE #========================================================================== from PyEGRO.meta.gpr import gpr_utils from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results import json print ( \" \\n Using GPR Surrogate Model with PCE\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize GPR handler (**GPR need to be trained) gpr_handler = gpr_utils . DeviceAgnosticGPR ( prefer_gpu = True ) gpr_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = gpr_handler , data_info = data_info , pce_samples = 200 , # PCE samples per evaluation pce_order = 3 , # PCE polynomial order pop_size = 25 , n_gen = 100 , metric = 'hv' ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'PCE_RESULT_GPR' )","title":"Using GPR Models"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#using-cokriging-models","text":"Cokriging models are particularly useful when you have multi-fidelity data or multiple correlated responses. #========================================================================== # Using Surrogate Model - Cokriging with PCE #========================================================================== from PyEGRO.meta.cokriging import cokriging_utils from PyEGRO.robustopt.method_pce import run_robust_optimization , save_optimization_results import json print ( \" \\n Using Cokriging Surrogate Model with PCE\" ) print ( \"=\" * 50 ) # Load existing problem definition with open ( 'DATA_PREPARATION/data_info.json' , 'r' ) as f : data_info = json . load ( f ) # Initialize Cokriging handler (**Cokriging model need to be trained) cokriging_handler = cokriging_utils . DeviceAgnosticCoKriging ( prefer_gpu = True ) cokriging_handler . load_model ( 'RESULT_MODEL_COKRIGING' ) # Run optimization with surrogate results = run_robust_optimization ( model_handler = cokriging_handler , data_info = data_info , pce_samples = 200 , # PCE samples per evaluation pce_order = 3 , # PCE polynomial order pop_size = 25 , n_gen = 100 , metric = 'hv' , reference_point = [ 5 , 5 ] # Custom reference point for HV calculation ) save_optimization_results ( results = results , data_info = data_info , save_dir = 'PCE_RESULT_CK' )","title":"Using Cokriging Models"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#comparing-pce-with-mcs","text":"This example demonstrates a comparison between PCE and MCS approaches for the same problem. import numpy as np import time from PyEGRO.robustopt.method_pce import run_robust_optimization as pce_optimization from PyEGRO.robustopt.method_mcs import run_robust_optimization as mcs_optimization from PyEGRO.robustopt.method_pce import save_optimization_results # Define a test function def test_function ( X ): \"\"\"Test function for uncertainty quantification comparison.\"\"\" x1 , x2 = X [:, 0 ], X [:, 1 ] e1 = X [:, 2 ] # Environmental variable # Base function with non-linearity f_base = ( x1 ** 2 + x2 - 11 ) ** 2 + ( x1 + x2 ** 2 - 7 ) ** 2 # Add environmental impact f = f_base + 0.5 * e1 * ( x1 + x2 ) return f # Define problem with uncertain design variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.1 , 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 5 , 5 ], 'cov' : 0.1 , 'description' : 'Second design variable' }, { 'name' : 'e1' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 1 , 'description' : 'Environmental variable' } ] } # Run PCE-based optimization print ( \" \\n Running PCE-based optimization...\" ) start_time_pce = time . time () pce_results = pce_optimization ( data_info = data_info , true_func = test_function , pce_samples = 200 , pce_order = 3 , pop_size = 50 , n_gen = 30 ) pce_time = time . time () - start_time_pce # Run MCS-based optimization print ( \" \\n Running MCS-based optimization...\" ) start_time_mcs = time . time () mcs_results = mcs_optimization ( data_info = data_info , true_func = test_function , mcs_samples = 5000 , # Typically MCS needs more samples than PCE pop_size = 50 , n_gen = 30 ) mcs_time = time . time () - start_time_mcs # Save results save_optimization_results ( pce_results , data_info , save_dir = 'RESULT_PCE_COMPARISON' ) save_optimization_results ( mcs_results , data_info , save_dir = 'RESULT_MCS_COMPARISON' ) # Compare results print ( \" \\n Method Comparison:\" ) print ( \"-\" * 50 ) print ( f \"PCE approach (Order=3, Samples=200):\" ) print ( f \" Runtime: { pce_time : .2f } seconds\" ) print ( f \" Final HV: { pce_results [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Number of total evaluations: { pce_results [ 'convergence_history' ][ 'n_evals' ][ - 1 ] } \" ) print ( f \" \\n MCS approach (Samples=5000):\" ) print ( f \" Runtime: { mcs_time : .2f } seconds\" ) print ( f \" Final HV: { mcs_results [ 'convergence_history' ][ 'metric_values' ][ - 1 ] : .6f } \" ) print ( f \" Number of total evaluations: { mcs_results [ 'convergence_history' ][ 'n_evals' ][ - 1 ] } \" ) # Extract best solutions from each approach pce_idx_best = np . argmin ( pce_results [ 'pareto_front' ][:, 0 ]) mcs_idx_best = np . argmin ( mcs_results [ 'pareto_front' ][:, 0 ]) print ( \" \\n Best Solutions Found:\" ) print ( f \"PCE Best: x1= { pce_results [ 'pareto_set' ][ pce_idx_best , 0 ] : .4f } , \" f \"x2= { pce_results [ 'pareto_set' ][ pce_idx_best , 1 ] : .4f } \" ) print ( f \"MCS Best: x1= { mcs_results [ 'pareto_set' ][ mcs_idx_best , 0 ] : .4f } , \" f \"x2= { mcs_results [ 'pareto_set' ][ mcs_idx_best , 1 ] : .4f } \" ) # Compare performance vs. robustness tradeoffs print ( \" \\n Tradeoff Analysis:\" ) print ( f \"PCE Mean Range: [ { np . min ( pce_results [ 'pareto_front' ][:, 0 ]) : .4f } , \" f \" { np . max ( pce_results [ 'pareto_front' ][:, 0 ]) : .4f } ]\" ) print ( f \"PCE StdDev Range: [ { np . min ( pce_results [ 'pareto_front' ][:, 1 ]) : .4f } , \" f \" { np . max ( pce_results [ 'pareto_front' ][:, 1 ]) : .4f } ]\" ) print ( f \"MCS Mean Range: [ { np . min ( mcs_results [ 'pareto_front' ][:, 0 ]) : .4f } , \" f \" { np . max ( mcs_results [ 'pareto_front' ][:, 0 ]) : .4f } ]\" ) print ( f \"MCS StdDev Range: [ { np . min ( mcs_results [ 'pareto_front' ][:, 1 ]) : .4f } , \" f \" { np . max ( mcs_results [ 'pareto_front' ][:, 1 ]) : .4f } ]\" )","title":"Comparing PCE with MCS"},{"location":"basic-usage/robustopt/approach-pce/robustopt_pce_examples/#pce-parameter-study","text":"This example demonstrates how different PCE parameters affect the accuracy and computational cost of the uncertainty quantification. import numpy as np import time from PyEGRO.robustopt.method_pce import PCESampler # Define test function for parameter study def nonlinear_function ( x ): \"\"\"Non-linear test function for uncertainty analysis.\"\"\" return x ** 3 + 2 * x ** 2 - 5 * x + 3 # Define range of PCE settings to test pce_orders = [ 2 , 3 , 4 , 5 , 6 ] pce_sample_sizes = [ 50 , 100 , 200 , 400 , 800 ] # Analytical solution for normal with std=1 true_mean = 3 # For standard normal, the mean of x^3 + 2x^2 - 5x + 3 true_std = np . sqrt ( 15 + 4 ) # Std of x^3 + 2x^2 - 5x + 3 for standard normal # Track results results = [] print ( f \" { 'Order' : <6 } | { 'Samples' : <8 } | { 'Mean Error %' : <12 } | { 'StdDev Error %' : <14 } | { 'Time (ms)' : <10 } \" ) print ( \"-\" * 60 ) for order in pce_orders : for samples in pce_sample_sizes : # Create PCE sampler sampler = PCESampler ( n_samples = samples , order = order ) # Time the evaluation start_time = time . time () # Generate samples x_samples = sampler . generate_samples ( mean = 0 , std = 1 ) # Evaluate function y_samples = nonlinear_function ( x_samples ) # Calculate statistics mean = np . mean ( y_samples ) std = np . std ( y_samples ) elapsed = ( time . time () - start_time ) * 1000 # Convert to milliseconds # Calculate errors mean_error = abs ( mean - true_mean ) / true_mean * 100 std_error = abs ( std - true_std ) / true_std * 100 # Print result print ( f \" { order : <6 } | { samples : <8 } | { mean_error : <12.4f } | { std_error : <14.4f } | { elapsed : <10.2f } \" ) # Store result results . append ({ 'order' : order , 'samples' : samples , 'mean_error' : mean_error , 'std_error' : std_error , 'time_ms' : elapsed }) # Summarize findings print ( \" \\n Recommended Settings:\" ) print ( \"-\" * 30 ) # Find best accuracy per computational budget for samples in [ 50 , 200 , 800 ]: best_order = min ([ r for r in results if r [ 'samples' ] == samples ], key = lambda x : x [ 'mean_error' ] + x [ 'std_error' ]) print ( f \"For { samples } samples: Order { best_order [ 'order' ] } \" f \"(Mean Error: { best_order [ 'mean_error' ] : .4f } %, \" f \"StdDev Error: { best_order [ 'std_error' ] : .4f } %)\" ) # Find minimum sample size for given accuracy target target_accuracy = 1.0 # 1% error for order in pce_orders : min_samples = min ([ r for r in results if r [ 'order' ] == order and ( r [ 'mean_error' ] + r [ 'std_error' ]) / 2 < target_accuracy ], key = lambda x : x [ 'samples' ], default = None ) if min_samples : print ( f \"For Order { order } : Minimum { min_samples [ 'samples' ] } samples needed \" f \"for < { target_accuracy } % error\" ) This comprehensive guide demonstrates the various ways to use PCE for robust optimization in PyEGRO, from basic examples to more complex applications and comparisons with other methods.","title":"PCE Parameter Study"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/","text":"PyEGRO.sensitivity.SAmcs Usage Examples \u00b6 This document provides examples of how to use the PyEGRO.sensitivity.SAmcs module for sensitivity analysis using Monte Carlo Simulation with the Sobol method. Table of Contents \u00b6 Quick Start Basic Setup Running the Analysis Working with Different Models Using a Direct Function Using a Surrogate Model Example Applications Beam Design Problem Borehole Function Chemical Reaction Model Circuit Design Problem Quick Start \u00b6 Basic Setup \u00b6 The simplest way to use the PyEGRO.sensitivity.SAmcs module is with the convenience function run_sensitivity_analysis . import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a simple test function def test_function ( X ): \"\"\" Ishigami function - common benchmark for sensitivity analysis. \"\"\" a = 7 b = 0.1 x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] return np . sin ( x1 ) + a * np . sin ( x2 ) ** 2 + b * x3 ** 4 * np . sin ( x1 ) # Define data_info with variable definitions data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'First input variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Second input variable' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Third input variable' } ] } # Save data_info to a JSON file import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run the analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , num_samples = 1024 , # Base sample size output_dir = \"RESULT_SA_ISHIGAMI\" , random_seed = 42 , show_progress = True ) # Print the results print ( \" \\n Sensitivity Indices:\" ) print ( results_df ) Running the Analysis \u00b6 For more control over the analysis process, you can use the MCSSensitivityAnalysis class directly: from PyEGRO.sensitivity.SAmcs import MCSSensitivityAnalysis # Initialize the analysis analyzer = MCSSensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , output_dir = \"RESULT_SA_DETAILED\" , show_variables_info = True , random_seed = 42 ) # Run the analysis results_df = analyzer . run_analysis ( num_samples = 2048 , # Increase sample size for better accuracy show_progress = True ) # Examine the summary statistics import json with open ( \"RESULT_SA_DETAILED/analysis_summary.json\" , 'r' ) as f : summary = json . load ( f ) print ( \" \\n Analysis Summary:\" ) for key , value in summary . items (): print ( f \" { key } : { value } \" ) Working with Different Models \u00b6 Using a Direct Function \u00b6 For analytical functions or inexpensive simulations, you can directly use the function for evaluations: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a complex function with interactions def complex_function ( X ): \"\"\" Function with variable interactions and non-linear effects. \"\"\" x1 , x2 , x3 , x4 = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ] # Base effects base = 2 * x1 ** 2 + x2 + 0.5 * x3 + 0.1 * x4 # Interaction effects interactions = 5 * x1 * x2 + 2 * x2 * x3 + x3 * x4 + 0.5 * x1 * x4 # Non-linear effects non_linear = np . sin ( x1 ) * np . cos ( x2 ) + np . exp ( 0.1 * x3 ) + np . log ( abs ( x4 ) + 1 ) return base + interactions + non_linear # Define data_info data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Primary variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 2 , 2 ], 'cov' : 0.1 , 'description' : 'Secondary variable' }, { 'name' : 'x3' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 1 , 'description' : 'Environmental variable 1' }, { 'name' : 'x4' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 3 , 'high' : 3 , 'description' : 'Environmental variable 2' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/complex_data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/complex_data_info.json\" , true_func = complex_function , num_samples = 2048 , output_dir = \"RESULT_SA_COMPLEX\" , show_progress = True ) # Print top influential variables sorted_results = results_df . sort_values ( 'Total-order' , ascending = False ) print ( \" \\n Variables Ranked by Total-order Influence:\" ) print ( sorted_results [[ 'Parameter' , 'Total-order' , 'First-order' ]]) Using a Surrogate Model \u00b6 For computationally expensive models, you can use a surrogate model: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Load a pre-trained surrogate model model_handler = DeviceAgnosticGPR ( prefer_gpu = True ) model_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run analysis with the surrogate model results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , model_handler = model_handler , num_samples = 4096 , # Can use more samples since evaluations are cheap output_dir = \"RESULT_SA_SURROGATE\" , show_progress = True ) # Examine first-order vs total-order differences to detect interactions interaction_strength = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Strength' ] = interaction_strength print ( \" \\n Variable Interactions:\" ) for i , row in results_df . iterrows (): print ( f \" { row [ 'Parameter' ] } : Interaction Strength = { row [ 'Interaction_Strength' ] : .4f } \" ) Example Applications \u00b6 Beam Design Problem \u00b6 This example demonstrates sensitivity analysis for a beam design problem: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a beam design problem def beam_performance ( X ): \"\"\" Calculate the maximum deflection of a cantilever beam. Variables: - length: Beam length (m) - width: Beam width (m) - height: Beam height (m) - load: Applied load (N) - modulus: Young's modulus (Pa) \"\"\" length = X [:, 0 ] width = X [:, 1 ] height = X [:, 2 ] load = X [:, 3 ] modulus = X [:, 4 ] # Calculate moment of inertia inertia = ( width * height ** 3 ) / 12 # Calculate maximum deflection deflection = ( load * length ** 3 ) / ( 3 * modulus * inertia ) return deflection # Define data_info beam_data = { 'variables' : [ { 'name' : 'length' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1.0 , 3.0 ], 'cov' : 0.01 , 'description' : 'Beam length (m)' }, { 'name' : 'width' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.05 , 0.15 ], 'cov' : 0.02 , 'description' : 'Beam width (m)' }, { 'name' : 'height' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.1 , 0.3 ], 'cov' : 0.02 , 'description' : 'Beam height (m)' }, { 'name' : 'load' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 1000 , 'std' : 200 , 'description' : 'Applied load (N)' }, { 'name' : 'modulus' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.1e11 , 'std' : 1e10 , 'description' : 'Young \\' s modulus (Pa)' } ] } # Save data_info import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/beam_data.json\" , 'w' ) as f : json . dump ( beam_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/beam_data.json\" , true_func = beam_performance , num_samples = 2048 , output_dir = \"RESULT_SA_BEAM\" , show_progress = True ) # Analyze and interpret results print ( \" \\n Beam Design Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df ) # Load summary stats with open ( \"RESULT_SA_BEAM/analysis_summary.json\" , 'r' ) as f : summary = json . load ( f ) print ( \" \\n Design Insights:\" ) print ( f \"Most influential parameter: { summary [ 'most_influential_parameter' ] } \" ) print ( f \"Interaction strength: { summary [ 'interaction_strength' ] : .4f } \" ) print ( \" \\n Engineering Recommendations:\" ) # Sort by total-order sorted_params = results_df . sort_values ( 'Total-order' , ascending = False )[ 'Parameter' ] . tolist () print ( f \"1. Focus on controlling { sorted_params [ 0 ] } for maximum effect on performance\" ) print ( f \"2. Parameters to optimize first: { ', ' . join ( sorted_params [: 2 ]) } \" ) print ( f \"3. Parameters with minimal impact: { ', ' . join ( sorted_params [ - 2 :]) } \" ) Borehole Function \u00b6 The borehole function is a common benchmark in engineering sensitivity analysis: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define the borehole function def borehole_function ( X ): \"\"\" Borehole function - models water flow through a borehole. \"\"\" rw = X [:, 0 ] # radius of borehole (m) r = X [:, 1 ] # radius of influence (m) Tu = X [:, 2 ] # transmissivity of upper aquifer (m\u00b2/year) Hu = X [:, 3 ] # potentiometric head of upper aquifer (m) Tl = X [:, 4 ] # transmissivity of lower aquifer (m\u00b2/year) Hl = X [:, 5 ] # potentiometric head of lower aquifer (m) L = X [:, 6 ] # length of borehole (m) Kw = X [:, 7 ] # hydraulic conductivity of borehole (m/year) numerator = 2 * np . pi * Tu * ( Hu - Hl ) denominator = np . log ( r / rw ) * ( 1 + ( 2 * L * Tu ) / ( np . log ( r / rw ) * rw ** 2 * Kw ) + Tu / Tl ) return numerator / denominator # Define data_info borehole_data = { 'variables' : [ { 'name' : 'rw' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.05 , 0.15 ], 'description' : 'Radius of borehole (m)' }, { 'name' : 'r' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 100 , 50000 ], 'description' : 'Radius of influence (m)' }, { 'name' : 'Tu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63070 , 'high' : 115600 , 'description' : 'Transmissivity of upper aquifer (m\u00b2/year)' }, { 'name' : 'Hu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 990 , 'high' : 1110 , 'description' : 'Potentiometric head of upper aquifer (m)' }, { 'name' : 'Tl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63.1 , 'high' : 116 , 'description' : 'Transmissivity of lower aquifer (m\u00b2/year)' }, { 'name' : 'Hl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 700 , 'high' : 820 , 'description' : 'Potentiometric head of lower aquifer (m)' }, { 'name' : 'L' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 1120 , 1680 ], 'description' : 'Length of borehole (m)' }, { 'name' : 'Kw' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 9855 , 'high' : 12045 , 'description' : 'Hydraulic conductivity of borehole (m/year)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/borehole_data.json\" , 'w' ) as f : json . dump ( borehole_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/borehole_data.json\" , true_func = borehole_function , num_samples = 2048 , output_dir = \"RESULT_SA_BOREHOLE\" , show_progress = True ) # Print results print ( \" \\n Borehole Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Calculate interaction effects interaction_effect = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Effect' ] = interaction_effect # Identify key parameters and interactions top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () top_interactions = results_df . sort_values ( 'Interaction_Effect' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Borehole Analysis Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( f \"Parameters with strongest interactions: { ', ' . join ( top_interactions ) } \" ) Chemical Reaction Model \u00b6 This example illustrates sensitivity analysis for a chemical reaction kinetics model: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a chemical reaction model def chemical_reaction_model ( X ): \"\"\" Model of a chemical reaction with Arrhenius kinetics. Variables: - A: Pre-exponential factor (1/s) - Ea: Activation energy (J/mol) - T: Temperature (K) - Ca0: Initial concentration of reactant A (mol/L) - Cb0: Initial concentration of reactant B (mol/L) - t: Reaction time (s) - n: Reaction order for A - m: Reaction order for B \"\"\" A = X [:, 0 ] Ea = X [:, 1 ] T = X [:, 2 ] Ca0 = X [:, 3 ] Cb0 = X [:, 4 ] t = X [:, 5 ] n = X [:, 6 ] m = X [:, 7 ] # Gas constant R = 8.314 # J/(mol\u00b7K) # Arrhenius equation for rate constant k = A * np . exp ( - Ea / ( R * T )) # Simplified batch reactor model (analytical solution for equal orders) # For demonstration purposes, using a simplified conversion calculation if np . all ( np . abs ( n - m ) < 1e-6 ): # Check if n and m are approximately equal # For n=m=1 (second-order reaction) rate = k * t conversion = ( Ca0 * Cb0 * rate ) / ( 1 + Ca0 * rate ) else : # For different orders, use a simple approximation conversion = 1 - np . exp ( - k * t * ( Ca0 ** ( n - 1 )) * ( Cb0 ** m )) return conversion # Define data_info reaction_data = { 'variables' : [ { 'name' : 'A' , 'vars_type' : 'design_vars' , 'distribution' : 'lognormal' , 'range_bounds' : [ 1e7 , 1e10 ], 'cov' : 0.1 , 'description' : 'Pre-exponential factor (1/s)' }, { 'name' : 'Ea' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 50000 , 80000 ], 'cov' : 0.05 , 'description' : 'Activation energy (J/mol)' }, { 'name' : 'T' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 300 , 500 ], 'cov' : 0.02 , 'description' : 'Temperature (K)' }, { 'name' : 'Ca0' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.0 , 'std' : 0.1 , 'description' : 'Initial concentration of A (mol/L)' }, { 'name' : 'Cb0' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 1.5 , 'std' : 0.1 , 'description' : 'Initial concentration of B (mol/L)' }, { 'name' : 't' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 100 , 1000 ], 'description' : 'Reaction time (s)' }, { 'name' : 'n' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.8 , 1.2 ], 'description' : 'Reaction order for A' }, { 'name' : 'm' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.8 , 1.2 ], 'description' : 'Reaction order for B' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/reaction_data.json\" , 'w' ) as f : json . dump ( reaction_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/reaction_data.json\" , true_func = chemical_reaction_model , num_samples = 2048 , output_dir = \"RESULT_SA_REACTION\" , show_progress = True ) # Print results print ( \" \\n Chemical Reaction Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Chemical engineering insights top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Chemical Process Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Recommendations for Process Optimization:\" ) if 'T' in top_params : print ( \"- Temperature control is critical for reaction performance\" ) if 'A' in top_params or 'Ea' in top_params : print ( \"- Catalyst selection (affecting kinetic parameters) is important\" ) if 't' in top_params : print ( \"- Reaction time should be carefully optimized\" ) if 'n' in top_params or 'm' in top_params : print ( \"- The reaction mechanism should be thoroughly investigated\" ) Circuit Design Problem \u00b6 This example demonstrates sensitivity analysis for an electronic circuit design: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a circuit model (RC low-pass filter) def circuit_performance ( X ): \"\"\" Calculate the cutoff frequency and performance metrics of an RC filter. Variables: - R1: Resistor 1 value (ohms) - R2: Resistor 2 value (ohms) - C1: Capacitor 1 value (farads) - C2: Capacitor 2 value (farads) - Vin: Input voltage (volts) - T: Temperature (\u00b0C) - f: Frequency of interest (Hz) \"\"\" R1 = X [:, 0 ] R2 = X [:, 1 ] C1 = X [:, 2 ] C2 = X [:, 3 ] Vin = X [:, 4 ] T = X [:, 5 ] f = X [:, 6 ] # Temperature effect on resistors (simplified) alpha_R = 0.0004 # Temperature coefficient for resistors R1_actual = R1 * ( 1 + alpha_R * ( T - 25 )) R2_actual = R2 * ( 1 + alpha_R * ( T - 25 )) # Temperature effect on capacitors (simplified) alpha_C = - 0.0003 # Temperature coefficient for capacitors C1_actual = C1 * ( 1 + alpha_C * ( T - 25 )) C2_actual = C2 * ( 1 + alpha_C * ( T - 25 )) # Parallel combination of R2 and C2 Z_R2 = R2_actual Z_C2 = 1 / ( 2 j * np . pi * f * C2_actual ) Z_parallel = ( Z_R2 * Z_C2 ) / ( Z_R2 + Z_C2 ) # Series with R1 and C1 Z_R1 = R1_actual Z_C1 = 1 / ( 2 j * np . pi * f * C1_actual ) Z_total = Z_R1 + Z_C1 + Z_parallel # Calculate transfer function magnitude H = Z_parallel / Z_total gain_dB = 20 * np . log10 ( np . abs ( H )) # For sensitivity analysis, use the absolute gain loss # (Negative because we want to minimize loss) return - gain_dB # Define data_info circuit_data = { 'variables' : [ { 'name' : 'R1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1000 , 10000 ], # 1k to 10k ohms 'cov' : 0.05 , # 5% tolerance 'description' : 'Resistor 1 (ohms)' }, { 'name' : 'R2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 10000 , 100000 ], # 10k to 100k ohms 'cov' : 0.05 , # 5% tolerance 'description' : 'Resistor 2 (ohms)' }, { 'name' : 'C1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1e-9 , 1e-6 ], # 1nF to 1uF 'cov' : 0.1 , # 10% tolerance 'description' : 'Capacitor 1 (farads)' }, { 'name' : 'C2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1e-9 , 1e-6 ], # 1nF to 1uF 'cov' : 0.1 , # 10% tolerance 'description' : 'Capacitor 2 (farads)' }, { 'name' : 'Vin' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 5.0 , 'std' : 0.25 , # 5% variation 'description' : 'Input voltage (volts)' }, { 'name' : 'T' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 0 , 'high' : 70 , 'description' : 'Temperature (\u00b0C)' }, { 'name' : 'f' , 'vars_type' : 'env_vars' , 'distribution' : 'lognormal' , 'mean' : 1000 , # 1kHz 'cov' : 0.5 , 'description' : 'Frequency (Hz)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/circuit_data.json\" , 'w' ) as f : json . dump ( circuit_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/circuit_data.json\" , true_func = circuit_performance , num_samples = 2048 , output_dir = \"RESULT_SA_CIRCUIT\" , show_progress = True ) # Print results print ( \" \\n Circuit Design Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Electronics design insights top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Circuit Design Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Design Recommendations:\" ) for param in top_params : if param in [ 'R1' , 'R2' ]: print ( f \"- Use precision resistors for { param } with tighter tolerance\" ) elif param in [ 'C1' , 'C2' ]: print ( f \"- Use high-stability capacitors for { param } (e.g., film instead of ceramic)\" ) elif param == 'T' : print ( \"- Implement temperature compensation or temperature control\" ) elif param == 'f' : print ( \"- Design the circuit for a wider frequency bandwidth\" ) elif param == 'Vin' : print ( \"- Add voltage regulation to stabilize input voltage\" )","title":"MCS Approach"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#pyegrosensitivitysamcs-usage-examples","text":"This document provides examples of how to use the PyEGRO.sensitivity.SAmcs module for sensitivity analysis using Monte Carlo Simulation with the Sobol method.","title":"PyEGRO.sensitivity.SAmcs Usage Examples"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#table-of-contents","text":"Quick Start Basic Setup Running the Analysis Working with Different Models Using a Direct Function Using a Surrogate Model Example Applications Beam Design Problem Borehole Function Chemical Reaction Model Circuit Design Problem","title":"Table of Contents"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#quick-start","text":"","title":"Quick Start"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#basic-setup","text":"The simplest way to use the PyEGRO.sensitivity.SAmcs module is with the convenience function run_sensitivity_analysis . import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a simple test function def test_function ( X ): \"\"\" Ishigami function - common benchmark for sensitivity analysis. \"\"\" a = 7 b = 0.1 x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] return np . sin ( x1 ) + a * np . sin ( x2 ) ** 2 + b * x3 ** 4 * np . sin ( x1 ) # Define data_info with variable definitions data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'First input variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Second input variable' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Third input variable' } ] } # Save data_info to a JSON file import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run the analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , num_samples = 1024 , # Base sample size output_dir = \"RESULT_SA_ISHIGAMI\" , random_seed = 42 , show_progress = True ) # Print the results print ( \" \\n Sensitivity Indices:\" ) print ( results_df )","title":"Basic Setup"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#running-the-analysis","text":"For more control over the analysis process, you can use the MCSSensitivityAnalysis class directly: from PyEGRO.sensitivity.SAmcs import MCSSensitivityAnalysis # Initialize the analysis analyzer = MCSSensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , output_dir = \"RESULT_SA_DETAILED\" , show_variables_info = True , random_seed = 42 ) # Run the analysis results_df = analyzer . run_analysis ( num_samples = 2048 , # Increase sample size for better accuracy show_progress = True ) # Examine the summary statistics import json with open ( \"RESULT_SA_DETAILED/analysis_summary.json\" , 'r' ) as f : summary = json . load ( f ) print ( \" \\n Analysis Summary:\" ) for key , value in summary . items (): print ( f \" { key } : { value } \" )","title":"Running the Analysis"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#working-with-different-models","text":"","title":"Working with Different Models"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#using-a-direct-function","text":"For analytical functions or inexpensive simulations, you can directly use the function for evaluations: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a complex function with interactions def complex_function ( X ): \"\"\" Function with variable interactions and non-linear effects. \"\"\" x1 , x2 , x3 , x4 = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ] # Base effects base = 2 * x1 ** 2 + x2 + 0.5 * x3 + 0.1 * x4 # Interaction effects interactions = 5 * x1 * x2 + 2 * x2 * x3 + x3 * x4 + 0.5 * x1 * x4 # Non-linear effects non_linear = np . sin ( x1 ) * np . cos ( x2 ) + np . exp ( 0.1 * x3 ) + np . log ( abs ( x4 ) + 1 ) return base + interactions + non_linear # Define data_info data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Primary variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 2 , 2 ], 'cov' : 0.1 , 'description' : 'Secondary variable' }, { 'name' : 'x3' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 1 , 'description' : 'Environmental variable 1' }, { 'name' : 'x4' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 3 , 'high' : 3 , 'description' : 'Environmental variable 2' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/complex_data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/complex_data_info.json\" , true_func = complex_function , num_samples = 2048 , output_dir = \"RESULT_SA_COMPLEX\" , show_progress = True ) # Print top influential variables sorted_results = results_df . sort_values ( 'Total-order' , ascending = False ) print ( \" \\n Variables Ranked by Total-order Influence:\" ) print ( sorted_results [[ 'Parameter' , 'Total-order' , 'First-order' ]])","title":"Using a Direct Function"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#using-a-surrogate-model","text":"For computationally expensive models, you can use a surrogate model: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Load a pre-trained surrogate model model_handler = DeviceAgnosticGPR ( prefer_gpu = True ) model_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run analysis with the surrogate model results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , model_handler = model_handler , num_samples = 4096 , # Can use more samples since evaluations are cheap output_dir = \"RESULT_SA_SURROGATE\" , show_progress = True ) # Examine first-order vs total-order differences to detect interactions interaction_strength = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Strength' ] = interaction_strength print ( \" \\n Variable Interactions:\" ) for i , row in results_df . iterrows (): print ( f \" { row [ 'Parameter' ] } : Interaction Strength = { row [ 'Interaction_Strength' ] : .4f } \" )","title":"Using a Surrogate Model"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#example-applications","text":"","title":"Example Applications"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#beam-design-problem","text":"This example demonstrates sensitivity analysis for a beam design problem: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a beam design problem def beam_performance ( X ): \"\"\" Calculate the maximum deflection of a cantilever beam. Variables: - length: Beam length (m) - width: Beam width (m) - height: Beam height (m) - load: Applied load (N) - modulus: Young's modulus (Pa) \"\"\" length = X [:, 0 ] width = X [:, 1 ] height = X [:, 2 ] load = X [:, 3 ] modulus = X [:, 4 ] # Calculate moment of inertia inertia = ( width * height ** 3 ) / 12 # Calculate maximum deflection deflection = ( load * length ** 3 ) / ( 3 * modulus * inertia ) return deflection # Define data_info beam_data = { 'variables' : [ { 'name' : 'length' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1.0 , 3.0 ], 'cov' : 0.01 , 'description' : 'Beam length (m)' }, { 'name' : 'width' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.05 , 0.15 ], 'cov' : 0.02 , 'description' : 'Beam width (m)' }, { 'name' : 'height' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 0.1 , 0.3 ], 'cov' : 0.02 , 'description' : 'Beam height (m)' }, { 'name' : 'load' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 1000 , 'std' : 200 , 'description' : 'Applied load (N)' }, { 'name' : 'modulus' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.1e11 , 'std' : 1e10 , 'description' : 'Young \\' s modulus (Pa)' } ] } # Save data_info import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/beam_data.json\" , 'w' ) as f : json . dump ( beam_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/beam_data.json\" , true_func = beam_performance , num_samples = 2048 , output_dir = \"RESULT_SA_BEAM\" , show_progress = True ) # Analyze and interpret results print ( \" \\n Beam Design Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df ) # Load summary stats with open ( \"RESULT_SA_BEAM/analysis_summary.json\" , 'r' ) as f : summary = json . load ( f ) print ( \" \\n Design Insights:\" ) print ( f \"Most influential parameter: { summary [ 'most_influential_parameter' ] } \" ) print ( f \"Interaction strength: { summary [ 'interaction_strength' ] : .4f } \" ) print ( \" \\n Engineering Recommendations:\" ) # Sort by total-order sorted_params = results_df . sort_values ( 'Total-order' , ascending = False )[ 'Parameter' ] . tolist () print ( f \"1. Focus on controlling { sorted_params [ 0 ] } for maximum effect on performance\" ) print ( f \"2. Parameters to optimize first: { ', ' . join ( sorted_params [: 2 ]) } \" ) print ( f \"3. Parameters with minimal impact: { ', ' . join ( sorted_params [ - 2 :]) } \" )","title":"Beam Design Problem"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#borehole-function","text":"The borehole function is a common benchmark in engineering sensitivity analysis: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define the borehole function def borehole_function ( X ): \"\"\" Borehole function - models water flow through a borehole. \"\"\" rw = X [:, 0 ] # radius of borehole (m) r = X [:, 1 ] # radius of influence (m) Tu = X [:, 2 ] # transmissivity of upper aquifer (m\u00b2/year) Hu = X [:, 3 ] # potentiometric head of upper aquifer (m) Tl = X [:, 4 ] # transmissivity of lower aquifer (m\u00b2/year) Hl = X [:, 5 ] # potentiometric head of lower aquifer (m) L = X [:, 6 ] # length of borehole (m) Kw = X [:, 7 ] # hydraulic conductivity of borehole (m/year) numerator = 2 * np . pi * Tu * ( Hu - Hl ) denominator = np . log ( r / rw ) * ( 1 + ( 2 * L * Tu ) / ( np . log ( r / rw ) * rw ** 2 * Kw ) + Tu / Tl ) return numerator / denominator # Define data_info borehole_data = { 'variables' : [ { 'name' : 'rw' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.05 , 0.15 ], 'description' : 'Radius of borehole (m)' }, { 'name' : 'r' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 100 , 50000 ], 'description' : 'Radius of influence (m)' }, { 'name' : 'Tu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63070 , 'high' : 115600 , 'description' : 'Transmissivity of upper aquifer (m\u00b2/year)' }, { 'name' : 'Hu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 990 , 'high' : 1110 , 'description' : 'Potentiometric head of upper aquifer (m)' }, { 'name' : 'Tl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63.1 , 'high' : 116 , 'description' : 'Transmissivity of lower aquifer (m\u00b2/year)' }, { 'name' : 'Hl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 700 , 'high' : 820 , 'description' : 'Potentiometric head of lower aquifer (m)' }, { 'name' : 'L' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 1120 , 1680 ], 'description' : 'Length of borehole (m)' }, { 'name' : 'Kw' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 9855 , 'high' : 12045 , 'description' : 'Hydraulic conductivity of borehole (m/year)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/borehole_data.json\" , 'w' ) as f : json . dump ( borehole_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/borehole_data.json\" , true_func = borehole_function , num_samples = 2048 , output_dir = \"RESULT_SA_BOREHOLE\" , show_progress = True ) # Print results print ( \" \\n Borehole Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Calculate interaction effects interaction_effect = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Effect' ] = interaction_effect # Identify key parameters and interactions top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () top_interactions = results_df . sort_values ( 'Interaction_Effect' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Borehole Analysis Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( f \"Parameters with strongest interactions: { ', ' . join ( top_interactions ) } \" )","title":"Borehole Function"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#chemical-reaction-model","text":"This example illustrates sensitivity analysis for a chemical reaction kinetics model: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a chemical reaction model def chemical_reaction_model ( X ): \"\"\" Model of a chemical reaction with Arrhenius kinetics. Variables: - A: Pre-exponential factor (1/s) - Ea: Activation energy (J/mol) - T: Temperature (K) - Ca0: Initial concentration of reactant A (mol/L) - Cb0: Initial concentration of reactant B (mol/L) - t: Reaction time (s) - n: Reaction order for A - m: Reaction order for B \"\"\" A = X [:, 0 ] Ea = X [:, 1 ] T = X [:, 2 ] Ca0 = X [:, 3 ] Cb0 = X [:, 4 ] t = X [:, 5 ] n = X [:, 6 ] m = X [:, 7 ] # Gas constant R = 8.314 # J/(mol\u00b7K) # Arrhenius equation for rate constant k = A * np . exp ( - Ea / ( R * T )) # Simplified batch reactor model (analytical solution for equal orders) # For demonstration purposes, using a simplified conversion calculation if np . all ( np . abs ( n - m ) < 1e-6 ): # Check if n and m are approximately equal # For n=m=1 (second-order reaction) rate = k * t conversion = ( Ca0 * Cb0 * rate ) / ( 1 + Ca0 * rate ) else : # For different orders, use a simple approximation conversion = 1 - np . exp ( - k * t * ( Ca0 ** ( n - 1 )) * ( Cb0 ** m )) return conversion # Define data_info reaction_data = { 'variables' : [ { 'name' : 'A' , 'vars_type' : 'design_vars' , 'distribution' : 'lognormal' , 'range_bounds' : [ 1e7 , 1e10 ], 'cov' : 0.1 , 'description' : 'Pre-exponential factor (1/s)' }, { 'name' : 'Ea' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 50000 , 80000 ], 'cov' : 0.05 , 'description' : 'Activation energy (J/mol)' }, { 'name' : 'T' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 300 , 500 ], 'cov' : 0.02 , 'description' : 'Temperature (K)' }, { 'name' : 'Ca0' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.0 , 'std' : 0.1 , 'description' : 'Initial concentration of A (mol/L)' }, { 'name' : 'Cb0' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 1.5 , 'std' : 0.1 , 'description' : 'Initial concentration of B (mol/L)' }, { 'name' : 't' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 100 , 1000 ], 'description' : 'Reaction time (s)' }, { 'name' : 'n' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.8 , 1.2 ], 'description' : 'Reaction order for A' }, { 'name' : 'm' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.8 , 1.2 ], 'description' : 'Reaction order for B' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/reaction_data.json\" , 'w' ) as f : json . dump ( reaction_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/reaction_data.json\" , true_func = chemical_reaction_model , num_samples = 2048 , output_dir = \"RESULT_SA_REACTION\" , show_progress = True ) # Print results print ( \" \\n Chemical Reaction Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Chemical engineering insights top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Chemical Process Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Recommendations for Process Optimization:\" ) if 'T' in top_params : print ( \"- Temperature control is critical for reaction performance\" ) if 'A' in top_params or 'Ea' in top_params : print ( \"- Catalyst selection (affecting kinetic parameters) is important\" ) if 't' in top_params : print ( \"- Reaction time should be carefully optimized\" ) if 'n' in top_params or 'm' in top_params : print ( \"- The reaction mechanism should be thoroughly investigated\" )","title":"Chemical Reaction Model"},{"location":"basic-usage/sensitivity/approach-mcs/samcs_examples/#circuit-design-problem","text":"This example demonstrates sensitivity analysis for an electronic circuit design: import numpy as np from PyEGRO.sensitivity.SAmcs import run_sensitivity_analysis # Define a circuit model (RC low-pass filter) def circuit_performance ( X ): \"\"\" Calculate the cutoff frequency and performance metrics of an RC filter. Variables: - R1: Resistor 1 value (ohms) - R2: Resistor 2 value (ohms) - C1: Capacitor 1 value (farads) - C2: Capacitor 2 value (farads) - Vin: Input voltage (volts) - T: Temperature (\u00b0C) - f: Frequency of interest (Hz) \"\"\" R1 = X [:, 0 ] R2 = X [:, 1 ] C1 = X [:, 2 ] C2 = X [:, 3 ] Vin = X [:, 4 ] T = X [:, 5 ] f = X [:, 6 ] # Temperature effect on resistors (simplified) alpha_R = 0.0004 # Temperature coefficient for resistors R1_actual = R1 * ( 1 + alpha_R * ( T - 25 )) R2_actual = R2 * ( 1 + alpha_R * ( T - 25 )) # Temperature effect on capacitors (simplified) alpha_C = - 0.0003 # Temperature coefficient for capacitors C1_actual = C1 * ( 1 + alpha_C * ( T - 25 )) C2_actual = C2 * ( 1 + alpha_C * ( T - 25 )) # Parallel combination of R2 and C2 Z_R2 = R2_actual Z_C2 = 1 / ( 2 j * np . pi * f * C2_actual ) Z_parallel = ( Z_R2 * Z_C2 ) / ( Z_R2 + Z_C2 ) # Series with R1 and C1 Z_R1 = R1_actual Z_C1 = 1 / ( 2 j * np . pi * f * C1_actual ) Z_total = Z_R1 + Z_C1 + Z_parallel # Calculate transfer function magnitude H = Z_parallel / Z_total gain_dB = 20 * np . log10 ( np . abs ( H )) # For sensitivity analysis, use the absolute gain loss # (Negative because we want to minimize loss) return - gain_dB # Define data_info circuit_data = { 'variables' : [ { 'name' : 'R1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1000 , 10000 ], # 1k to 10k ohms 'cov' : 0.05 , # 5% tolerance 'description' : 'Resistor 1 (ohms)' }, { 'name' : 'R2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 10000 , 100000 ], # 10k to 100k ohms 'cov' : 0.05 , # 5% tolerance 'description' : 'Resistor 2 (ohms)' }, { 'name' : 'C1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1e-9 , 1e-6 ], # 1nF to 1uF 'cov' : 0.1 , # 10% tolerance 'description' : 'Capacitor 1 (farads)' }, { 'name' : 'C2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 1e-9 , 1e-6 ], # 1nF to 1uF 'cov' : 0.1 , # 10% tolerance 'description' : 'Capacitor 2 (farads)' }, { 'name' : 'Vin' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 5.0 , 'std' : 0.25 , # 5% variation 'description' : 'Input voltage (volts)' }, { 'name' : 'T' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 0 , 'high' : 70 , 'description' : 'Temperature (\u00b0C)' }, { 'name' : 'f' , 'vars_type' : 'env_vars' , 'distribution' : 'lognormal' , 'mean' : 1000 , # 1kHz 'cov' : 0.5 , 'description' : 'Frequency (Hz)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/circuit_data.json\" , 'w' ) as f : json . dump ( circuit_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/circuit_data.json\" , true_func = circuit_performance , num_samples = 2048 , output_dir = \"RESULT_SA_CIRCUIT\" , show_progress = True ) # Print results print ( \" \\n Circuit Design Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Electronics design insights top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Circuit Design Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Design Recommendations:\" ) for param in top_params : if param in [ 'R1' , 'R2' ]: print ( f \"- Use precision resistors for { param } with tighter tolerance\" ) elif param in [ 'C1' , 'C2' ]: print ( f \"- Use high-stability capacitors for { param } (e.g., film instead of ceramic)\" ) elif param == 'T' : print ( \"- Implement temperature compensation or temperature control\" ) elif param == 'f' : print ( \"- Design the circuit for a wider frequency bandwidth\" ) elif param == 'Vin' : print ( \"- Add voltage regulation to stabilize input voltage\" )","title":"Circuit Design Problem"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/","text":"PyEGRO.sensitivity.SApce Usage Examples \u00b6 This document provides examples of how to use the PyEGRO.sensitivity.SApce module for sensitivity analysis using Polynomial Chaos Expansion (PCE). Table of Contents \u00b6 Quick Start Basic Setup Running the Analysis Working with Different Models Using a Direct Function Using a Surrogate Model Example Applications Ishigami Function Rosenbrock Function Borehole Function Circuit Design Problem Quick Start \u00b6 Basic Setup \u00b6 The simplest way to use the PyEGRO.sensitivity.SApce module is with the convenience function run_sensitivity_analysis . import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define a simple test function def test_function ( X ): \"\"\" Simple quadratic function with interactions. \"\"\" x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] return x1 ** 2 + 0.5 * x2 ** 2 + 0.1 * x3 ** 2 + 0.7 * x1 * x2 + 0.2 * x2 * x3 # Define data_info with variable definitions data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'First input variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'Second input variable' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'Third input variable' } ] } # Save data_info to a JSON file import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run the analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , polynomial_order = 3 , num_samples = 500 , output_dir = \"RESULT_SA_QUADRATIC\" , random_seed = 42 , show_progress = True ) # Print the results print ( \" \\n Sensitivity Indices:\" ) print ( results_df ) Running the Analysis \u00b6 For more control over the analysis process, you can use the PCESensitivityAnalysis class directly: from PyEGRO.sensitivity.SApce import PCESensitivityAnalysis # Initialize the analysis analyzer = PCESensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , output_dir = \"RESULT_SA_DETAILED\" , show_variables_info = True , random_seed = 42 ) # Run the analysis results_df = analyzer . run_analysis ( polynomial_order = 4 , # Higher order for more accuracy num_samples = 1000 , show_progress = True ) # Examine the summary statistics import json with open ( \"RESULT_SA_DETAILED/analysis_summary.json\" , 'r' ) as f : summary = json . load ( f ) print ( \" \\n Analysis Summary:\" ) for key , value in summary . items (): print ( f \" { key } : { value } \" ) Working with Different Models \u00b6 Using a Direct Function \u00b6 For analytical functions or inexpensive simulations, you can directly use the function for evaluations: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define a complex function with interactions def complex_function ( X ): \"\"\" Function with various non-linear effects and interactions. \"\"\" x1 , x2 , x3 , x4 = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ] # Different term types linear = 2 * x1 + x2 quadratic = 3 * x1 ** 2 + 0.5 * x2 ** 2 + 0.1 * x3 ** 2 interaction = 2 * x1 * x2 + 0.7 * x2 * x3 + 0.3 * x3 * x4 non_linear = np . sin ( x1 ) + np . exp ( 0.1 * x2 ) + np . sqrt ( np . abs ( x3 )) + np . log ( np . abs ( x4 ) + 1 ) return linear + quadratic + interaction + non_linear # Define data_info data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'First variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 2 , 2 ], 'cov' : 0.1 , 'description' : 'Second variable' }, { 'name' : 'x3' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 1 , 'description' : 'Third variable' }, { 'name' : 'x4' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 2 , 'high' : 2 , 'description' : 'Fourth variable' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/complex_data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/complex_data_info.json\" , true_func = complex_function , polynomial_order = 4 , num_samples = 1000 , output_dir = \"RESULT_SA_COMPLEX\" , show_progress = True ) # Print top influential variables sorted_results = results_df . sort_values ( 'Total-order' , ascending = False ) print ( \" \\n Variables Ranked by Total-order Influence:\" ) print ( sorted_results [[ 'Parameter' , 'Total-order' , 'First-order' ]]) Using a Surrogate Model \u00b6 For computationally expensive models, you can use a surrogate model: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Load a pre-trained surrogate model model_handler = DeviceAgnosticGPR ( prefer_gpu = True ) model_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run analysis with the surrogate model results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , model_handler = model_handler , polynomial_order = 5 , # Can use higher order since evaluations are cheap num_samples = 2000 , # Can use more samples since evaluations are cheap output_dir = \"RESULT_SA_SURROGATE\" , show_progress = True ) # Examine first-order vs total-order differences to detect interactions interaction_strength = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Strength' ] = interaction_strength print ( \" \\n Variable Interactions:\" ) for i , row in results_df . iterrows (): print ( f \" { row [ 'Parameter' ] } : Interaction Strength = { row [ 'Interaction_Strength' ] : .4f } \" ) Example Applications \u00b6 Ishigami Function \u00b6 The Ishigami function is a common benchmark for sensitivity analysis: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define the Ishigami function def ishigami_function ( X ): \"\"\" Ishigami function - common benchmark for sensitivity analysis. \"\"\" a = 7 b = 0.1 x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] return np . sin ( x1 ) + a * np . sin ( x2 ) ** 2 + b * x3 ** 4 * np . sin ( x1 ) # Define data_info ishigami_data = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'First variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Second variable' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Third variable' } ] } # Save data_info import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/ishigami_data.json\" , 'w' ) as f : json . dump ( ishigami_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/ishigami_data.json\" , true_func = ishigami_function , polynomial_order = 6 , # Higher order needed for trigonometric functions num_samples = 1000 , output_dir = \"RESULT_SA_ISHIGAMI\" , show_progress = True ) # Print results print ( \" \\n Ishigami Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df ) # Validate against known analytical values print ( \" \\n Analytical Comparison:\" ) print ( \"First-order indices should be approximately:\" ) print ( \"x1: 0.314, x2: 0.442, x3: 0.0\" ) print ( \"Total-order indices should be approximately:\" ) print ( \"x1: 0.558, x2: 0.442, x3: 0.244\" ) Rosenbrock Function \u00b6 The Rosenbrock function demonstrates sensitivity analysis on an optimization benchmark: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define the Rosenbrock function def rosenbrock_function ( X ): \"\"\" Rosenbrock function - common optimization benchmark. f(x,y) = (a-x)\u00b2 + b(y-x\u00b2)\u00b2 \"\"\" n_dim = X . shape [ 1 ] result = 0 for i in range ( n_dim - 1 ): a = 1 b = 100 result += ( a - X [:, i ]) ** 2 + b * ( X [:, i + 1 ] - X [:, i ] ** 2 ) ** 2 return result # Define data_info for 4D Rosenbrock rosenbrock_data = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'First dimension' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Second dimension' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Third dimension' }, { 'name' : 'x4' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Fourth dimension' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/rosenbrock_data.json\" , 'w' ) as f : json . dump ( rosenbrock_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/rosenbrock_data.json\" , true_func = rosenbrock_function , polynomial_order = 5 , num_samples = 1000 , output_dir = \"RESULT_SA_ROSENBROCK\" , show_progress = True ) # Print results print ( \" \\n Rosenbrock Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df ) # Calculate interaction effects interaction_effect = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Effect' ] = interaction_effect # Print interaction analysis print ( \" \\n Variable Interaction Analysis:\" ) for i , row in results_df . iterrows (): if row [ 'Interaction_Effect' ] > 0.1 : print ( f \" { row [ 'Parameter' ] } : Strong interaction effect ( { row [ 'Interaction_Effect' ] : .4f } )\" ) elif row [ 'Interaction_Effect' ] > 0.01 : print ( f \" { row [ 'Parameter' ] } : Moderate interaction effect ( { row [ 'Interaction_Effect' ] : .4f } )\" ) else : print ( f \" { row [ 'Parameter' ] } : Weak interaction effect ( { row [ 'Interaction_Effect' ] : .4f } )\" ) Borehole Function \u00b6 The borehole function is a common benchmark in engineering sensitivity analysis: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define the borehole function def borehole_function ( X ): \"\"\" Borehole function - models water flow through a borehole. \"\"\" rw = X [:, 0 ] # radius of borehole (m) r = X [:, 1 ] # radius of influence (m) Tu = X [:, 2 ] # transmissivity of upper aquifer (m\u00b2/year) Hu = X [:, 3 ] # potentiometric head of upper aquifer (m) Tl = X [:, 4 ] # transmissivity of lower aquifer (m\u00b2/year) Hl = X [:, 5 ] # potentiometric head of lower aquifer (m) L = X [:, 6 ] # length of borehole (m) Kw = X [:, 7 ] # hydraulic conductivity of borehole (m/year) numerator = 2 * np . pi * Tu * ( Hu - Hl ) denominator = np . log ( r / rw ) * ( 1 + ( 2 * L * Tu ) / ( np . log ( r / rw ) * rw ** 2 * Kw ) + Tu / Tl ) return numerator / denominator # Define data_info borehole_data = { 'variables' : [ { 'name' : 'rw' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.05 , 0.15 ], 'description' : 'Radius of borehole (m)' }, { 'name' : 'r' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 100 , 50000 ], 'description' : 'Radius of influence (m)' }, { 'name' : 'Tu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63070 , 'high' : 115600 , 'description' : 'Transmissivity of upper aquifer (m\u00b2/year)' }, { 'name' : 'Hu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 990 , 'high' : 1110 , 'description' : 'Potentiometric head of upper aquifer (m)' }, { 'name' : 'Tl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63.1 , 'high' : 116 , 'description' : 'Transmissivity of lower aquifer (m\u00b2/year)' }, { 'name' : 'Hl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 700 , 'high' : 820 , 'description' : 'Potentiometric head of lower aquifer (m)' }, { 'name' : 'L' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 1120 , 1680 ], 'description' : 'Length of borehole (m)' }, { 'name' : 'Kw' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 9855 , 'high' : 12045 , 'description' : 'Hydraulic conductivity of borehole (m/year)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/borehole_data.json\" , 'w' ) as f : json . dump ( borehole_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/borehole_data.json\" , true_func = borehole_function , polynomial_order = 3 , num_samples = 1000 , output_dir = \"RESULT_SA_BOREHOLE\" , show_progress = True ) # Print results print ( \" \\n Borehole Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Identify key parameters top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Borehole Analysis Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Engineering Recommendations:\" ) for param in top_params : print ( f \"- Focus on accurate measurement and control of { param } \" ) Circuit Design Problem \u00b6 This example demonstrates sensitivity analysis for an electronic circuit design: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define a circuit model (RC low-pass filter) def circuit_performance ( X ): \"\"\" Calculate the cutoff frequency and performance metrics of an RC filter. Variables: - R1: Resistor 1 value (ohms) - R2: Resistor 2 value (ohms) - C1: Capacitor 1 value (farads) - C2: Capacitor 2 value (farads) - f: Frequency of interest (Hz) \"\"\" R1 = X [:, 0 ] R2 = X [:, 1 ] C1 = X [:, 2 ] C2 = X [:, 3 ] f = X [:, 4 ] # Impedances Z_R1 = R1 Z_R2 = R2 Z_C1 = 1 / ( 2 j * np . pi * f * C1 ) Z_C2 = 1 / ( 2 j * np . pi * f * C2 ) # Parallel combination of R2 and C2 Z_parallel = ( Z_R2 * Z_C2 ) / ( Z_R2 + Z_C2 ) # Series with R1 and C1 Z_total = Z_R1 + Z_C1 + Z_parallel # Calculate transfer function magnitude H = Z_parallel / Z_total gain_dB = 20 * np . log10 ( np . abs ( H )) # For sensitivity analysis, use the absolute gain loss return - gain_dB # Define data_info circuit_data = { 'variables' : [ { 'name' : 'R1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 980 , 1020 ], # 1k\u03a9 with 2% tolerance 'cov' : 0.02 , 'description' : 'Resistor 1 (ohms)' }, { 'name' : 'R2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 9800 , 10200 ], # 10k\u03a9 with 2% tolerance 'cov' : 0.02 , 'description' : 'Resistor 2 (ohms)' }, { 'name' : 'C1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 9.5e-9 , 10.5e-9 ], # 10nF with 5% tolerance 'cov' : 0.05 , 'description' : 'Capacitor 1 (farads)' }, { 'name' : 'C2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 9.5e-9 , 10.5e-9 ], # 10nF with 5% tolerance 'cov' : 0.05 , 'description' : 'Capacitor 2 (farads)' }, { 'name' : 'f' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 1000 , 'high' : 10000 , 'description' : 'Frequency (Hz)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/circuit_data.json\" , 'w' ) as f : json . dump ( circuit_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/circuit_data.json\" , true_func = circuit_performance , polynomial_order = 4 , num_samples = 800 , output_dir = \"RESULT_SA_CIRCUIT\" , show_progress = True ) # Print results print ( \" \\n Circuit Design Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Electronics design insights top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 2 )[ 'Parameter' ] . tolist () print ( \" \\n Circuit Design Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Design Recommendations:\" ) for param in top_params : if param in [ 'R1' , 'R2' ]: print ( f \"- Use precision resistors for { param } with tighter tolerance\" ) elif param in [ 'C1' , 'C2' ]: print ( f \"- Use high-stability capacitors for { param } (e.g., film instead of ceramic)\" ) elif param == 'f' : print ( \"- Design the circuit for the specific frequency range of interest\" )","title":"PCE Approach"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#pyegrosensitivitysapce-usage-examples","text":"This document provides examples of how to use the PyEGRO.sensitivity.SApce module for sensitivity analysis using Polynomial Chaos Expansion (PCE).","title":"PyEGRO.sensitivity.SApce Usage Examples"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#table-of-contents","text":"Quick Start Basic Setup Running the Analysis Working with Different Models Using a Direct Function Using a Surrogate Model Example Applications Ishigami Function Rosenbrock Function Borehole Function Circuit Design Problem","title":"Table of Contents"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#quick-start","text":"","title":"Quick Start"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#basic-setup","text":"The simplest way to use the PyEGRO.sensitivity.SApce module is with the convenience function run_sensitivity_analysis . import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define a simple test function def test_function ( X ): \"\"\" Simple quadratic function with interactions. \"\"\" x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] return x1 ** 2 + 0.5 * x2 ** 2 + 0.1 * x3 ** 2 + 0.7 * x1 * x2 + 0.2 * x2 * x3 # Define data_info with variable definitions data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'First input variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'Second input variable' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'Third input variable' } ] } # Save data_info to a JSON file import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run the analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , polynomial_order = 3 , num_samples = 500 , output_dir = \"RESULT_SA_QUADRATIC\" , random_seed = 42 , show_progress = True ) # Print the results print ( \" \\n Sensitivity Indices:\" ) print ( results_df )","title":"Basic Setup"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#running-the-analysis","text":"For more control over the analysis process, you can use the PCESensitivityAnalysis class directly: from PyEGRO.sensitivity.SApce import PCESensitivityAnalysis # Initialize the analysis analyzer = PCESensitivityAnalysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = test_function , output_dir = \"RESULT_SA_DETAILED\" , show_variables_info = True , random_seed = 42 ) # Run the analysis results_df = analyzer . run_analysis ( polynomial_order = 4 , # Higher order for more accuracy num_samples = 1000 , show_progress = True ) # Examine the summary statistics import json with open ( \"RESULT_SA_DETAILED/analysis_summary.json\" , 'r' ) as f : summary = json . load ( f ) print ( \" \\n Analysis Summary:\" ) for key , value in summary . items (): print ( f \" { key } : { value } \" )","title":"Running the Analysis"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#working-with-different-models","text":"","title":"Working with Different Models"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#using-a-direct-function","text":"For analytical functions or inexpensive simulations, you can directly use the function for evaluations: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define a complex function with interactions def complex_function ( X ): \"\"\" Function with various non-linear effects and interactions. \"\"\" x1 , x2 , x3 , x4 = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ] # Different term types linear = 2 * x1 + x2 quadratic = 3 * x1 ** 2 + 0.5 * x2 ** 2 + 0.1 * x3 ** 2 interaction = 2 * x1 * x2 + 0.7 * x2 * x3 + 0.3 * x3 * x4 non_linear = np . sin ( x1 ) + np . exp ( 0.1 * x2 ) + np . sqrt ( np . abs ( x3 )) + np . log ( np . abs ( x4 ) + 1 ) return linear + quadratic + interaction + non_linear # Define data_info data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 1 , 1 ], 'description' : 'First variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ - 2 , 2 ], 'cov' : 0.1 , 'description' : 'Second variable' }, { 'name' : 'x3' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 1 , 'description' : 'Third variable' }, { 'name' : 'x4' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : - 2 , 'high' : 2 , 'description' : 'Fourth variable' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/complex_data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/complex_data_info.json\" , true_func = complex_function , polynomial_order = 4 , num_samples = 1000 , output_dir = \"RESULT_SA_COMPLEX\" , show_progress = True ) # Print top influential variables sorted_results = results_df . sort_values ( 'Total-order' , ascending = False ) print ( \" \\n Variables Ranked by Total-order Influence:\" ) print ( sorted_results [[ 'Parameter' , 'Total-order' , 'First-order' ]])","title":"Using a Direct Function"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#using-a-surrogate-model","text":"For computationally expensive models, you can use a surrogate model: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Load a pre-trained surrogate model model_handler = DeviceAgnosticGPR ( prefer_gpu = True ) model_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run analysis with the surrogate model results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , model_handler = model_handler , polynomial_order = 5 , # Can use higher order since evaluations are cheap num_samples = 2000 , # Can use more samples since evaluations are cheap output_dir = \"RESULT_SA_SURROGATE\" , show_progress = True ) # Examine first-order vs total-order differences to detect interactions interaction_strength = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Strength' ] = interaction_strength print ( \" \\n Variable Interactions:\" ) for i , row in results_df . iterrows (): print ( f \" { row [ 'Parameter' ] } : Interaction Strength = { row [ 'Interaction_Strength' ] : .4f } \" )","title":"Using a Surrogate Model"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#example-applications","text":"","title":"Example Applications"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#ishigami-function","text":"The Ishigami function is a common benchmark for sensitivity analysis: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define the Ishigami function def ishigami_function ( X ): \"\"\" Ishigami function - common benchmark for sensitivity analysis. \"\"\" a = 7 b = 0.1 x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] return np . sin ( x1 ) + a * np . sin ( x2 ) ** 2 + b * x3 ** 4 * np . sin ( x1 ) # Define data_info ishigami_data = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'First variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Second variable' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - np . pi , np . pi ], 'description' : 'Third variable' } ] } # Save data_info import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/ishigami_data.json\" , 'w' ) as f : json . dump ( ishigami_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/ishigami_data.json\" , true_func = ishigami_function , polynomial_order = 6 , # Higher order needed for trigonometric functions num_samples = 1000 , output_dir = \"RESULT_SA_ISHIGAMI\" , show_progress = True ) # Print results print ( \" \\n Ishigami Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df ) # Validate against known analytical values print ( \" \\n Analytical Comparison:\" ) print ( \"First-order indices should be approximately:\" ) print ( \"x1: 0.314, x2: 0.442, x3: 0.0\" ) print ( \"Total-order indices should be approximately:\" ) print ( \"x1: 0.558, x2: 0.442, x3: 0.244\" )","title":"Ishigami Function"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#rosenbrock-function","text":"The Rosenbrock function demonstrates sensitivity analysis on an optimization benchmark: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define the Rosenbrock function def rosenbrock_function ( X ): \"\"\" Rosenbrock function - common optimization benchmark. f(x,y) = (a-x)\u00b2 + b(y-x\u00b2)\u00b2 \"\"\" n_dim = X . shape [ 1 ] result = 0 for i in range ( n_dim - 1 ): a = 1 b = 100 result += ( a - X [:, i ]) ** 2 + b * ( X [:, i + 1 ] - X [:, i ] ** 2 ) ** 2 return result # Define data_info for 4D Rosenbrock rosenbrock_data = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'First dimension' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Second dimension' }, { 'name' : 'x3' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Third dimension' }, { 'name' : 'x4' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ - 2 , 2 ], 'description' : 'Fourth dimension' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/rosenbrock_data.json\" , 'w' ) as f : json . dump ( rosenbrock_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/rosenbrock_data.json\" , true_func = rosenbrock_function , polynomial_order = 5 , num_samples = 1000 , output_dir = \"RESULT_SA_ROSENBROCK\" , show_progress = True ) # Print results print ( \" \\n Rosenbrock Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df ) # Calculate interaction effects interaction_effect = results_df [ 'Total-order' ] - results_df [ 'First-order' ] results_df [ 'Interaction_Effect' ] = interaction_effect # Print interaction analysis print ( \" \\n Variable Interaction Analysis:\" ) for i , row in results_df . iterrows (): if row [ 'Interaction_Effect' ] > 0.1 : print ( f \" { row [ 'Parameter' ] } : Strong interaction effect ( { row [ 'Interaction_Effect' ] : .4f } )\" ) elif row [ 'Interaction_Effect' ] > 0.01 : print ( f \" { row [ 'Parameter' ] } : Moderate interaction effect ( { row [ 'Interaction_Effect' ] : .4f } )\" ) else : print ( f \" { row [ 'Parameter' ] } : Weak interaction effect ( { row [ 'Interaction_Effect' ] : .4f } )\" )","title":"Rosenbrock Function"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#borehole-function","text":"The borehole function is a common benchmark in engineering sensitivity analysis: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define the borehole function def borehole_function ( X ): \"\"\" Borehole function - models water flow through a borehole. \"\"\" rw = X [:, 0 ] # radius of borehole (m) r = X [:, 1 ] # radius of influence (m) Tu = X [:, 2 ] # transmissivity of upper aquifer (m\u00b2/year) Hu = X [:, 3 ] # potentiometric head of upper aquifer (m) Tl = X [:, 4 ] # transmissivity of lower aquifer (m\u00b2/year) Hl = X [:, 5 ] # potentiometric head of lower aquifer (m) L = X [:, 6 ] # length of borehole (m) Kw = X [:, 7 ] # hydraulic conductivity of borehole (m/year) numerator = 2 * np . pi * Tu * ( Hu - Hl ) denominator = np . log ( r / rw ) * ( 1 + ( 2 * L * Tu ) / ( np . log ( r / rw ) * rw ** 2 * Kw ) + Tu / Tl ) return numerator / denominator # Define data_info borehole_data = { 'variables' : [ { 'name' : 'rw' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 0.05 , 0.15 ], 'description' : 'Radius of borehole (m)' }, { 'name' : 'r' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 100 , 50000 ], 'description' : 'Radius of influence (m)' }, { 'name' : 'Tu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63070 , 'high' : 115600 , 'description' : 'Transmissivity of upper aquifer (m\u00b2/year)' }, { 'name' : 'Hu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 990 , 'high' : 1110 , 'description' : 'Potentiometric head of upper aquifer (m)' }, { 'name' : 'Tl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63.1 , 'high' : 116 , 'description' : 'Transmissivity of lower aquifer (m\u00b2/year)' }, { 'name' : 'Hl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 700 , 'high' : 820 , 'description' : 'Potentiometric head of lower aquifer (m)' }, { 'name' : 'L' , 'vars_type' : 'design_vars' , 'distribution' : 'uniform' , 'range_bounds' : [ 1120 , 1680 ], 'description' : 'Length of borehole (m)' }, { 'name' : 'Kw' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 9855 , 'high' : 12045 , 'description' : 'Hydraulic conductivity of borehole (m/year)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/borehole_data.json\" , 'w' ) as f : json . dump ( borehole_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/borehole_data.json\" , true_func = borehole_function , polynomial_order = 3 , num_samples = 1000 , output_dir = \"RESULT_SA_BOREHOLE\" , show_progress = True ) # Print results print ( \" \\n Borehole Function Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Identify key parameters top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 3 )[ 'Parameter' ] . tolist () print ( \" \\n Borehole Analysis Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Engineering Recommendations:\" ) for param in top_params : print ( f \"- Focus on accurate measurement and control of { param } \" )","title":"Borehole Function"},{"location":"basic-usage/sensitivity/approach-pce/sapce_examples/#circuit-design-problem","text":"This example demonstrates sensitivity analysis for an electronic circuit design: import numpy as np from PyEGRO.sensitivity.SApce import run_sensitivity_analysis # Define a circuit model (RC low-pass filter) def circuit_performance ( X ): \"\"\" Calculate the cutoff frequency and performance metrics of an RC filter. Variables: - R1: Resistor 1 value (ohms) - R2: Resistor 2 value (ohms) - C1: Capacitor 1 value (farads) - C2: Capacitor 2 value (farads) - f: Frequency of interest (Hz) \"\"\" R1 = X [:, 0 ] R2 = X [:, 1 ] C1 = X [:, 2 ] C2 = X [:, 3 ] f = X [:, 4 ] # Impedances Z_R1 = R1 Z_R2 = R2 Z_C1 = 1 / ( 2 j * np . pi * f * C1 ) Z_C2 = 1 / ( 2 j * np . pi * f * C2 ) # Parallel combination of R2 and C2 Z_parallel = ( Z_R2 * Z_C2 ) / ( Z_R2 + Z_C2 ) # Series with R1 and C1 Z_total = Z_R1 + Z_C1 + Z_parallel # Calculate transfer function magnitude H = Z_parallel / Z_total gain_dB = 20 * np . log10 ( np . abs ( H )) # For sensitivity analysis, use the absolute gain loss return - gain_dB # Define data_info circuit_data = { 'variables' : [ { 'name' : 'R1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 980 , 1020 ], # 1k\u03a9 with 2% tolerance 'cov' : 0.02 , 'description' : 'Resistor 1 (ohms)' }, { 'name' : 'R2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 9800 , 10200 ], # 10k\u03a9 with 2% tolerance 'cov' : 0.02 , 'description' : 'Resistor 2 (ohms)' }, { 'name' : 'C1' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 9.5e-9 , 10.5e-9 ], # 10nF with 5% tolerance 'cov' : 0.05 , 'description' : 'Capacitor 1 (farads)' }, { 'name' : 'C2' , 'vars_type' : 'design_vars' , 'distribution' : 'normal' , 'range_bounds' : [ 9.5e-9 , 10.5e-9 ], # 10nF with 5% tolerance 'cov' : 0.05 , 'description' : 'Capacitor 2 (farads)' }, { 'name' : 'f' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 1000 , 'high' : 10000 , 'description' : 'Frequency (Hz)' } ] } # Save data_info import json with open ( \"DATA_PREPARATION/circuit_data.json\" , 'w' ) as f : json . dump ( circuit_data , f ) # Run analysis results_df = run_sensitivity_analysis ( data_info_path = \"DATA_PREPARATION/circuit_data.json\" , true_func = circuit_performance , polynomial_order = 4 , num_samples = 800 , output_dir = \"RESULT_SA_CIRCUIT\" , show_progress = True ) # Print results print ( \" \\n Circuit Design Sensitivity Analysis Results:\" ) print ( \"-\" * 50 ) print ( results_df . sort_values ( 'Total-order' , ascending = False )) # Electronics design insights top_params = results_df . sort_values ( 'Total-order' , ascending = False ) . head ( 2 )[ 'Parameter' ] . tolist () print ( \" \\n Circuit Design Insights:\" ) print ( f \"Most influential parameters: { ', ' . join ( top_params ) } \" ) print ( \" \\n Design Recommendations:\" ) for param in top_params : if param in [ 'R1' , 'R2' ]: print ( f \"- Use precision resistors for { param } with tighter tolerance\" ) elif param in [ 'C1' , 'C2' ]: print ( f \"- Use high-stability capacitors for { param } (e.g., film instead of ceramic)\" ) elif param == 'f' : print ( \"- Design the circuit for the specific frequency range of interest\" )","title":"Circuit Design Problem"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/","text":"PyEGRO Uncertainty Quantification Module Usage Guide \u00b6 (PyEGRO.uncertainty.UQmcs) This guide provides examples and usage patterns for PyEGRO's Uncertainty Quantification module using Monte Carlo Simulation (UQmcs). The module helps you analyze how uncertainty in input variables propagates to output responses, making it useful for robust design and risk assessment. Table of Contents \u00b6 Quick Start \u00b6 Basic Setup Running the Analysis Uncertainty Specification Methods \u00b6 Using Coefficient of Variation (CoV) Using Standard Deviation (Std) Using Delta for Uniform Uncertainty Handling Near-Zero Regions Working with Different Models \u00b6 Using a Direct Function Using a Metamodel Example Applications \u00b6 1D Multi-modal Function 2D Multi-modal Function Ishigami Function Beam Analysis with Material Density Uncertainty Borehole Function Chemical Process Model Circuit Design Problem Advance Usage \u00b6 1. Quick Start \u00b6 Basic Setup Before running any analysis, you need to define your variables (design variables and environmental variables) in a configuration file. Here's how to set up a simple configuration: import numpy as np import json import os # Create a dictionary defining your variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 10 ], 'cov' : 0.05 , # 5% coefficient of variation 'distribution' : 'normal' , 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], 'std' : 0.2 , # Using standard deviation instead of CoV 'distribution' : 'normal' , 'description' : 'Second design variable' }, { 'name' : 'noise' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.1 , 'description' : 'Measurement noise' } ] } # Save the configuration to a JSON file os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) Running the Analysis \u00b6 Once your configuration is ready, you can run the uncertainty propagation analysis: from PyEGRO.uncertainty.UQmcs import run_uncertainty_analysis # Define your objective function def simple_function ( X ): return X [:, 0 ] ** 2 + X [:, 1 ] + 0.5 * X [:, 2 ] # Run the analysis results = run_uncertainty_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = simple_function , num_design_samples = 500 , # Number of design points to evaluate num_mcs_samples = 10000 , # Number of Monte Carlo samples per design point output_dir = \"RESULT_QOI\" , # Directory to save results show_progress = True ) # The results are also available as a pandas DataFrame print ( results . head ()) 2. Uncertainty Specification Methods \u00b6 The UQmcs module supports multiple ways to specify uncertainty for design and environmental variables. Using Coefficient of Variation (CoV) \u00b6 Coefficient of Variation (CoV) is the ratio of the standard deviation to the mean, expressed as a decimal or percentage. It's useful when the uncertainty scales with the magnitude of the variable. import numpy as np import json from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define a simple quadratic function def quadratic_func ( X ): return X [:, 0 ] ** 2 # Setup with CoV data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1 , 10 ], # Positive range (good for CoV) 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' , 'description' : 'Design variable with CoV uncertainty' } ] } # Save and run analysis with open ( \"DATA_PREPARATION/cov_example.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/cov_example.json\" , true_func = quadratic_func , output_dir = \"RESULT_COV\" ) results_cov = uq_cov . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 ) Using Standard Deviation (Std) \u00b6 Standard deviation specifies the absolute width of the uncertainty distribution. This is preferable when the variable range crosses zero or when you want a constant uncertainty regardless of the mean value. # Setup with standard deviation data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], # Range crosses zero (better to use std) 'std' : 0.3 , # Fixed standard deviation of 0.3 'distribution' : 'normal' , 'description' : 'Design variable with std uncertainty' } ] } # Save and run analysis with open ( \"DATA_PREPARATION/std_example.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_std = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/std_example.json\" , true_func = quadratic_func , output_dir = \"RESULT_STD\" ) results_std = uq_std . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 ) Using Delta for Uniform Uncertainty \u00b6 The delta parameter allows you to specify uniform uncertainty around design points, which is useful when you prefer a uniform distribution rather than normal. # Setup with delta for uniform uncertainty data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 10 ], 'delta' : 0.5 , # Half-width of 0.5 around design points 'distribution' : 'uniform' , 'description' : 'Design variable with uniform uncertainty' } ] } # Save and run analysis with open ( \"DATA_PREPARATION/delta_example.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_delta = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/delta_example.json\" , true_func = quadratic_func , output_dir = \"RESULT_DELTA\" ) results_delta = uq_delta . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 ) Handling Near-Zero Regions \u00b6 When variables cross near zero and you use CoV, issues can arise because CoV is undefined at zero. The updated code handles this automatically, but it's best to use std for such cases. import numpy as np import json from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a function that's sensitive to the sign of x def sine_func ( X ): return np . sin ( X [:, 0 ]) # Compare CoV and Std for a variable that crosses zero data_info_cov = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 3.14 , 3.14 ], # Crosses zero 'cov' : 0.1 , # CoV will be handled specially near zero 'distribution' : 'normal' , 'description' : 'Design variable with CoV (crossing zero)' } ] } data_info_std = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 3.14 , 3.14 ], # Crosses zero 'std' : 0.2 , # Better approach for variables crossing zero 'distribution' : 'normal' , 'description' : 'Design variable with std (crossing zero)' } ] } # Save configurations with open ( \"DATA_PREPARATION/near_zero_cov.json\" , 'w' ) as f : json . dump ( data_info_cov , f ) with open ( \"DATA_PREPARATION/near_zero_std.json\" , 'w' ) as f : json . dump ( data_info_std , f ) # Run analyses uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/near_zero_cov.json\" , true_func = sine_func , output_dir = \"RESULT_NEAR_ZERO_COV\" ) uq_std = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/near_zero_std.json\" , true_func = sine_func , output_dir = \"RESULT_NEAR_ZERO_STD\" ) results_cov = uq_cov . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) results_std = uq_std . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Plot to compare plt . figure ( figsize = ( 12 , 5 )) plt . subplot ( 1 , 2 , 1 ) plt . scatter ( results_cov [ 'x' ], results_cov [ 'Mean' ], c = results_cov [ 'StdDev' ], cmap = 'viridis' ) plt . colorbar ( label = 'Standard Deviation' ) plt . title ( 'Using CoV for Variable Crossing Zero' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Mean Response' ) plt . subplot ( 1 , 2 , 2 ) plt . scatter ( results_std [ 'x' ], results_std [ 'Mean' ], c = results_std [ 'StdDev' ], cmap = 'viridis' ) plt . colorbar ( label = 'Standard Deviation' ) plt . title ( 'Using Std for Variable Crossing Zero' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Mean Response' ) plt . tight_layout () plt . savefig ( \"near_zero_comparison.png\" ) plt . show () 3. Working with Different Models \u00b6 Using a Direct Function \u00b6 For analytical functions or computationally inexpensive simulations, you can use the objective function directly: import numpy as np from PyEGRO.uncertainty.UQmcs import run_uncertainty_analysis # Define a non-linear function with multiple inputs def complex_function ( X ): \"\"\" Non-linear function with multiple inputs and complex behavior. \"\"\" x1 , x2 , x3 = X [:, 0 ], X [:, 1 ], X [:, 2 ] return x1 ** 2 * np . sin ( x2 ) + x3 * np . exp ( - 0.5 * ( x1 - x2 ) ** 2 ) # Define data_info with various uncertainty specifications data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 5 ], 'cov' : 0.1 , # Using CoV for positive range 'distribution' : 'normal' , 'description' : 'First design variable with CoV' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 2 * np . pi ], 'std' : 0.2 , # Using std for more direct control 'distribution' : 'normal' , 'description' : 'Second design variable with std' }, { 'name' : 'x3' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : 0 , # Using min/max notation 'max' : 10 , 'description' : 'Environmental variable with uniform distribution' } ] } # Save data_info import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/complex_data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run analysis using the convenience function results_df = run_uncertainty_analysis ( data_info_path = \"DATA_PREPARATION/complex_data_info.json\" , true_func = complex_function , num_design_samples = 500 , num_mcs_samples = 10000 , output_dir = \"RESULT_QOI_COMPLEX\" , show_progress = True ) Using a Metamodel \u00b6 For computationally expensive models, you can use a surrogate model: import numpy as np from PyEGRO.uncertainty.UQmcs import run_uncertainty_analysis from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Load a pre-trained surrogate model model_handler = DeviceAgnosticGPR ( prefer_gpu = True ) model_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run analysis with the surrogate model results_df = run_uncertainty_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , model_handler = model_handler , num_design_samples = 2000 , # Can use more samples since evaluations are cheap num_mcs_samples = 100000 , # Higher accuracy with more MC samples output_dir = \"RESULT_QOI_SURROGATE\" , show_progress = True ) 4. Example Applications \u00b6 1D Multi-modal Function with Standard Deviation \u00b6 Analyzing a function with multiple modes to study how uncertainty affects global behavior, using standard deviation for better handling of values crossing zero: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define a multi-modal function def multimodal_1d ( X ): x = X [:, 0 ] return np . sin ( 5 * x ) * np . exp ( - 0.5 * x ** 2 ) + 0.2 * np . sin ( 15 * x ) # Setup data configuration using std instead of cov since range crosses zero data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 3 , 3 ], 'std' : 0.1 , # Using standard deviation instead of cov for zero-crossing range 'distribution' : 'normal' , 'description' : 'Input variable with std-based uncertainty' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/multimodal_1d.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/multimodal_1d.json\" , true_func = multimodal_1d , output_dir = \"RESULT_MULTIMODAL_1D\" ) # Run general analysis results = uq . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 ) # Analyze specific points of interest interest_points = [ { 'x' : - 2.0 }, # Local minimum { 'x' : 0.0 }, # Global maximum { 'x' : 2.0 } # Local minimum ] for i , point in enumerate ( interest_points ): print ( f \"Analyzing point { i + 1 } : { point } \" ) result = uq . analyze_specific_point ( design_point = point , num_mcs_samples = 50000 , create_pdf = True , create_reliability = True ) print ( f \"Mean response: { result [ 'statistics' ][ 'mean' ] } \" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] } ]\" ) print () 1D Multi-modal Function (case 2) \u00b6 import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a multi-modal function with multiple local minima def multimodal_peaks ( X ): \"\"\" Multi-modal test function with multiple local minima. f(x) = -(100*exp(-(x-10)\u00b2/0.8) + 80*exp(-(x-20)\u00b2/50) + 20*exp(-(x-30)\u00b2/18) - 200) Has distinct peaks at x=10, x=20, and x=30 with different widths. \"\"\" x = X [:, 0 ] term1 = 100 * np . exp ( - (( x - 10 ) ** 2 ) / 0.8 ) term2 = 80 * np . exp ( - (( x - 20 ) ** 2 ) / 50 ) term3 = 20 * np . exp ( - (( x - 30 ) ** 2 ) / 18 ) return - ( term1 + term2 + term3 - 200 ) # Setup data configuration using cov (appropriate here since values are all positive) data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 40 ], 'cov' : 0.05 , # 5% coefficient of variation (works well for positive range) 'distribution' : 'normal' , 'description' : 'Design variable with CoV-based uncertainty' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/peaks_function.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/peaks_function.json\" , true_func = multimodal_peaks , output_dir = \"RESULT_PEAKS\" ) # Run general analysis with more samples for better resolution results = uq . run_analysis ( num_design_samples = 2000 , num_mcs_samples = 50000 ) # Analyze only the second peak at x=20 point = { 'x' : 20.0 } # Second peak (wide, medium sensitivity) print ( f \"Analyzing point: { point } \" ) result = uq . analyze_specific_point ( design_point = point , num_mcs_samples = 50000 , create_pdf = True , create_reliability = True ) print ( f \"Mean response: { result [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \"Standard deviation: { result [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \"Coefficient of variation: { result [ 'statistics' ][ 'cov' ] : .4f } \" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) # Add probability of failure analysis # Define a performance requirement (e.g., response must be better than 115) performance_threshold = 115.0 samples = result [ 'statistics' ][ 'samples' ] if 'samples' in result [ 'statistics' ] else None if samples is None : # If samples aren't available in the results, generate them again x_samples = np . random . normal ( 20.0 , 20.0 * 0.05 , 50000 ) # Using point value and CoV x_samples = x_samples . reshape ( - 1 , 1 ) samples = multimodal_peaks ( x_samples ) # Calculate probability of failure pf = np . mean ( samples < performance_threshold ) print ( f \" \\n Probability of Failure Analysis:\" ) print ( f \"Performance threshold: { performance_threshold : .4f } \" ) print ( f \"Probability that response < threshold: { pf : .6f } \" ) print ( f \"Reliability: { 1 - pf : .6f } \" ) 2D Multi-modal Function with Uniform Uncertainty \u00b6 Analyzing a 2D function with uniform uncertainty around design points using the delta parameter: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define a 2D multi-modal function (modified Himmelblau's function) def multimodal_2d ( X ): x1 , x2 = X [:, 0 ], X [:, 1 ] return - (( x1 ** 2 + x2 - 11 ) ** 2 + ( x1 + x2 ** 2 - 7 ) ** 2 ) + 200 # Setup data configuration with delta for uniform uncertainty data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], 'delta' : 0.2 , # Half-width of 0.2 around design points 'distribution' : 'uniform' , 'description' : 'First design variable with uniform uncertainty' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], 'delta' : 0.2 , # Half-width of 0.2 around design points 'distribution' : 'uniform' , 'description' : 'Second design variable with uniform uncertainty' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/multimodal_2d.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/multimodal_2d.json\" , true_func = multimodal_2d , output_dir = \"RESULT_MULTIMODAL_2D\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 400 , # Higher resolution for 2D visualization num_mcs_samples = 10000 ) # Analyze specific points (known optima of Himmelblau's function) optima = [ { 'x1' : 3.0 , 'x2' : 2.0 }, { 'x1' : - 2.805118 , 'x2' : 3.131312 }, { 'x1' : - 3.779310 , 'x2' : - 3.283186 }, { 'x1' : 3.584428 , 'x2' : - 1.848126 } ] for i , point in enumerate ( optima ): print ( f \"Analyzing optimum { i + 1 } : { point } \" ) result = uq . analyze_specific_point ( design_point = point , num_mcs_samples = 25000 , create_pdf = True , create_reliability = True ) print ( f \"Mean response: { result [ 'statistics' ][ 'mean' ] } \" ) print ( f \"Standard deviation: { result [ 'statistics' ][ 'std' ] } \" ) Ishigami function \u00b6 Sets up all three input variables (X1, X2, X3) as environmental variables with uniform distributions between -\u03c0 and \u03c0 import numpy as np import json import os from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define the Ishigami function def ishigami_function ( X ): \"\"\" Ishigami function: f(X1, X2, X3) = sin(X1) + a*(sin(X2))^2 + b*X3^4*sin(X1) Parameters: X[:, 0] = X1 X[:, 1] = X2 X[:, 2] = X3 \"\"\" x1 , x2 , x3 = X [:, 0 ], X [:, 1 ], X [:, 2 ] # Parameters for Ishigami function a = 7 b = 0.1 # Calculate Ishigami function return np . sin ( x1 ) + a * np . sin ( x2 ) ** 2 + b * x3 ** 4 * np . sin ( x1 ) # Define data configuration data_info = { 'variables' : [ { 'name' : 'X1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - np . pi , 'max' : np . pi , 'description' : 'Random variable 1 (uniform in [-\u03c0, \u03c0])' }, { 'name' : 'X2' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - np . pi , 'max' : np . pi , 'description' : 'Random variable 2 (uniform in [-\u03c0, \u03c0])' }, { 'name' : 'X3' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - np . pi , 'max' : np . pi , 'description' : 'Random variable 3 (uniform in [-\u03c0, \u03c0])' } ] } # Create directory and save configuration os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/ishigami_function.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/ishigami_function.json\" , true_func = ishigami_function , output_dir = \"RESULT_ISHIGAMI\" ) # Since we only have environmental variables, we need an empty dictionary # for the design_point parameter (no design variables to specify) empty_design = {} # Analyze with default environmental conditions and create visualizations result = uq_cov . analyze_specific_point ( design_point = empty_design , num_mcs_samples = 100000 , create_pdf = True , create_reliability = True ) Beam Analysis with Material Density Uncertainty \u00b6 This example analyzes a structural beam with uncertainty in the material properties: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Beam deflection model (simplified) def beam_model ( X ): \"\"\" Calculate maximum beam deflection with uncertain parameters X[:, 0] = Length (m) X[:, 1] = Width (m) X[:, 2] = Height (m) X[:, 3] = Load (N) X[:, 4] = Elastic modulus (Pa) X[:, 5] = Density (kg/m^3) \"\"\" L , b , h , P , E , rho = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ], X [:, 5 ] # Calculate moment of inertia I = b * h ** 3 / 12 # Maximum deflection for a simply supported beam with point load at center deflection = P * L ** 3 / ( 48 * E * I ) # Weight of the beam weight = L * b * h * rho # Return a weighted sum of deflection and weight (multi-objective) return deflection + 0.01 * weight # Setup data configuration data_info = { 'variables' : [ { 'name' : 'Length' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1.0 , 3.0 ], 'std' : 0.05 , # Fixed standard deviation of 5cm 'distribution' : 'normal' , 'description' : 'Beam length (m) with std-based tolerance' }, { 'name' : 'Width' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.05 , 0.2 ], 'delta' : 0.005 , # Uniform uncertainty of \u00b15mm 'distribution' : 'uniform' , 'description' : 'Beam width (m) with uniform tolerance' }, { 'name' : 'Height' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.1 , 0.5 ], 'cov' : 0.03 , # 3% manufacturing tolerance 'distribution' : 'normal' , 'description' : 'Beam height (m)' }, { 'name' : 'Load' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 1000 , # 1000 N 'std' : 200 , # 200 N standard deviation 'description' : 'Point load at center (N)' }, { 'name' : 'Elastic_Modulus' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 70e9 , # Aluminum (Pa) 'std' : 3.5e9 , # Standard deviation of 3.5 GPa 'description' : 'Elastic modulus (Pa) with std-based variability' }, { 'name' : 'Density' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2700 , # Aluminum (kg/m^3) 'cov' : 0.03 , # 3% variability (using CoV for demonstration) 'description' : 'Material density (kg/m^3) with CoV-based variability' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/beam_model.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/beam_model.json\" , true_func = beam_model , output_dir = \"RESULT_BEAM\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Analyze a specific design point nominal_design = { 'Length' : 2.0 , 'Width' : 0.1 , 'Height' : 0.3 } # Analyze the nominal design with default environmental conditions result = uq . analyze_specific_point ( design_point = nominal_design , num_mcs_samples = 100000 , create_pdf = True , create_reliability = True ) print ( \"Nominal design analysis:\" ) print ( f \"Mean deflection: { result [ 'statistics' ][ 'mean' ] : .6f } m\" ) print ( f \"Std deviation: { result [ 'statistics' ][ 'std' ] : .6f } m\" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .6f } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .6f } ] m\" ) # Analyze with modified environmental conditions (higher load variability) env_override = { 'Load' : { 'mean' : 1000 , 'std' : 400 } # Double the standard deviation } result_high_load = uq . analyze_specific_point ( design_point = nominal_design , env_vars_override = env_override , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) print ( \" \\n Design with higher load variability:\" ) print ( f \"Mean deflection: { result_high_load [ 'statistics' ][ 'mean' ] : .6f } m\" ) print ( f \"Std deviation: { result_high_load [ 'statistics' ][ 'std' ] : .6f } m\" ) print ( f \"95% CI: [ { result_high_load [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .6f } , { result_high_load [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .6f } ] m\" ) Borehole Function \u00b6 This example uses the borehole function, a common test function in the uncertainty quantification literature: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Borehole function def borehole ( X ): \"\"\" Borehole function with 8 inputs: X[:, 0] = rw (radius of borehole) X[:, 1] = r (radius of influence) X[:, 2] = Tu (transmissivity of upper aquifer) X[:, 3] = Hu (potentiometric head of upper aquifer) X[:, 4] = Tl (transmissivity of lower aquifer) X[:, 5] = Hl (potentiometric head of lower aquifer) X[:, 6] = L (length of borehole) X[:, 7] = Kw (hydraulic conductivity of borehole) \"\"\" rw , r , Tu , Hu , Tl , Hl , L , Kw = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ], X [:, 5 ], X [:, 6 ], X [:, 7 ] frac1 = 2 * np . pi * Tu * ( Hu - Hl ) frac2a = 2 * L * Tu / ( np . log ( r / rw ) * rw ** 2 * Kw ) frac2b = Tu / Tl frac2 = np . log ( r / rw ) * ( 1 + frac2a + frac2b ) return frac1 / frac2 # Setup data configuration data_info = { 'variables' : [ { 'name' : 'rw' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.05 , 0.15 ], 'std' : 0.005 , # Standard deviation of 5mm 'distribution' : 'normal' , 'description' : 'Radius of borehole (m) with std-based uncertainty' }, { 'name' : 'r' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : 100 , # Using min instead of low 'max' : 50000 , # Using max instead of high 'description' : 'Radius of influence (m) using min/max notation' }, { 'name' : 'Tu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63070 , 'high' : 115600 , 'description' : 'Transmissivity of upper aquifer (m^2/yr)' }, { 'name' : 'Hu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 990 , 'high' : 1110 , 'description' : 'Potentiometric head of upper aquifer (m)' }, { 'name' : 'Tl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63.1 , 'high' : 116 , 'description' : 'Transmissivity of lower aquifer (m^2/yr)' }, { 'name' : 'Hl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 700 , 'high' : 820 , 'description' : 'Potentiometric head of lower aquifer (m)' }, { 'name' : 'L' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1120 , 1680 ], 'cov' : 0.05 , 'distribution' : 'normal' , 'description' : 'Length of borehole (m)' }, { 'name' : 'Kw' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 9855 , 'high' : 12045 , 'description' : 'Hydraulic conductivity of borehole (m/yr)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/borehole.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/borehole.json\" , true_func = borehole , output_dir = \"RESULT_BOREHOLE\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 100 , num_mcs_samples = 5000 ) # Analyze specific design points designs_to_analyze = [ { 'rw' : 0.10 , 'L' : 1400 }, # Nominal design { 'rw' : 0.05 , 'L' : 1120 }, # Minimum dimensions { 'rw' : 0.15 , 'L' : 1680 } # Maximum dimensions ] for i , design in enumerate ( designs_to_analyze ): print ( f \" \\n Analyzing design { i + 1 } : { design } \" ) result = uq . analyze_specific_point ( design_point = design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) print ( f \"Mean flow rate: { result [ 'statistics' ][ 'mean' ] : .2f } m^3/yr\" ) print ( f \"Coefficient of variation: { result [ 'statistics' ][ 'cov' ] : .4f } \" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .2f } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .2f } ] m^3/yr\" ) Chemical Process Model \u00b6 This example analyzes a chemical reactor model with uncertainties in reaction parameters: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Simplified Chemical Reactor Model def reactor_model ( X ): \"\"\" Simple model of a chemical reactor with Arrhenius kinetics X[:, 0] = Temperature (K) X[:, 1] = Residence time (min) X[:, 2] = Initial concentration (mol/L) X[:, 3] = Pre-exponential factor (1/min) X[:, 4] = Activation energy (J/mol) \"\"\" T , t , C0 , A , Ea = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ] # Gas constant in J/(mol\u00b7K) R = 8.314 # Reaction rate constant (Arrhenius equation) k = A * np . exp ( - Ea / ( R * T )) # Conversion for first-order reaction conversion = 1 - np . exp ( - k * t ) # Yield calculation (with a penalty for high temperature due to side reactions) yield_factor = conversion * ( 1 - 0.1 * np . maximum ( 0 , ( T - 600 ) / 100 ) ** 2 ) # Productivity (throughput) productivity = yield_factor * C0 # Energy cost factor energy_cost = 0.01 * T * t # Overall objective (maximize productivity, minimize energy) return productivity - energy_cost # Setup data configuration data_info = { 'variables' : [ { 'name' : 'Temperature' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 500 , 700 ], # Kelvin 'std' : 10.0 , # Standard deviation of 10K 'distribution' : 'normal' , 'description' : 'Reactor temperature (K) with std-based control accuracy' }, { 'name' : 'ResidenceTime' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 5 , 30 ], # minutes 'delta' : 1.0 , # Uniform uncertainty of \u00b11 minute 'distribution' : 'uniform' , 'description' : 'Residence time (min) with uniform uncertainty' }, { 'name' : 'InitialConcentration' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.0 , # mol/L 'std' : 0.1 , # standard deviation 'description' : 'Initial reactant concentration (mol/L)' }, { 'name' : 'PreExponentialFactor' , 'vars_type' : 'env_vars' , 'distribution' : 'lognormal' , 'mean' : np . log ( 1e6 ), # log space 'std' : 0.2 , # log space 'description' : 'Reaction rate pre-exponential factor (1/min)' }, { 'name' : 'ActivationEnergy' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 80000 , # 80 kJ/mol 'std' : 4000 , # 4 kJ/mol 'description' : 'Reaction activation energy (J/mol)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/reactor.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/reactor.json\" , true_func = reactor_model , output_dir = \"RESULT_REACTOR\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Find the design point with highest mean response best_idx = results [ 'Mean' ] . idxmax () best_design = { 'Temperature' : results . loc [ best_idx , 'Temperature' ], 'ResidenceTime' : results . loc [ best_idx , 'ResidenceTime' ] } print ( f \"Best design found: { best_design } \" ) print ( f \"Mean performance: { results . loc [ best_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ best_idx , 'StdDev' ] : .4f } \" ) # Analyze this optimal point in more detail result_optimal = uq . analyze_specific_point ( design_point = best_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) # Analyze a robust design point (may not have highest mean, but lower variability) # Find design with highest ratio of mean to standard deviation robustness_metric = results [ 'Mean' ] / results [ 'StdDev' ] robust_idx = robustness_metric . idxmax () robust_design = { 'Temperature' : results . loc [ robust_idx , 'Temperature' ], 'ResidenceTime' : results . loc [ robust_idx , 'ResidenceTime' ] } print ( f \" \\n Most robust design: { robust_design } \" ) print ( f \"Mean performance: { results . loc [ robust_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ robust_idx , 'StdDev' ] : .4f } \" ) print ( f \"Coefficient of variation: { results . loc [ robust_idx , 'StdDev' ] / results . loc [ robust_idx , 'Mean' ] : .4f } \" ) # Analyze this robust point in more detail result_robust = uq . analyze_specific_point ( design_point = robust_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) Circuit Design Problem \u00b6 This example analyzes an electronic circuit with tolerance uncertainties in components: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Circuit performance model (voltage divider with passive components) def circuit_model ( X ): \"\"\" Simple circuit performance model for a low-pass RC filter circuit X[:, 0] = R1 (k\u03a9) - First resistor X[:, 1] = R2 (k\u03a9) - Second resistor X[:, 2] = C (\u03bcF) - Capacitor X[:, 3] = Vin (V) - Input voltage X[:, 4] = f (kHz) - Frequency X[:, 5] = T (\u00b0C) - Temperature \"\"\" R1 , R2 , C , Vin , f , T = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ], X [:, 5 ] # Convert units R1 = R1 * 1e3 # k\u03a9 to \u03a9 R2 = R2 * 1e3 # k\u03a9 to \u03a9 C = C * 1e-6 # \u03bcF to F f = f * 1e3 # kHz to Hz # Temperature effects on resistors (typical TCR for carbon resistors) TCR = 1000e-6 # ppm/\u00b0C R1_T = R1 * ( 1 + TCR * ( T - 25 )) R2_T = R2 * ( 1 + TCR * ( T - 25 )) # Angular frequency omega = 2 * np . pi * f # Calculate impedance of the capacitor Zc = 1 / ( 1 j * omega * C ) # Voltage divider with capacitor in parallel with R2 Z_parallel = ( R2_T * Zc ) / ( R2_T + Zc ) Vout = Vin * ( Z_parallel / ( R1_T + Z_parallel )) # Get magnitude and phase magnitude = np . abs ( Vout ) phase = np . angle ( Vout , deg = True ) # Create a combined performance metric (maximize magnitude at target frequency, # minimize deviation from target phase) target_phase = - 45 # desired phase shift performance = magnitude - 0.1 * np . abs ( phase - target_phase ) return performance # Setup data configuration data_info = { 'variables' : [ { 'name' : 'R1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1.0 , 10.0 ], # k\u03a9 'cov' : 0.05 , # 5% tolerance (using CoV since it's standard for resistors) 'distribution' : 'normal' , 'description' : 'First resistor (k\u03a9) with CoV-based tolerance' }, { 'name' : 'R2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1.0 , 10.0 ], # k\u03a9 'cov' : 0.05 , # 5% tolerance 'distribution' : 'normal' , 'description' : 'Second resistor (k\u03a9)' }, { 'name' : 'C' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.1 , 10.0 ], # \u03bcF 'cov' : 0.10 , # 10% tolerance (typical for capacitors) 'distribution' : 'normal' , 'description' : 'Capacitor (\u03bcF)' }, { 'name' : 'Vin' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 5.0 , # 5V 'std' : 0.25 , # 0.25V (5% of nominal) 'description' : 'Input voltage (V)' }, { 'name' : 'Frequency' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 0.5 , # 0.5 kHz 'high' : 5.0 , # 5 kHz 'description' : 'Operating frequency (kHz)' }, { 'name' : 'Temperature' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 25 , # 25\u00b0C (room temperature) 'std' : 15 , # \u00b115\u00b0C 'description' : 'Operating temperature (\u00b0C)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/circuit.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/circuit.json\" , true_func = circuit_model , output_dir = \"RESULT_CIRCUIT\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Find optimal design best_idx = results [ 'Mean' ] . idxmax () best_design = { 'R1' : results . loc [ best_idx , 'R1' ], 'R2' : results . loc [ best_idx , 'R2' ], 'C' : results . loc [ best_idx , 'C' ] } print ( f \"Best design found: { best_design } \" ) print ( f \"Mean performance: { results . loc [ best_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ best_idx , 'StdDev' ] : .4f } \" ) # Find robust design (high mean, low std dev) robustness_metric = results [ 'Mean' ] / results [ 'StdDev' ] robust_idx = robustness_metric . idxmax () robust_design = { 'R1' : results . loc [ robust_idx , 'R1' ], 'R2' : results . loc [ robust_idx , 'R2' ], 'C' : results . loc [ robust_idx , 'C' ] } print ( f \" \\n Most robust design: { robust_design } \" ) print ( f \"Mean performance: { results . loc [ robust_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ robust_idx , 'StdDev' ] : .4f } \" ) # Analyze both designs with detailed analysis result_optimal = uq . analyze_specific_point ( design_point = best_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) result_robust = uq . analyze_specific_point ( design_point = robust_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) 5. Advance Usage \u00b6 Comparison of Different Uncertainty Specifications \u00b6 This example compares all three methods of specifying uncertainty (CoV, Std, and Delta) on the same problem: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a simple exponential function def exp_function ( X ): return np . exp ( X [:, 0 ]) # Create three different configurations # 1. Using CoV data_info_cov = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 3 ], 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' , 'description' : 'Using CoV for uncertainty' } ] } # 2. Using Std data_info_std = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 3 ], 'std' : 0.2 , # Fixed standard deviation 'distribution' : 'normal' , 'description' : 'Using Std for uncertainty' } ] } # 3. Using Delta (uniform) data_info_delta = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 3 ], 'delta' : 0.2 , # Half-width for uniform distribution 'distribution' : 'uniform' , 'description' : 'Using Delta for uniform uncertainty' } ] } # Save configurations import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/exp_cov.json\" , 'w' ) as f : json . dump ( data_info_cov , f ) with open ( \"DATA_PREPARATION/exp_std.json\" , 'w' ) as f : json . dump ( data_info_std , f ) with open ( \"DATA_PREPARATION/exp_delta.json\" , 'w' ) as f : json . dump ( data_info_delta , f ) # Create UQ instances uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/exp_cov.json\" , true_func = exp_function , output_dir = \"RESULT_EXP_COV\" ) uq_std = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/exp_std.json\" , true_func = exp_function , output_dir = \"RESULT_EXP_STD\" ) uq_delta = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/exp_delta.json\" , true_func = exp_function , output_dir = \"RESULT_EXP_DELTA\" ) # Run analyses results_cov = uq_cov . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) results_std = uq_std . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) results_delta = uq_delta . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Create comparison plot plt . figure ( figsize = ( 15 , 10 )) # Plot means plt . subplot ( 2 , 1 , 1 ) plt . plot ( results_cov [ 'x' ], results_cov [ 'Mean' ], 'r-' , label = 'CoV (Normal)' ) plt . plot ( results_std [ 'x' ], results_std [ 'Mean' ], 'g-' , label = 'Std (Normal)' ) plt . plot ( results_delta [ 'x' ], results_delta [ 'Mean' ], 'b-' , label = 'Delta (Uniform)' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Mean' ) plt . title ( 'Comparison of Mean Response' ) plt . legend () plt . grid ( True ) # Plot standard deviations plt . subplot ( 2 , 1 , 2 ) plt . plot ( results_cov [ 'x' ], results_cov [ 'StdDev' ], 'r-' , label = 'CoV (Normal)' ) plt . plot ( results_std [ 'x' ], results_std [ 'StdDev' ], 'g-' , label = 'Std (Normal)' ) plt . plot ( results_delta [ 'x' ], results_delta [ 'StdDev' ], 'b-' , label = 'Delta (Uniform)' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Standard Deviation' ) plt . title ( 'Comparison of Uncertainty Propagation' ) plt . legend () plt . grid ( True ) plt . tight_layout () plt . savefig ( 'uncertainty_specification_comparison.png' ) plt . show () # Analysis at a specific point x_value = 2.0 design_point = { 'x' : x_value } result_cov = uq_cov . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) result_std = uq_std . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) result_delta = uq_delta . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) print ( f \" \\n Comparison at x = { x_value } :\" ) print ( f \"1. CoV method:\" ) print ( f \" Mean: { result_cov [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \" Std: { result_cov [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \" 95% CI: [ { result_cov [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_cov [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) print ( f \" \\n 2. Std method:\" ) print ( f \" Mean: { result_std [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \" Std: { result_std [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \" 95% CI: [ { result_std [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_std [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) print ( f \" \\n 3. Delta method (uniform):\" ) print ( f \" Mean: { result_delta [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \" Std: { result_delta [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \" 95% CI: [ { result_delta [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_delta [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) Using Environmental Variables with Different Specifications \u00b6 This example demonstrates how to use environmental variables with different uncertainty specifications: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Simple function to demonstrate environmental variable effects def env_sensitivity_function ( X ): \"\"\" Function showing sensitivity to different environmental variables X[:, 0] = Design variable X[:, 1] = Normal env variable with std X[:, 2] = Normal env variable with cov X[:, 3] = Uniform env variable with min/max X[:, 4] = Uniform env variable with low/high \"\"\" dv = X [:, 0 ] # Design variable env1 = X [:, 1 ] # Normal with std env2 = X [:, 2 ] # Normal with cov env3 = X [:, 3 ] # Uniform with min/max env4 = X [:, 4 ] # Uniform with low/high # Base response depends on design variable base = dv ** 2 # Environmental effects effect1 = 2 * env1 # Linear effect effect2 = env2 ** 2 # Quadratic effect effect3 = np . sin ( 3 * env3 ) # Oscillatory effect effect4 = np . exp ( 0.1 * env4 ) # Exponential effect return base + effect1 + effect2 + effect3 + effect4 # Setup data configuration with different ways to specify env variables data_info = { 'variables' : [ { 'name' : 'design_var' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 5 ], 'std' : 0.2 , 'distribution' : 'normal' , 'description' : 'Design variable with std' }, { 'name' : 'env_normal_std' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.5 , # Using std directly 'description' : 'Normal env variable with std' }, { 'name' : 'env_normal_cov' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.0 , 'cov' : 0.1 , # Using cov instead of std 'description' : 'Normal env variable with cov' }, { 'name' : 'env_uniform_minmax' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - 1.0 , # Using min/max notation 'max' : 1.0 , 'description' : 'Uniform env variable with min/max' }, { 'name' : 'env_uniform_lowhigh' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 0.0 , # Using low/high notation (for backward compatibility) 'high' : 2.0 , 'description' : 'Uniform env variable with low/high' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/env_variables_demo.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/env_variables_demo.json\" , true_func = env_sensitivity_function , output_dir = \"RESULT_ENV_VARS_DEMO\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 100 , num_mcs_samples = 5000 ) # Analyze a specific point with environmental variable overrides design_point = { 'design_var' : 2.5 } # Default environmental conditions result_default = uq . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) # Override with different specifications env_override = { 'env_normal_std' : { 'mean' : 1.0 , 'std' : 1.0 }, # Override using std 'env_normal_cov' : { 'mean' : 3.0 , 'cov' : 0.2 }, # Override using cov 'env_uniform_minmax' : { 'min' : - 2.0 , 'max' : 0.0 }, # Override using min/max 'env_uniform_lowhigh' : { 'low' : 1.0 , 'high' : 3.0 } # Override using low/high } result_override = uq . analyze_specific_point ( design_point = design_point , env_vars_override = env_override , num_mcs_samples = 10000 ) # Print comparison print ( f \"Analysis at design_var = { design_point [ 'design_var' ] } :\" ) print ( \" \\n Default Environmental Conditions:\" ) print ( f \"Mean: { result_default [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \"Std: { result_default [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \"95% CI: [ { result_default [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_default [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) print ( \" \\n Modified Environmental Conditions:\" ) print ( f \"Mean: { result_override [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \"Std: { result_override [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \"95% CI: [ { result_override [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_override [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) Environmental Variable Sensitivity Analysis \u00b6 This example shows how to perform a basic sensitivity analysis with different environmental variable configurations: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a function sensitive to environmental variables def env_sensitivity ( X ): \"\"\" Function with design and environmental variables X[:, 0] = Design variable (temperature) X[:, 1] = Environmental variable 1 (humidity) X[:, 2] = Environmental variable 2 (pressure) \"\"\" temp = X [:, 0 ] humidity = X [:, 1 ] pressure = X [:, 2 ] # Complex interaction between variables return temp ** 2 + 5 * humidity -. + 0.2 * pressure + 0.5 * temp * humidity - 0.1 * temp * pressure # Create data configuration data_info = { 'variables' : [ { 'name' : 'temperature' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 20 , 30 ], 'std' : 0.5 , # Standard deviation of 0.5\u00b0C 'distribution' : 'normal' , 'description' : 'Operating temperature (\u00b0C)' }, { 'name' : 'humidity' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 50 , 'std' : 10 , # Standard deviation of 10% 'description' : 'Relative humidity (%)' }, { 'name' : 'pressure' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 101.3 , 'std' : 2.0 , # Standard deviation of 2 kPa 'description' : 'Atmospheric pressure (kPa)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/sensitivity.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/sensitivity.json\" , true_func = env_sensitivity , output_dir = \"RESULT_SENSITIVITY\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 50 , num_mcs_samples = 5000 ) # Define different environmental scenarios nominal_point = { 'temperature' : 25.0 } scenarios = [ { \"name\" : \"Nominal\" , \"override\" : None }, { \"name\" : \"High Humidity\" , \"override\" : { 'humidity' : { 'mean' : 80 , 'std' : 5 }}}, { \"name\" : \"Low Humidity\" , \"override\" : { 'humidity' : { 'mean' : 20 , 'std' : 5 }}}, { \"name\" : \"High Pressure\" , \"override\" : { 'pressure' : { 'mean' : 110 , 'std' : 1 }}}, { \"name\" : \"Low Pressure\" , \"override\" : { 'pressure' : { 'mean' : 90 , 'std' : 1 }}}, { \"name\" : \"Extreme Conditions\" , \"override\" : { 'humidity' : { 'mean' : 85 , 'std' : 8 }, 'pressure' : { 'mean' : 88 , 'std' : 3 } }} ] # Analyze each scenario scenario_results = [] for scenario in scenarios : result = uq . analyze_specific_point ( design_point = nominal_point , env_vars_override = scenario [ \"override\" ], num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) scenario_results . append ({ \"name\" : scenario [ \"name\" ], \"mean\" : result [ 'statistics' ][ 'mean' ], \"std\" : result [ 'statistics' ][ 'std' ], \"cov\" : result [ 'statistics' ][ 'cov' ], \"ci_lower\" : result [ 'statistics' ][ 'percentiles' ][ '2.5' ], \"ci_upper\" : result [ 'statistics' ][ 'percentiles' ][ '97.5' ] }) # Create comparison plot plt . figure ( figsize = ( 12 , 8 )) # Plot means with error bars means = [ r [ \"mean\" ] for r in scenario_results ] stds = [ r [ \"std\" ] for r in scenario_results ] names = [ r [ \"name\" ] for r in scenario_results ] x_pos = np . arange ( len ( names )) plt . bar ( x_pos , means , yerr = stds , align = 'center' , alpha = 0.7 , capsize = 10 ) plt . xticks ( x_pos , names , rotation = 45 ) plt . ylabel ( 'Mean Response' ) plt . title ( 'Environmental Sensitivity Analysis' ) plt . grid ( True , axis = 'y' , linestyle = '--' , alpha = 0.7 ) # Add values on top of bars for i , ( mean , std ) in enumerate ( zip ( means , stds )): plt . text ( i , mean + std + 0.5 , f \" { mean : .1f } \u00b1 { std : .1f } \" , ha = 'center' ) plt . tight_layout () plt . savefig ( 'env_sensitivity_analysis.png' ) plt . show () # Print detailed results print ( \" \\n Detailed Scenario Results:\" ) print ( \"-\" * 80 ) print ( f \" { 'Scenario' : <20 } { 'Mean' : <8 } { 'StdDev' : <8 } { 'CoV' : <8 } { '95% CI' : <20 } \" ) print ( \"-\" * 80 ) for r in scenario_results : print ( f \" { r [ 'name' ] : <20 } { r [ 'mean' ] : <8.2f } { r [ 'std' ] : <8.2f } { r [ 'cov' ] : <8.2f } [ { r [ 'ci_lower' ] : <8.2f } , { r [ 'ci_upper' ] : <8.2f } ]\" ) Remarks \u00b6 The UQmcs module now supports multiple ways to specify uncertainty in both design and environmental variables, making it more flexible and robust for various engineering scenarios. The key improvements include: Support for both CoV and Std : Use cov when uncertainty scales with the variable's value Use std for direct control of uncertainty width or when dealing with variables that cross zero Uniform uncertainty with Delta parameter : The delta parameter provides a simple way to specify uniform uncertainty around design points Consistent notation for bounds : Environmental variables now support both low / high and min / max notation for better compatibility These features allow for more accurate uncertainty modeling across a wide range of problems, from mechanical design to chemical processes to electronic circuits. When choosing between uncertainty specification methods, consider: - Use std for variables whose range crosses zero - Use cov for positive variables where uncertainty scales with magnitude - Use delta with uniform distribution for manufacturing tolerances or when uncertainty has sharp bounds For more information, refer to the UQmcs API Reference document. ** Load .fig.pkl for ploting \u00b6 import pickle import matplotlib.pyplot as plt # Load the pickled figure with open ( 'uncertainty_analysis.fig.pkl' , 'rb' ) as file : fig = pickle . load ( file ) # Display the figure plt . show ()","title":"MCS Approach"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#pyegro-uncertainty-quantification-module-usage-guide","text":"(PyEGRO.uncertainty.UQmcs) This guide provides examples and usage patterns for PyEGRO's Uncertainty Quantification module using Monte Carlo Simulation (UQmcs). The module helps you analyze how uncertainty in input variables propagates to output responses, making it useful for robust design and risk assessment.","title":"PyEGRO Uncertainty Quantification Module Usage Guide"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#table-of-contents","text":"","title":"Table of Contents"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#quick-start-section","text":"Basic Setup Before running any analysis, you need to define your variables (design variables and environmental variables) in a configuration file. Here's how to set up a simple configuration: import numpy as np import json import os # Create a dictionary defining your variables data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 10 ], 'cov' : 0.05 , # 5% coefficient of variation 'distribution' : 'normal' , 'description' : 'First design variable' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], 'std' : 0.2 , # Using standard deviation instead of CoV 'distribution' : 'normal' , 'description' : 'Second design variable' }, { 'name' : 'noise' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.1 , 'description' : 'Measurement noise' } ] } # Save the configuration to a JSON file os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/data_info.json\" , 'w' ) as f : json . dump ( data_info , f )","title":"1. Quick Start"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#running-the-analysis","text":"Once your configuration is ready, you can run the uncertainty propagation analysis: from PyEGRO.uncertainty.UQmcs import run_uncertainty_analysis # Define your objective function def simple_function ( X ): return X [:, 0 ] ** 2 + X [:, 1 ] + 0.5 * X [:, 2 ] # Run the analysis results = run_uncertainty_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , true_func = simple_function , num_design_samples = 500 , # Number of design points to evaluate num_mcs_samples = 10000 , # Number of Monte Carlo samples per design point output_dir = \"RESULT_QOI\" , # Directory to save results show_progress = True ) # The results are also available as a pandas DataFrame print ( results . head ())","title":"Running the Analysis"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#uncertainty-specification-methods","text":"The UQmcs module supports multiple ways to specify uncertainty for design and environmental variables.","title":"2. Uncertainty Specification Methods"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#using-coefficient-of-variation-cov","text":"Coefficient of Variation (CoV) is the ratio of the standard deviation to the mean, expressed as a decimal or percentage. It's useful when the uncertainty scales with the magnitude of the variable. import numpy as np import json from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define a simple quadratic function def quadratic_func ( X ): return X [:, 0 ] ** 2 # Setup with CoV data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1 , 10 ], # Positive range (good for CoV) 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' , 'description' : 'Design variable with CoV uncertainty' } ] } # Save and run analysis with open ( \"DATA_PREPARATION/cov_example.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/cov_example.json\" , true_func = quadratic_func , output_dir = \"RESULT_COV\" ) results_cov = uq_cov . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 )","title":"Using Coefficient of Variation (CoV)"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#using-standard-deviation-std","text":"Standard deviation specifies the absolute width of the uncertainty distribution. This is preferable when the variable range crosses zero or when you want a constant uncertainty regardless of the mean value. # Setup with standard deviation data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], # Range crosses zero (better to use std) 'std' : 0.3 , # Fixed standard deviation of 0.3 'distribution' : 'normal' , 'description' : 'Design variable with std uncertainty' } ] } # Save and run analysis with open ( \"DATA_PREPARATION/std_example.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_std = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/std_example.json\" , true_func = quadratic_func , output_dir = \"RESULT_STD\" ) results_std = uq_std . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 )","title":"Using Standard Deviation (Std)"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#using-delta-for-uniform-uncertainty","text":"The delta parameter allows you to specify uniform uncertainty around design points, which is useful when you prefer a uniform distribution rather than normal. # Setup with delta for uniform uncertainty data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 10 ], 'delta' : 0.5 , # Half-width of 0.5 around design points 'distribution' : 'uniform' , 'description' : 'Design variable with uniform uncertainty' } ] } # Save and run analysis with open ( \"DATA_PREPARATION/delta_example.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_delta = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/delta_example.json\" , true_func = quadratic_func , output_dir = \"RESULT_DELTA\" ) results_delta = uq_delta . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 )","title":"Using Delta for Uniform Uncertainty"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#handling-near-zero-regions","text":"When variables cross near zero and you use CoV, issues can arise because CoV is undefined at zero. The updated code handles this automatically, but it's best to use std for such cases. import numpy as np import json from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a function that's sensitive to the sign of x def sine_func ( X ): return np . sin ( X [:, 0 ]) # Compare CoV and Std for a variable that crosses zero data_info_cov = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 3.14 , 3.14 ], # Crosses zero 'cov' : 0.1 , # CoV will be handled specially near zero 'distribution' : 'normal' , 'description' : 'Design variable with CoV (crossing zero)' } ] } data_info_std = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 3.14 , 3.14 ], # Crosses zero 'std' : 0.2 , # Better approach for variables crossing zero 'distribution' : 'normal' , 'description' : 'Design variable with std (crossing zero)' } ] } # Save configurations with open ( \"DATA_PREPARATION/near_zero_cov.json\" , 'w' ) as f : json . dump ( data_info_cov , f ) with open ( \"DATA_PREPARATION/near_zero_std.json\" , 'w' ) as f : json . dump ( data_info_std , f ) # Run analyses uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/near_zero_cov.json\" , true_func = sine_func , output_dir = \"RESULT_NEAR_ZERO_COV\" ) uq_std = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/near_zero_std.json\" , true_func = sine_func , output_dir = \"RESULT_NEAR_ZERO_STD\" ) results_cov = uq_cov . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) results_std = uq_std . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Plot to compare plt . figure ( figsize = ( 12 , 5 )) plt . subplot ( 1 , 2 , 1 ) plt . scatter ( results_cov [ 'x' ], results_cov [ 'Mean' ], c = results_cov [ 'StdDev' ], cmap = 'viridis' ) plt . colorbar ( label = 'Standard Deviation' ) plt . title ( 'Using CoV for Variable Crossing Zero' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Mean Response' ) plt . subplot ( 1 , 2 , 2 ) plt . scatter ( results_std [ 'x' ], results_std [ 'Mean' ], c = results_std [ 'StdDev' ], cmap = 'viridis' ) plt . colorbar ( label = 'Standard Deviation' ) plt . title ( 'Using Std for Variable Crossing Zero' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Mean Response' ) plt . tight_layout () plt . savefig ( \"near_zero_comparison.png\" ) plt . show ()","title":"Handling Near-Zero Regions"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#working-with-different-models","text":"","title":"3. Working with Different Models"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#using-a-direct-function","text":"For analytical functions or computationally inexpensive simulations, you can use the objective function directly: import numpy as np from PyEGRO.uncertainty.UQmcs import run_uncertainty_analysis # Define a non-linear function with multiple inputs def complex_function ( X ): \"\"\" Non-linear function with multiple inputs and complex behavior. \"\"\" x1 , x2 , x3 = X [:, 0 ], X [:, 1 ], X [:, 2 ] return x1 ** 2 * np . sin ( x2 ) + x3 * np . exp ( - 0.5 * ( x1 - x2 ) ** 2 ) # Define data_info with various uncertainty specifications data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 5 ], 'cov' : 0.1 , # Using CoV for positive range 'distribution' : 'normal' , 'description' : 'First design variable with CoV' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 2 * np . pi ], 'std' : 0.2 , # Using std for more direct control 'distribution' : 'normal' , 'description' : 'Second design variable with std' }, { 'name' : 'x3' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : 0 , # Using min/max notation 'max' : 10 , 'description' : 'Environmental variable with uniform distribution' } ] } # Save data_info import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/complex_data_info.json\" , 'w' ) as f : json . dump ( data_info , f ) # Run analysis using the convenience function results_df = run_uncertainty_analysis ( data_info_path = \"DATA_PREPARATION/complex_data_info.json\" , true_func = complex_function , num_design_samples = 500 , num_mcs_samples = 10000 , output_dir = \"RESULT_QOI_COMPLEX\" , show_progress = True )","title":"Using a Direct Function"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#using-a-metamodel","text":"For computationally expensive models, you can use a surrogate model: import numpy as np from PyEGRO.uncertainty.UQmcs import run_uncertainty_analysis from PyEGRO.meta.gpr.gpr_utils import DeviceAgnosticGPR # Load a pre-trained surrogate model model_handler = DeviceAgnosticGPR ( prefer_gpu = True ) model_handler . load_model ( 'RESULT_MODEL_GPR' ) # Run analysis with the surrogate model results_df = run_uncertainty_analysis ( data_info_path = \"DATA_PREPARATION/data_info.json\" , model_handler = model_handler , num_design_samples = 2000 , # Can use more samples since evaluations are cheap num_mcs_samples = 100000 , # Higher accuracy with more MC samples output_dir = \"RESULT_QOI_SURROGATE\" , show_progress = True )","title":"Using a Metamodel"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#example-applications","text":"","title":"4. Example Applications"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#1d-multi-modal-function-with-standard-deviation","text":"Analyzing a function with multiple modes to study how uncertainty affects global behavior, using standard deviation for better handling of values crossing zero: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define a multi-modal function def multimodal_1d ( X ): x = X [:, 0 ] return np . sin ( 5 * x ) * np . exp ( - 0.5 * x ** 2 ) + 0.2 * np . sin ( 15 * x ) # Setup data configuration using std instead of cov since range crosses zero data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 3 , 3 ], 'std' : 0.1 , # Using standard deviation instead of cov for zero-crossing range 'distribution' : 'normal' , 'description' : 'Input variable with std-based uncertainty' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/multimodal_1d.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/multimodal_1d.json\" , true_func = multimodal_1d , output_dir = \"RESULT_MULTIMODAL_1D\" ) # Run general analysis results = uq . run_analysis ( num_design_samples = 100 , num_mcs_samples = 10000 ) # Analyze specific points of interest interest_points = [ { 'x' : - 2.0 }, # Local minimum { 'x' : 0.0 }, # Global maximum { 'x' : 2.0 } # Local minimum ] for i , point in enumerate ( interest_points ): print ( f \"Analyzing point { i + 1 } : { point } \" ) result = uq . analyze_specific_point ( design_point = point , num_mcs_samples = 50000 , create_pdf = True , create_reliability = True ) print ( f \"Mean response: { result [ 'statistics' ][ 'mean' ] } \" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] } ]\" ) print ()","title":"1D Multi-modal Function with Standard Deviation"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#1d-multi-modal-function-case-2","text":"import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a multi-modal function with multiple local minima def multimodal_peaks ( X ): \"\"\" Multi-modal test function with multiple local minima. f(x) = -(100*exp(-(x-10)\u00b2/0.8) + 80*exp(-(x-20)\u00b2/50) + 20*exp(-(x-30)\u00b2/18) - 200) Has distinct peaks at x=10, x=20, and x=30 with different widths. \"\"\" x = X [:, 0 ] term1 = 100 * np . exp ( - (( x - 10 ) ** 2 ) / 0.8 ) term2 = 80 * np . exp ( - (( x - 20 ) ** 2 ) / 50 ) term3 = 20 * np . exp ( - (( x - 30 ) ** 2 ) / 18 ) return - ( term1 + term2 + term3 - 200 ) # Setup data configuration using cov (appropriate here since values are all positive) data_info = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 40 ], 'cov' : 0.05 , # 5% coefficient of variation (works well for positive range) 'distribution' : 'normal' , 'description' : 'Design variable with CoV-based uncertainty' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/peaks_function.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/peaks_function.json\" , true_func = multimodal_peaks , output_dir = \"RESULT_PEAKS\" ) # Run general analysis with more samples for better resolution results = uq . run_analysis ( num_design_samples = 2000 , num_mcs_samples = 50000 ) # Analyze only the second peak at x=20 point = { 'x' : 20.0 } # Second peak (wide, medium sensitivity) print ( f \"Analyzing point: { point } \" ) result = uq . analyze_specific_point ( design_point = point , num_mcs_samples = 50000 , create_pdf = True , create_reliability = True ) print ( f \"Mean response: { result [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \"Standard deviation: { result [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \"Coefficient of variation: { result [ 'statistics' ][ 'cov' ] : .4f } \" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) # Add probability of failure analysis # Define a performance requirement (e.g., response must be better than 115) performance_threshold = 115.0 samples = result [ 'statistics' ][ 'samples' ] if 'samples' in result [ 'statistics' ] else None if samples is None : # If samples aren't available in the results, generate them again x_samples = np . random . normal ( 20.0 , 20.0 * 0.05 , 50000 ) # Using point value and CoV x_samples = x_samples . reshape ( - 1 , 1 ) samples = multimodal_peaks ( x_samples ) # Calculate probability of failure pf = np . mean ( samples < performance_threshold ) print ( f \" \\n Probability of Failure Analysis:\" ) print ( f \"Performance threshold: { performance_threshold : .4f } \" ) print ( f \"Probability that response < threshold: { pf : .6f } \" ) print ( f \"Reliability: { 1 - pf : .6f } \" )","title":"1D Multi-modal Function (case 2)"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#2d-multi-modal-function-with-uniform-uncertainty","text":"Analyzing a 2D function with uniform uncertainty around design points using the delta parameter: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define a 2D multi-modal function (modified Himmelblau's function) def multimodal_2d ( X ): x1 , x2 = X [:, 0 ], X [:, 1 ] return - (( x1 ** 2 + x2 - 11 ) ** 2 + ( x1 + x2 ** 2 - 7 ) ** 2 ) + 200 # Setup data configuration with delta for uniform uncertainty data_info = { 'variables' : [ { 'name' : 'x1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], 'delta' : 0.2 , # Half-width of 0.2 around design points 'distribution' : 'uniform' , 'description' : 'First design variable with uniform uncertainty' }, { 'name' : 'x2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ - 5 , 5 ], 'delta' : 0.2 , # Half-width of 0.2 around design points 'distribution' : 'uniform' , 'description' : 'Second design variable with uniform uncertainty' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/multimodal_2d.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/multimodal_2d.json\" , true_func = multimodal_2d , output_dir = \"RESULT_MULTIMODAL_2D\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 400 , # Higher resolution for 2D visualization num_mcs_samples = 10000 ) # Analyze specific points (known optima of Himmelblau's function) optima = [ { 'x1' : 3.0 , 'x2' : 2.0 }, { 'x1' : - 2.805118 , 'x2' : 3.131312 }, { 'x1' : - 3.779310 , 'x2' : - 3.283186 }, { 'x1' : 3.584428 , 'x2' : - 1.848126 } ] for i , point in enumerate ( optima ): print ( f \"Analyzing optimum { i + 1 } : { point } \" ) result = uq . analyze_specific_point ( design_point = point , num_mcs_samples = 25000 , create_pdf = True , create_reliability = True ) print ( f \"Mean response: { result [ 'statistics' ][ 'mean' ] } \" ) print ( f \"Standard deviation: { result [ 'statistics' ][ 'std' ] } \" )","title":"2D Multi-modal Function with Uniform Uncertainty"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#ishigami-function","text":"Sets up all three input variables (X1, X2, X3) as environmental variables with uniform distributions between -\u03c0 and \u03c0 import numpy as np import json import os from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Define the Ishigami function def ishigami_function ( X ): \"\"\" Ishigami function: f(X1, X2, X3) = sin(X1) + a*(sin(X2))^2 + b*X3^4*sin(X1) Parameters: X[:, 0] = X1 X[:, 1] = X2 X[:, 2] = X3 \"\"\" x1 , x2 , x3 = X [:, 0 ], X [:, 1 ], X [:, 2 ] # Parameters for Ishigami function a = 7 b = 0.1 # Calculate Ishigami function return np . sin ( x1 ) + a * np . sin ( x2 ) ** 2 + b * x3 ** 4 * np . sin ( x1 ) # Define data configuration data_info = { 'variables' : [ { 'name' : 'X1' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - np . pi , 'max' : np . pi , 'description' : 'Random variable 1 (uniform in [-\u03c0, \u03c0])' }, { 'name' : 'X2' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - np . pi , 'max' : np . pi , 'description' : 'Random variable 2 (uniform in [-\u03c0, \u03c0])' }, { 'name' : 'X3' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - np . pi , 'max' : np . pi , 'description' : 'Random variable 3 (uniform in [-\u03c0, \u03c0])' } ] } # Create directory and save configuration os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/ishigami_function.json\" , 'w' ) as f : json . dump ( data_info , f ) uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/ishigami_function.json\" , true_func = ishigami_function , output_dir = \"RESULT_ISHIGAMI\" ) # Since we only have environmental variables, we need an empty dictionary # for the design_point parameter (no design variables to specify) empty_design = {} # Analyze with default environmental conditions and create visualizations result = uq_cov . analyze_specific_point ( design_point = empty_design , num_mcs_samples = 100000 , create_pdf = True , create_reliability = True )","title":"Ishigami function"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#beam-analysis-with-material-density-uncertainty","text":"This example analyzes a structural beam with uncertainty in the material properties: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Beam deflection model (simplified) def beam_model ( X ): \"\"\" Calculate maximum beam deflection with uncertain parameters X[:, 0] = Length (m) X[:, 1] = Width (m) X[:, 2] = Height (m) X[:, 3] = Load (N) X[:, 4] = Elastic modulus (Pa) X[:, 5] = Density (kg/m^3) \"\"\" L , b , h , P , E , rho = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ], X [:, 5 ] # Calculate moment of inertia I = b * h ** 3 / 12 # Maximum deflection for a simply supported beam with point load at center deflection = P * L ** 3 / ( 48 * E * I ) # Weight of the beam weight = L * b * h * rho # Return a weighted sum of deflection and weight (multi-objective) return deflection + 0.01 * weight # Setup data configuration data_info = { 'variables' : [ { 'name' : 'Length' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1.0 , 3.0 ], 'std' : 0.05 , # Fixed standard deviation of 5cm 'distribution' : 'normal' , 'description' : 'Beam length (m) with std-based tolerance' }, { 'name' : 'Width' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.05 , 0.2 ], 'delta' : 0.005 , # Uniform uncertainty of \u00b15mm 'distribution' : 'uniform' , 'description' : 'Beam width (m) with uniform tolerance' }, { 'name' : 'Height' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.1 , 0.5 ], 'cov' : 0.03 , # 3% manufacturing tolerance 'distribution' : 'normal' , 'description' : 'Beam height (m)' }, { 'name' : 'Load' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 1000 , # 1000 N 'std' : 200 , # 200 N standard deviation 'description' : 'Point load at center (N)' }, { 'name' : 'Elastic_Modulus' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 70e9 , # Aluminum (Pa) 'std' : 3.5e9 , # Standard deviation of 3.5 GPa 'description' : 'Elastic modulus (Pa) with std-based variability' }, { 'name' : 'Density' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2700 , # Aluminum (kg/m^3) 'cov' : 0.03 , # 3% variability (using CoV for demonstration) 'description' : 'Material density (kg/m^3) with CoV-based variability' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/beam_model.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/beam_model.json\" , true_func = beam_model , output_dir = \"RESULT_BEAM\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Analyze a specific design point nominal_design = { 'Length' : 2.0 , 'Width' : 0.1 , 'Height' : 0.3 } # Analyze the nominal design with default environmental conditions result = uq . analyze_specific_point ( design_point = nominal_design , num_mcs_samples = 100000 , create_pdf = True , create_reliability = True ) print ( \"Nominal design analysis:\" ) print ( f \"Mean deflection: { result [ 'statistics' ][ 'mean' ] : .6f } m\" ) print ( f \"Std deviation: { result [ 'statistics' ][ 'std' ] : .6f } m\" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .6f } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .6f } ] m\" ) # Analyze with modified environmental conditions (higher load variability) env_override = { 'Load' : { 'mean' : 1000 , 'std' : 400 } # Double the standard deviation } result_high_load = uq . analyze_specific_point ( design_point = nominal_design , env_vars_override = env_override , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) print ( \" \\n Design with higher load variability:\" ) print ( f \"Mean deflection: { result_high_load [ 'statistics' ][ 'mean' ] : .6f } m\" ) print ( f \"Std deviation: { result_high_load [ 'statistics' ][ 'std' ] : .6f } m\" ) print ( f \"95% CI: [ { result_high_load [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .6f } , { result_high_load [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .6f } ] m\" )","title":"Beam Analysis with Material Density Uncertainty"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#borehole-function","text":"This example uses the borehole function, a common test function in the uncertainty quantification literature: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Borehole function def borehole ( X ): \"\"\" Borehole function with 8 inputs: X[:, 0] = rw (radius of borehole) X[:, 1] = r (radius of influence) X[:, 2] = Tu (transmissivity of upper aquifer) X[:, 3] = Hu (potentiometric head of upper aquifer) X[:, 4] = Tl (transmissivity of lower aquifer) X[:, 5] = Hl (potentiometric head of lower aquifer) X[:, 6] = L (length of borehole) X[:, 7] = Kw (hydraulic conductivity of borehole) \"\"\" rw , r , Tu , Hu , Tl , Hl , L , Kw = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ], X [:, 5 ], X [:, 6 ], X [:, 7 ] frac1 = 2 * np . pi * Tu * ( Hu - Hl ) frac2a = 2 * L * Tu / ( np . log ( r / rw ) * rw ** 2 * Kw ) frac2b = Tu / Tl frac2 = np . log ( r / rw ) * ( 1 + frac2a + frac2b ) return frac1 / frac2 # Setup data configuration data_info = { 'variables' : [ { 'name' : 'rw' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.05 , 0.15 ], 'std' : 0.005 , # Standard deviation of 5mm 'distribution' : 'normal' , 'description' : 'Radius of borehole (m) with std-based uncertainty' }, { 'name' : 'r' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : 100 , # Using min instead of low 'max' : 50000 , # Using max instead of high 'description' : 'Radius of influence (m) using min/max notation' }, { 'name' : 'Tu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63070 , 'high' : 115600 , 'description' : 'Transmissivity of upper aquifer (m^2/yr)' }, { 'name' : 'Hu' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 990 , 'high' : 1110 , 'description' : 'Potentiometric head of upper aquifer (m)' }, { 'name' : 'Tl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 63.1 , 'high' : 116 , 'description' : 'Transmissivity of lower aquifer (m^2/yr)' }, { 'name' : 'Hl' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 700 , 'high' : 820 , 'description' : 'Potentiometric head of lower aquifer (m)' }, { 'name' : 'L' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1120 , 1680 ], 'cov' : 0.05 , 'distribution' : 'normal' , 'description' : 'Length of borehole (m)' }, { 'name' : 'Kw' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 9855 , 'high' : 12045 , 'description' : 'Hydraulic conductivity of borehole (m/yr)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/borehole.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/borehole.json\" , true_func = borehole , output_dir = \"RESULT_BOREHOLE\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 100 , num_mcs_samples = 5000 ) # Analyze specific design points designs_to_analyze = [ { 'rw' : 0.10 , 'L' : 1400 }, # Nominal design { 'rw' : 0.05 , 'L' : 1120 }, # Minimum dimensions { 'rw' : 0.15 , 'L' : 1680 } # Maximum dimensions ] for i , design in enumerate ( designs_to_analyze ): print ( f \" \\n Analyzing design { i + 1 } : { design } \" ) result = uq . analyze_specific_point ( design_point = design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) print ( f \"Mean flow rate: { result [ 'statistics' ][ 'mean' ] : .2f } m^3/yr\" ) print ( f \"Coefficient of variation: { result [ 'statistics' ][ 'cov' ] : .4f } \" ) print ( f \"95% CI: [ { result [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .2f } , { result [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .2f } ] m^3/yr\" )","title":"Borehole Function"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#chemical-process-model","text":"This example analyzes a chemical reactor model with uncertainties in reaction parameters: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Simplified Chemical Reactor Model def reactor_model ( X ): \"\"\" Simple model of a chemical reactor with Arrhenius kinetics X[:, 0] = Temperature (K) X[:, 1] = Residence time (min) X[:, 2] = Initial concentration (mol/L) X[:, 3] = Pre-exponential factor (1/min) X[:, 4] = Activation energy (J/mol) \"\"\" T , t , C0 , A , Ea = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ] # Gas constant in J/(mol\u00b7K) R = 8.314 # Reaction rate constant (Arrhenius equation) k = A * np . exp ( - Ea / ( R * T )) # Conversion for first-order reaction conversion = 1 - np . exp ( - k * t ) # Yield calculation (with a penalty for high temperature due to side reactions) yield_factor = conversion * ( 1 - 0.1 * np . maximum ( 0 , ( T - 600 ) / 100 ) ** 2 ) # Productivity (throughput) productivity = yield_factor * C0 # Energy cost factor energy_cost = 0.01 * T * t # Overall objective (maximize productivity, minimize energy) return productivity - energy_cost # Setup data configuration data_info = { 'variables' : [ { 'name' : 'Temperature' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 500 , 700 ], # Kelvin 'std' : 10.0 , # Standard deviation of 10K 'distribution' : 'normal' , 'description' : 'Reactor temperature (K) with std-based control accuracy' }, { 'name' : 'ResidenceTime' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 5 , 30 ], # minutes 'delta' : 1.0 , # Uniform uncertainty of \u00b11 minute 'distribution' : 'uniform' , 'description' : 'Residence time (min) with uniform uncertainty' }, { 'name' : 'InitialConcentration' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.0 , # mol/L 'std' : 0.1 , # standard deviation 'description' : 'Initial reactant concentration (mol/L)' }, { 'name' : 'PreExponentialFactor' , 'vars_type' : 'env_vars' , 'distribution' : 'lognormal' , 'mean' : np . log ( 1e6 ), # log space 'std' : 0.2 , # log space 'description' : 'Reaction rate pre-exponential factor (1/min)' }, { 'name' : 'ActivationEnergy' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 80000 , # 80 kJ/mol 'std' : 4000 , # 4 kJ/mol 'description' : 'Reaction activation energy (J/mol)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/reactor.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/reactor.json\" , true_func = reactor_model , output_dir = \"RESULT_REACTOR\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Find the design point with highest mean response best_idx = results [ 'Mean' ] . idxmax () best_design = { 'Temperature' : results . loc [ best_idx , 'Temperature' ], 'ResidenceTime' : results . loc [ best_idx , 'ResidenceTime' ] } print ( f \"Best design found: { best_design } \" ) print ( f \"Mean performance: { results . loc [ best_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ best_idx , 'StdDev' ] : .4f } \" ) # Analyze this optimal point in more detail result_optimal = uq . analyze_specific_point ( design_point = best_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) # Analyze a robust design point (may not have highest mean, but lower variability) # Find design with highest ratio of mean to standard deviation robustness_metric = results [ 'Mean' ] / results [ 'StdDev' ] robust_idx = robustness_metric . idxmax () robust_design = { 'Temperature' : results . loc [ robust_idx , 'Temperature' ], 'ResidenceTime' : results . loc [ robust_idx , 'ResidenceTime' ] } print ( f \" \\n Most robust design: { robust_design } \" ) print ( f \"Mean performance: { results . loc [ robust_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ robust_idx , 'StdDev' ] : .4f } \" ) print ( f \"Coefficient of variation: { results . loc [ robust_idx , 'StdDev' ] / results . loc [ robust_idx , 'Mean' ] : .4f } \" ) # Analyze this robust point in more detail result_robust = uq . analyze_specific_point ( design_point = robust_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True )","title":"Chemical Process Model"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#circuit-design-problem","text":"This example analyzes an electronic circuit with tolerance uncertainties in components: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Circuit performance model (voltage divider with passive components) def circuit_model ( X ): \"\"\" Simple circuit performance model for a low-pass RC filter circuit X[:, 0] = R1 (k\u03a9) - First resistor X[:, 1] = R2 (k\u03a9) - Second resistor X[:, 2] = C (\u03bcF) - Capacitor X[:, 3] = Vin (V) - Input voltage X[:, 4] = f (kHz) - Frequency X[:, 5] = T (\u00b0C) - Temperature \"\"\" R1 , R2 , C , Vin , f , T = X [:, 0 ], X [:, 1 ], X [:, 2 ], X [:, 3 ], X [:, 4 ], X [:, 5 ] # Convert units R1 = R1 * 1e3 # k\u03a9 to \u03a9 R2 = R2 * 1e3 # k\u03a9 to \u03a9 C = C * 1e-6 # \u03bcF to F f = f * 1e3 # kHz to Hz # Temperature effects on resistors (typical TCR for carbon resistors) TCR = 1000e-6 # ppm/\u00b0C R1_T = R1 * ( 1 + TCR * ( T - 25 )) R2_T = R2 * ( 1 + TCR * ( T - 25 )) # Angular frequency omega = 2 * np . pi * f # Calculate impedance of the capacitor Zc = 1 / ( 1 j * omega * C ) # Voltage divider with capacitor in parallel with R2 Z_parallel = ( R2_T * Zc ) / ( R2_T + Zc ) Vout = Vin * ( Z_parallel / ( R1_T + Z_parallel )) # Get magnitude and phase magnitude = np . abs ( Vout ) phase = np . angle ( Vout , deg = True ) # Create a combined performance metric (maximize magnitude at target frequency, # minimize deviation from target phase) target_phase = - 45 # desired phase shift performance = magnitude - 0.1 * np . abs ( phase - target_phase ) return performance # Setup data configuration data_info = { 'variables' : [ { 'name' : 'R1' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1.0 , 10.0 ], # k\u03a9 'cov' : 0.05 , # 5% tolerance (using CoV since it's standard for resistors) 'distribution' : 'normal' , 'description' : 'First resistor (k\u03a9) with CoV-based tolerance' }, { 'name' : 'R2' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 1.0 , 10.0 ], # k\u03a9 'cov' : 0.05 , # 5% tolerance 'distribution' : 'normal' , 'description' : 'Second resistor (k\u03a9)' }, { 'name' : 'C' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0.1 , 10.0 ], # \u03bcF 'cov' : 0.10 , # 10% tolerance (typical for capacitors) 'distribution' : 'normal' , 'description' : 'Capacitor (\u03bcF)' }, { 'name' : 'Vin' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 5.0 , # 5V 'std' : 0.25 , # 0.25V (5% of nominal) 'description' : 'Input voltage (V)' }, { 'name' : 'Frequency' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 0.5 , # 0.5 kHz 'high' : 5.0 , # 5 kHz 'description' : 'Operating frequency (kHz)' }, { 'name' : 'Temperature' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 25 , # 25\u00b0C (room temperature) 'std' : 15 , # \u00b115\u00b0C 'description' : 'Operating temperature (\u00b0C)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/circuit.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance and run analysis uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/circuit.json\" , true_func = circuit_model , output_dir = \"RESULT_CIRCUIT\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Find optimal design best_idx = results [ 'Mean' ] . idxmax () best_design = { 'R1' : results . loc [ best_idx , 'R1' ], 'R2' : results . loc [ best_idx , 'R2' ], 'C' : results . loc [ best_idx , 'C' ] } print ( f \"Best design found: { best_design } \" ) print ( f \"Mean performance: { results . loc [ best_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ best_idx , 'StdDev' ] : .4f } \" ) # Find robust design (high mean, low std dev) robustness_metric = results [ 'Mean' ] / results [ 'StdDev' ] robust_idx = robustness_metric . idxmax () robust_design = { 'R1' : results . loc [ robust_idx , 'R1' ], 'R2' : results . loc [ robust_idx , 'R2' ], 'C' : results . loc [ robust_idx , 'C' ] } print ( f \" \\n Most robust design: { robust_design } \" ) print ( f \"Mean performance: { results . loc [ robust_idx , 'Mean' ] : .4f } \" ) print ( f \"Standard deviation: { results . loc [ robust_idx , 'StdDev' ] : .4f } \" ) # Analyze both designs with detailed analysis result_optimal = uq . analyze_specific_point ( design_point = best_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) result_robust = uq . analyze_specific_point ( design_point = robust_design , num_mcs_samples = 10000 , create_pdf = True , create_reliability = True )","title":"Circuit Design Problem"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#advance-usage","text":"","title":"5. Advance Usage"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#comparison-of-different-uncertainty-specifications","text":"This example compares all three methods of specifying uncertainty (CoV, Std, and Delta) on the same problem: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a simple exponential function def exp_function ( X ): return np . exp ( X [:, 0 ]) # Create three different configurations # 1. Using CoV data_info_cov = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 3 ], 'cov' : 0.1 , # 10% coefficient of variation 'distribution' : 'normal' , 'description' : 'Using CoV for uncertainty' } ] } # 2. Using Std data_info_std = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 3 ], 'std' : 0.2 , # Fixed standard deviation 'distribution' : 'normal' , 'description' : 'Using Std for uncertainty' } ] } # 3. Using Delta (uniform) data_info_delta = { 'variables' : [ { 'name' : 'x' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 3 ], 'delta' : 0.2 , # Half-width for uniform distribution 'distribution' : 'uniform' , 'description' : 'Using Delta for uniform uncertainty' } ] } # Save configurations import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/exp_cov.json\" , 'w' ) as f : json . dump ( data_info_cov , f ) with open ( \"DATA_PREPARATION/exp_std.json\" , 'w' ) as f : json . dump ( data_info_std , f ) with open ( \"DATA_PREPARATION/exp_delta.json\" , 'w' ) as f : json . dump ( data_info_delta , f ) # Create UQ instances uq_cov = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/exp_cov.json\" , true_func = exp_function , output_dir = \"RESULT_EXP_COV\" ) uq_std = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/exp_std.json\" , true_func = exp_function , output_dir = \"RESULT_EXP_STD\" ) uq_delta = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/exp_delta.json\" , true_func = exp_function , output_dir = \"RESULT_EXP_DELTA\" ) # Run analyses results_cov = uq_cov . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) results_std = uq_std . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) results_delta = uq_delta . run_analysis ( num_design_samples = 200 , num_mcs_samples = 5000 ) # Create comparison plot plt . figure ( figsize = ( 15 , 10 )) # Plot means plt . subplot ( 2 , 1 , 1 ) plt . plot ( results_cov [ 'x' ], results_cov [ 'Mean' ], 'r-' , label = 'CoV (Normal)' ) plt . plot ( results_std [ 'x' ], results_std [ 'Mean' ], 'g-' , label = 'Std (Normal)' ) plt . plot ( results_delta [ 'x' ], results_delta [ 'Mean' ], 'b-' , label = 'Delta (Uniform)' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Mean' ) plt . title ( 'Comparison of Mean Response' ) plt . legend () plt . grid ( True ) # Plot standard deviations plt . subplot ( 2 , 1 , 2 ) plt . plot ( results_cov [ 'x' ], results_cov [ 'StdDev' ], 'r-' , label = 'CoV (Normal)' ) plt . plot ( results_std [ 'x' ], results_std [ 'StdDev' ], 'g-' , label = 'Std (Normal)' ) plt . plot ( results_delta [ 'x' ], results_delta [ 'StdDev' ], 'b-' , label = 'Delta (Uniform)' ) plt . xlabel ( 'x' ) plt . ylabel ( 'Standard Deviation' ) plt . title ( 'Comparison of Uncertainty Propagation' ) plt . legend () plt . grid ( True ) plt . tight_layout () plt . savefig ( 'uncertainty_specification_comparison.png' ) plt . show () # Analysis at a specific point x_value = 2.0 design_point = { 'x' : x_value } result_cov = uq_cov . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) result_std = uq_std . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) result_delta = uq_delta . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) print ( f \" \\n Comparison at x = { x_value } :\" ) print ( f \"1. CoV method:\" ) print ( f \" Mean: { result_cov [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \" Std: { result_cov [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \" 95% CI: [ { result_cov [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_cov [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) print ( f \" \\n 2. Std method:\" ) print ( f \" Mean: { result_std [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \" Std: { result_std [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \" 95% CI: [ { result_std [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_std [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) print ( f \" \\n 3. Delta method (uniform):\" ) print ( f \" Mean: { result_delta [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \" Std: { result_delta [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \" 95% CI: [ { result_delta [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_delta [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" )","title":"Comparison of Different Uncertainty Specifications"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#using-environmental-variables-with-different-specifications","text":"This example demonstrates how to use environmental variables with different uncertainty specifications: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation # Simple function to demonstrate environmental variable effects def env_sensitivity_function ( X ): \"\"\" Function showing sensitivity to different environmental variables X[:, 0] = Design variable X[:, 1] = Normal env variable with std X[:, 2] = Normal env variable with cov X[:, 3] = Uniform env variable with min/max X[:, 4] = Uniform env variable with low/high \"\"\" dv = X [:, 0 ] # Design variable env1 = X [:, 1 ] # Normal with std env2 = X [:, 2 ] # Normal with cov env3 = X [:, 3 ] # Uniform with min/max env4 = X [:, 4 ] # Uniform with low/high # Base response depends on design variable base = dv ** 2 # Environmental effects effect1 = 2 * env1 # Linear effect effect2 = env2 ** 2 # Quadratic effect effect3 = np . sin ( 3 * env3 ) # Oscillatory effect effect4 = np . exp ( 0.1 * env4 ) # Exponential effect return base + effect1 + effect2 + effect3 + effect4 # Setup data configuration with different ways to specify env variables data_info = { 'variables' : [ { 'name' : 'design_var' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 0 , 5 ], 'std' : 0.2 , 'distribution' : 'normal' , 'description' : 'Design variable with std' }, { 'name' : 'env_normal_std' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 0 , 'std' : 0.5 , # Using std directly 'description' : 'Normal env variable with std' }, { 'name' : 'env_normal_cov' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 2.0 , 'cov' : 0.1 , # Using cov instead of std 'description' : 'Normal env variable with cov' }, { 'name' : 'env_uniform_minmax' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'min' : - 1.0 , # Using min/max notation 'max' : 1.0 , 'description' : 'Uniform env variable with min/max' }, { 'name' : 'env_uniform_lowhigh' , 'vars_type' : 'env_vars' , 'distribution' : 'uniform' , 'low' : 0.0 , # Using low/high notation (for backward compatibility) 'high' : 2.0 , 'description' : 'Uniform env variable with low/high' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/env_variables_demo.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/env_variables_demo.json\" , true_func = env_sensitivity_function , output_dir = \"RESULT_ENV_VARS_DEMO\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 100 , num_mcs_samples = 5000 ) # Analyze a specific point with environmental variable overrides design_point = { 'design_var' : 2.5 } # Default environmental conditions result_default = uq . analyze_specific_point ( design_point = design_point , num_mcs_samples = 10000 ) # Override with different specifications env_override = { 'env_normal_std' : { 'mean' : 1.0 , 'std' : 1.0 }, # Override using std 'env_normal_cov' : { 'mean' : 3.0 , 'cov' : 0.2 }, # Override using cov 'env_uniform_minmax' : { 'min' : - 2.0 , 'max' : 0.0 }, # Override using min/max 'env_uniform_lowhigh' : { 'low' : 1.0 , 'high' : 3.0 } # Override using low/high } result_override = uq . analyze_specific_point ( design_point = design_point , env_vars_override = env_override , num_mcs_samples = 10000 ) # Print comparison print ( f \"Analysis at design_var = { design_point [ 'design_var' ] } :\" ) print ( \" \\n Default Environmental Conditions:\" ) print ( f \"Mean: { result_default [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \"Std: { result_default [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \"95% CI: [ { result_default [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_default [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" ) print ( \" \\n Modified Environmental Conditions:\" ) print ( f \"Mean: { result_override [ 'statistics' ][ 'mean' ] : .4f } \" ) print ( f \"Std: { result_override [ 'statistics' ][ 'std' ] : .4f } \" ) print ( f \"95% CI: [ { result_override [ 'statistics' ][ 'percentiles' ][ '2.5' ] : .4f } , { result_override [ 'statistics' ][ 'percentiles' ][ '97.5' ] : .4f } ]\" )","title":"Using Environmental Variables with Different Specifications"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#environmental-variable-sensitivity-analysis","text":"This example shows how to perform a basic sensitivity analysis with different environmental variable configurations: import numpy as np from PyEGRO.uncertainty.UQmcs import UncertaintyPropagation import matplotlib.pyplot as plt # Define a function sensitive to environmental variables def env_sensitivity ( X ): \"\"\" Function with design and environmental variables X[:, 0] = Design variable (temperature) X[:, 1] = Environmental variable 1 (humidity) X[:, 2] = Environmental variable 2 (pressure) \"\"\" temp = X [:, 0 ] humidity = X [:, 1 ] pressure = X [:, 2 ] # Complex interaction between variables return temp ** 2 + 5 * humidity -. + 0.2 * pressure + 0.5 * temp * humidity - 0.1 * temp * pressure # Create data configuration data_info = { 'variables' : [ { 'name' : 'temperature' , 'vars_type' : 'design_vars' , 'range_bounds' : [ 20 , 30 ], 'std' : 0.5 , # Standard deviation of 0.5\u00b0C 'distribution' : 'normal' , 'description' : 'Operating temperature (\u00b0C)' }, { 'name' : 'humidity' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 50 , 'std' : 10 , # Standard deviation of 10% 'description' : 'Relative humidity (%)' }, { 'name' : 'pressure' , 'vars_type' : 'env_vars' , 'distribution' : 'normal' , 'mean' : 101.3 , 'std' : 2.0 , # Standard deviation of 2 kPa 'description' : 'Atmospheric pressure (kPa)' } ] } # Save configuration import json import os os . makedirs ( \"DATA_PREPARATION\" , exist_ok = True ) with open ( \"DATA_PREPARATION/sensitivity.json\" , 'w' ) as f : json . dump ( data_info , f ) # Create UncertaintyPropagation instance uq = UncertaintyPropagation ( data_info_path = \"DATA_PREPARATION/sensitivity.json\" , true_func = env_sensitivity , output_dir = \"RESULT_SENSITIVITY\" ) # Run analysis results = uq . run_analysis ( num_design_samples = 50 , num_mcs_samples = 5000 ) # Define different environmental scenarios nominal_point = { 'temperature' : 25.0 } scenarios = [ { \"name\" : \"Nominal\" , \"override\" : None }, { \"name\" : \"High Humidity\" , \"override\" : { 'humidity' : { 'mean' : 80 , 'std' : 5 }}}, { \"name\" : \"Low Humidity\" , \"override\" : { 'humidity' : { 'mean' : 20 , 'std' : 5 }}}, { \"name\" : \"High Pressure\" , \"override\" : { 'pressure' : { 'mean' : 110 , 'std' : 1 }}}, { \"name\" : \"Low Pressure\" , \"override\" : { 'pressure' : { 'mean' : 90 , 'std' : 1 }}}, { \"name\" : \"Extreme Conditions\" , \"override\" : { 'humidity' : { 'mean' : 85 , 'std' : 8 }, 'pressure' : { 'mean' : 88 , 'std' : 3 } }} ] # Analyze each scenario scenario_results = [] for scenario in scenarios : result = uq . analyze_specific_point ( design_point = nominal_point , env_vars_override = scenario [ \"override\" ], num_mcs_samples = 10000 , create_pdf = True , create_reliability = True ) scenario_results . append ({ \"name\" : scenario [ \"name\" ], \"mean\" : result [ 'statistics' ][ 'mean' ], \"std\" : result [ 'statistics' ][ 'std' ], \"cov\" : result [ 'statistics' ][ 'cov' ], \"ci_lower\" : result [ 'statistics' ][ 'percentiles' ][ '2.5' ], \"ci_upper\" : result [ 'statistics' ][ 'percentiles' ][ '97.5' ] }) # Create comparison plot plt . figure ( figsize = ( 12 , 8 )) # Plot means with error bars means = [ r [ \"mean\" ] for r in scenario_results ] stds = [ r [ \"std\" ] for r in scenario_results ] names = [ r [ \"name\" ] for r in scenario_results ] x_pos = np . arange ( len ( names )) plt . bar ( x_pos , means , yerr = stds , align = 'center' , alpha = 0.7 , capsize = 10 ) plt . xticks ( x_pos , names , rotation = 45 ) plt . ylabel ( 'Mean Response' ) plt . title ( 'Environmental Sensitivity Analysis' ) plt . grid ( True , axis = 'y' , linestyle = '--' , alpha = 0.7 ) # Add values on top of bars for i , ( mean , std ) in enumerate ( zip ( means , stds )): plt . text ( i , mean + std + 0.5 , f \" { mean : .1f } \u00b1 { std : .1f } \" , ha = 'center' ) plt . tight_layout () plt . savefig ( 'env_sensitivity_analysis.png' ) plt . show () # Print detailed results print ( \" \\n Detailed Scenario Results:\" ) print ( \"-\" * 80 ) print ( f \" { 'Scenario' : <20 } { 'Mean' : <8 } { 'StdDev' : <8 } { 'CoV' : <8 } { '95% CI' : <20 } \" ) print ( \"-\" * 80 ) for r in scenario_results : print ( f \" { r [ 'name' ] : <20 } { r [ 'mean' ] : <8.2f } { r [ 'std' ] : <8.2f } { r [ 'cov' ] : <8.2f } [ { r [ 'ci_lower' ] : <8.2f } , { r [ 'ci_upper' ] : <8.2f } ]\" )","title":"Environmental Variable Sensitivity Analysis"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#remarks","text":"The UQmcs module now supports multiple ways to specify uncertainty in both design and environmental variables, making it more flexible and robust for various engineering scenarios. The key improvements include: Support for both CoV and Std : Use cov when uncertainty scales with the variable's value Use std for direct control of uncertainty width or when dealing with variables that cross zero Uniform uncertainty with Delta parameter : The delta parameter provides a simple way to specify uniform uncertainty around design points Consistent notation for bounds : Environmental variables now support both low / high and min / max notation for better compatibility These features allow for more accurate uncertainty modeling across a wide range of problems, from mechanical design to chemical processes to electronic circuits. When choosing between uncertainty specification methods, consider: - Use std for variables whose range crosses zero - Use cov for positive variables where uncertainty scales with magnitude - Use delta with uniform distribution for manufacturing tolerances or when uncertainty has sharp bounds For more information, refer to the UQmcs API Reference document.","title":"Remarks"},{"location":"basic-usage/uncertainty/approach-mcs/uqmcs_examples/#load-figpkl-for-ploting","text":"import pickle import matplotlib.pyplot as plt # Load the pickled figure with open ( 'uncertainty_analysis.fig.pkl' , 'rb' ) as file : fig = pickle . load ( file ) # Display the figure plt . show ()","title":"** Load .fig.pkl for ploting"},{"location":"examples/ego-metamodel-examples/","text":"","title":"Metamodel Training via EGO"},{"location":"examples/robust-example/","text":"","title":"Robust Optimization"},{"location":"examples/uq-examples/","text":"","title":"Uncertainty Propagation"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/","text":"Basics Surrogate Model Training Examples \u00b6 Objective Function \u00b6 The Gaussian-shaped test function is defined as: \\[ f(X_1, X_2) = - \\left[ A_1 \\cdot \\exp \\left( - \\left( \\frac{(X_1 - \\mu_1)^2}{2 \\sigma_1^2} + \\frac{(X_2 - \\mu_2)^2}{2 \\sigma_2^2} \\right) \\right) + A_2 \\cdot \\exp \\left( - \\left( \\frac{(X_1 - \\mu_3)^2}{2 \\sigma_3^2} + \\frac{(X_2 - \\mu_4)^2}{2 \\sigma_4^2} \\right) \\right) \\right] - 200 \\] Where: \\(A_1, A_2\\) : Amplitudes of the Gaussian peaks. \\(\\mu_1, \\mu_2, \\mu_3, \\mu_4\\) : Centers of the peaks. \\(\\sigma_1, \\sigma_2, \\sigma_3, \\sigma_4\\) : Standard deviations controlling the spread of the peaks. Constants \u00b6 The constants for the Gaussian peaks are defined as follows: \\(A_1 = 100, A_2 = 150\\) \\(\\mu_1 = 3, \\mu_2 = 2.1, \\mu_3 = -1.5, \\mu_4 = -1.2\\) \\(\\sigma_1 = 3, \\sigma_2 = 3, \\sigma_3 = 1, \\sigma_4 = 1\\) Deterministic Optimal Points \u00b6 The deterministic solution yields the following minima in the design variables \\(X_1\\) and \\(X_2\\) from the ranges \\([-5, 5]\\) and \\([-6, 6]\\) : Global Minimum: Function Value: \\(f(X_1, X_2) = -31.85\\) At \\((X_1, X_2) = (-1.44, -1.16)\\) Local Minimum: Function Value: \\(f(X_1, X_2) = -100\\) At \\((X_1, X_2) = (3.0, 2.10)\\) This setup allows the function to test optimization algorithms for their ability to handle varying sensitivities across both global and local optima, as demonstrated by the deterministic solutions. The response surface across the entire design variable range is visualized in the Figure. 1. Define the Test Function \u00b6 To define the Gaussian-shaped test function, create a separate Python file named objective.py : File: objective.py import numpy as np def true_function ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f This function represents two Gaussian peaks, one for the global minimum and another for the local minimum. 2. Generate Design Samples \u00b6 Steps Explained : Define Variables: Add design variables x1 and x2 with their respective bounds and covariance values. Save Configuration: Save the configuration for reproducibility. Run Sampling: Generate 200 samples using the defined test function. To generate initial design samples using LHS, create a file named run_initial_design.py : File: run_initial_design.py from PyEGRO.doe.initial_design import InitialDesign from objective import true_function # Create design with LHS sampling design_lhs = InitialDesign ( sampling_method = \"lhs\" , show_progress = True ) # Add design variables design_lhs . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.2 , description = 'first design variable' ) design_lhs . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.15 , description = 'second design variable' ) # Save configuration design_lhs . save () # Run sampling results_lhs = design_lhs . run ( objective_function = true_function , num_samples = 200 ) Output Command line Displays Save Output Directory DATA_PREPARATION 3. Train the Surrogate Model \u00b6 Steps Explained : Initialize MetaTraining: Set up the training configuration and preprocessing options. Train Model: Train the Gaussian Process Regression (GPR) surrogate model using the sampled data. Save Results: Save the trained model and data scalers for future use. Use the generated design samples to train the surrogate model. Below is an example script for surrogate model training: File: train_surrogate.py from PyEGRO.meta.meta_trainer import MetaTraining # Initialize MetaTraining meta = MetaTraining () # Train surrogate model model , scaler_X , scaler_y = meta . train () # Save trained model and scalers meta . save () Output Command line Displays Save Output Directory RESULT_MODEL_GPR Performance plot example","title":"Basics Metamodel Training"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/#basics-surrogate-model-training-examples","text":"","title":"Basics Surrogate Model Training Examples"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/#objective-function","text":"The Gaussian-shaped test function is defined as: \\[ f(X_1, X_2) = - \\left[ A_1 \\cdot \\exp \\left( - \\left( \\frac{(X_1 - \\mu_1)^2}{2 \\sigma_1^2} + \\frac{(X_2 - \\mu_2)^2}{2 \\sigma_2^2} \\right) \\right) + A_2 \\cdot \\exp \\left( - \\left( \\frac{(X_1 - \\mu_3)^2}{2 \\sigma_3^2} + \\frac{(X_2 - \\mu_4)^2}{2 \\sigma_4^2} \\right) \\right) \\right] - 200 \\] Where: \\(A_1, A_2\\) : Amplitudes of the Gaussian peaks. \\(\\mu_1, \\mu_2, \\mu_3, \\mu_4\\) : Centers of the peaks. \\(\\sigma_1, \\sigma_2, \\sigma_3, \\sigma_4\\) : Standard deviations controlling the spread of the peaks.","title":"Objective Function"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/#constants","text":"The constants for the Gaussian peaks are defined as follows: \\(A_1 = 100, A_2 = 150\\) \\(\\mu_1 = 3, \\mu_2 = 2.1, \\mu_3 = -1.5, \\mu_4 = -1.2\\) \\(\\sigma_1 = 3, \\sigma_2 = 3, \\sigma_3 = 1, \\sigma_4 = 1\\)","title":"Constants"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/#deterministic-optimal-points","text":"The deterministic solution yields the following minima in the design variables \\(X_1\\) and \\(X_2\\) from the ranges \\([-5, 5]\\) and \\([-6, 6]\\) : Global Minimum: Function Value: \\(f(X_1, X_2) = -31.85\\) At \\((X_1, X_2) = (-1.44, -1.16)\\) Local Minimum: Function Value: \\(f(X_1, X_2) = -100\\) At \\((X_1, X_2) = (3.0, 2.10)\\) This setup allows the function to test optimization algorithms for their ability to handle varying sensitivities across both global and local optima, as demonstrated by the deterministic solutions. The response surface across the entire design variable range is visualized in the Figure.","title":"Deterministic Optimal Points"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/#1-define-the-test-function","text":"To define the Gaussian-shaped test function, create a separate Python file named objective.py : File: objective.py import numpy as np def true_function ( x ): X1 , X2 = x [:, 0 ], x [:, 1 ] a1 , x1 , y1 , sigma_x1 , sigma_y1 = 100 , 3 , 2.1 , 3 , 3 a2 , x2 , y2 , sigma_x2 , sigma_y2 = 150 , - 1.5 , - 1.2 , 1 , 1 f = - ( a1 * np . exp ( - (( X1 - x1 ) ** 2 / ( 2 * sigma_x1 ** 2 ) + ( X2 - y1 ) ** 2 / ( 2 * sigma_y1 ** 2 ))) + a2 * np . exp ( - (( X1 - x2 ) ** 2 / ( 2 * sigma_x2 ** 2 ) + ( X2 - y2 ) ** 2 / ( 2 * sigma_y2 ** 2 ))) - 200 ) return f This function represents two Gaussian peaks, one for the global minimum and another for the local minimum.","title":"1. Define the Test Function"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/#2-generate-design-samples","text":"Steps Explained : Define Variables: Add design variables x1 and x2 with their respective bounds and covariance values. Save Configuration: Save the configuration for reproducibility. Run Sampling: Generate 200 samples using the defined test function. To generate initial design samples using LHS, create a file named run_initial_design.py : File: run_initial_design.py from PyEGRO.doe.initial_design import InitialDesign from objective import true_function # Create design with LHS sampling design_lhs = InitialDesign ( sampling_method = \"lhs\" , show_progress = True ) # Add design variables design_lhs . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.2 , description = 'first design variable' ) design_lhs . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.15 , description = 'second design variable' ) # Save configuration design_lhs . save () # Run sampling results_lhs = design_lhs . run ( objective_function = true_function , num_samples = 200 ) Output Command line Displays Save Output Directory DATA_PREPARATION","title":"2. Generate Design Samples"},{"location":"examples/basics-metamodel-examples/basics-metamodel-examples/#3-train-the-surrogate-model","text":"Steps Explained : Initialize MetaTraining: Set up the training configuration and preprocessing options. Train Model: Train the Gaussian Process Regression (GPR) surrogate model using the sampled data. Save Results: Save the trained model and data scalers for future use. Use the generated design samples to train the surrogate model. Below is an example script for surrogate model training: File: train_surrogate.py from PyEGRO.meta.meta_trainer import MetaTraining # Initialize MetaTraining meta = MetaTraining () # Train surrogate model model , scaler_X , scaler_y = meta . train () # Save trained model and scalers meta . save () Output Command line Displays Save Output Directory RESULT_MODEL_GPR Performance plot example","title":"3. Train the Surrogate Model"},{"location":"getting-started/installation/","text":"Installation packages depends \u00b6 The PyEGRO depends on the following packages: numpy pandas pymoo PyDOE choaspy torch gpytorch scikit-learn matplotlib joblib rich Ensure these dependencies are installed before using the module: pip install numpy pandas pymoo PyDOE choaspy torch gpytorch scikit-learn matplotlib joblib rich Installation PyEGRO \u00b6 Install PyEGRO using pip: pip install PyEGRO","title":"Installation"},{"location":"getting-started/installation/#installation-packages-depends","text":"The PyEGRO depends on the following packages: numpy pandas pymoo PyDOE choaspy torch gpytorch scikit-learn matplotlib joblib rich Ensure these dependencies are installed before using the module: pip install numpy pandas pymoo PyDOE choaspy torch gpytorch scikit-learn matplotlib joblib rich","title":"Installation packages depends"},{"location":"getting-started/installation/#installation-pyegro","text":"Install PyEGRO using pip: pip install PyEGRO","title":"Installation PyEGRO"},{"location":"getting-started/quickstart/","text":"","title":"Quick Start"},{"location":"user-guide/doe/overviewdoe/","text":"Design of Experiment / Overview \u00b6 Sampling Methods \u00b6 Latin Hypercube Sampling (LHS): Latin Hypercube Sampling divides each variable's range into \\(N\\) equal intervals and ensures one sample is drawn from each interval without repetition. It maximizes uniformity in the sampling process: \\[ x_i \\sim U\\left(\\frac{k-1}{N}, \\frac{k}{N}\\right), \\quad k = 1, \\ldots, N \\] where: - \\( N \\) : Total number of samples - \\( U(a, b) \\) : Uniform distribution between \\( a \\) and \\( b \\) Sobol Sequence: Sobol sequences are quasi-random low-discrepancy sequences designed to achieve uniformity in the sampling space. The discrepancy \\( D \\) measures the uniformity, which is minimized in Sobol sampling: \\[ D \\leq \\frac{\\log(N)^d}{N} \\] where: - \\( N \\) : Total number of samples - \\( d \\) : Dimensionality of the problem Random Sampling: Random sampling draws samples \\( x \\) uniformly from the range \\( [a_i, b_i] \\) : \\[ x_i \\sim U(a_i, b_i) \\] Sampling Criteria Sampling criteria are additional constraints or goals applied during sampling, such as: - Maximin Criterion: Maximizes the minimum distance between samples. - Centering Criterion: Ensures that samples are centered within each interval. Objective Function Evaluation \u00b6 Once the samples \\( x \\) are generated, they are passed to an objective function \\( f(x) \\) to evaluate the desired output: \\[ \\text{Result} = f(x) \\] Where: - \\( x \\) : Sampled input vector - \\( f(x) \\) : User-defined objective function Performance Metrics for Sampling To evaluate the quality of the sampling, metrics such as space-filling , discrepancy , and variance are often used.","title":"Design of Experiment / Overview"},{"location":"user-guide/doe/overviewdoe/#design-of-experiment-overview","text":"","title":"Design of Experiment / Overview"},{"location":"user-guide/doe/overviewdoe/#sampling-methods","text":"Latin Hypercube Sampling (LHS): Latin Hypercube Sampling divides each variable's range into \\(N\\) equal intervals and ensures one sample is drawn from each interval without repetition. It maximizes uniformity in the sampling process: \\[ x_i \\sim U\\left(\\frac{k-1}{N}, \\frac{k}{N}\\right), \\quad k = 1, \\ldots, N \\] where: - \\( N \\) : Total number of samples - \\( U(a, b) \\) : Uniform distribution between \\( a \\) and \\( b \\) Sobol Sequence: Sobol sequences are quasi-random low-discrepancy sequences designed to achieve uniformity in the sampling space. The discrepancy \\( D \\) measures the uniformity, which is minimized in Sobol sampling: \\[ D \\leq \\frac{\\log(N)^d}{N} \\] where: - \\( N \\) : Total number of samples - \\( d \\) : Dimensionality of the problem Random Sampling: Random sampling draws samples \\( x \\) uniformly from the range \\( [a_i, b_i] \\) : \\[ x_i \\sim U(a_i, b_i) \\] Sampling Criteria Sampling criteria are additional constraints or goals applied during sampling, such as: - Maximin Criterion: Maximizes the minimum distance between samples. - Centering Criterion: Ensures that samples are centered within each interval.","title":"Sampling Methods"},{"location":"user-guide/doe/overviewdoe/#objective-function-evaluation","text":"Once the samples \\( x \\) are generated, they are passed to an objective function \\( f(x) \\) to evaluate the desired output: \\[ \\text{Result} = f(x) \\] Where: - \\( x \\) : Sampled input vector - \\( f(x) \\) : User-defined objective function Performance Metrics for Sampling To evaluate the quality of the sampling, metrics such as space-filling , discrepancy , and variance are often used.","title":"Objective Function Evaluation"},{"location":"user-guide/doe/usagedoe/","text":"Design of Experiment / Usage \u00b6 This document provides examples for using the PyEGRO DOE module across different scenarios. The examples showcase variable definition, sampling methods, and saving/loading configurations. Default Settings Usage \u00b6 Goal : Run the sampling process with default settings for rapid setup and testing. Code : from PyEGRO.initial_design import InitialDesign # Create objective function def objective_function ( x ): x1 , x2 = x [:, 0 ], x [:, 1 ] # format of input variable y = x1 ** 2 + x2 ** 2 # Simple quadratic function return y # Create design with default settings design = InitialDesign ( sampling_method = 'lhs' , # Default: Latin Hypercube Sampling show_progress = True ) # Define a basic design variable design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 , description = 'First variable' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.1 , description = 'Second variable' ) # Save data to file design . save () # Run the sampling process design . run ( objective_function = objective_function , num_samples = 10 # Default number of samples ) Output : training_data.csv : Contains generated samples with default settings. No additional configuration files are created unless explicitly saved. 1. Basic LHS Sampling \u00b6 Goal : Generate samples using Latin Hypercube Sampling (LHS) with two design variables. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] ** 2 + x [:, 1 ] ** 2 # Simple quadratic function design = InitialDesign ( output_dir = 'DATA_PREPARATION_LHS' , sampling_method = 'lhs' , sampling_criterion = 'maximin' , show_progress = True ) # Define design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 , description = 'First variable' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.1 , description = 'Second variable' ) # Save configuration and run sampling design . save ( \"lhs_config\" ) results = design . run ( objective_function = objective_function , num_samples = 50 ) Output : 1. lhs_config.json : Contains design configuration. 2. training_data.csv : Generated samples and objective function values. 2. Sobol Sequence Sampling \u00b6 Goal : Generate samples using the Sobol sequence for a low-discrepancy sampling approach. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return ( x [:, 0 ] - 1 ) ** 2 + ( x [:, 1 ] - 2 ) ** 2 design = InitialDesign ( output_dir = 'DATA_PREPARATION_SOBOL' , sampling_method = 'sobol' , show_progress = True ) # Define design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.2 , description = 'Variable x1' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.2 , description = 'Variable x2' ) # Save configuration and run sampling design . save ( \"sobol_config\" ) results = design . run ( objective_function = objective_function , num_samples = 100 ) Output : 1. sobol_config.json 2. training_data.csv 3. Mixed Variable Types \u00b6 Goal : Generate samples with both design and environmental variables. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] ** 2 + 3 * x [:, 1 ] design = InitialDesign ( output_dir = 'DATA_PREPARATION_MIXED' , sampling_method = 'lhs' , show_progress = True ) # Define design variable design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 , description = 'Design variable x1' ) # Define environmental variable design . add_env_variable ( name = 'env1' , distribution = 'normal' , mean = 10 , cov = 0.2 , description = 'Environmental variable env1' ) # Save configuration and run sampling design . save ( \"mixed_config\" ) results = design . run ( objective_function = objective_function , num_samples = 50 ) Output : 1. mixed_config.json 2. training_data.csv 4. Random Sampling \u00b6 Goal : Generate samples using uniform random sampling. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] * x [:, 1 ] design = InitialDesign ( output_dir = 'DATA_PREPARATION_RANDOM' , sampling_method = 'random' , show_progress = True ) # Define design variables design . add_design_variable ( name = 'x1' , range_bounds = [ 0 , 10 ], cov = 0.05 , description = 'Random variable x1' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 10 , 0 ], cov = 0.05 , description = 'Random variable x2' ) # Save configuration and run sampling design . save ( \"random_config\" ) results = design . run ( objective_function = objective_function , num_samples = 20 ) Output : 1. random_config.json 2. training_data.csv 5. Loading and Reusing Configurations \u00b6 Goal : Reuse a saved configuration to generate new samples. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] + x [:, 1 ] # Load configuration design = InitialDesign ( output_dir = 'DATA_PREPARATION_REUSE' , sampling_method = 'lhs' ) design . load ( \"lhs_config\" ) # Run with different sample count results = design . run ( objective_function = objective_function , num_samples = 100 ) Output : 1. Updated training_data.csv","title":"Design of Experiment / Usage"},{"location":"user-guide/doe/usagedoe/#design-of-experiment-usage","text":"This document provides examples for using the PyEGRO DOE module across different scenarios. The examples showcase variable definition, sampling methods, and saving/loading configurations.","title":"Design of Experiment / Usage"},{"location":"user-guide/doe/usagedoe/#default-settings-usage","text":"Goal : Run the sampling process with default settings for rapid setup and testing. Code : from PyEGRO.initial_design import InitialDesign # Create objective function def objective_function ( x ): x1 , x2 = x [:, 0 ], x [:, 1 ] # format of input variable y = x1 ** 2 + x2 ** 2 # Simple quadratic function return y # Create design with default settings design = InitialDesign ( sampling_method = 'lhs' , # Default: Latin Hypercube Sampling show_progress = True ) # Define a basic design variable design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 , description = 'First variable' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.1 , description = 'Second variable' ) # Save data to file design . save () # Run the sampling process design . run ( objective_function = objective_function , num_samples = 10 # Default number of samples ) Output : training_data.csv : Contains generated samples with default settings. No additional configuration files are created unless explicitly saved.","title":"Default Settings Usage"},{"location":"user-guide/doe/usagedoe/#1-basic-lhs-sampling","text":"Goal : Generate samples using Latin Hypercube Sampling (LHS) with two design variables. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] ** 2 + x [:, 1 ] ** 2 # Simple quadratic function design = InitialDesign ( output_dir = 'DATA_PREPARATION_LHS' , sampling_method = 'lhs' , sampling_criterion = 'maximin' , show_progress = True ) # Define design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 , description = 'First variable' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.1 , description = 'Second variable' ) # Save configuration and run sampling design . save ( \"lhs_config\" ) results = design . run ( objective_function = objective_function , num_samples = 50 ) Output : 1. lhs_config.json : Contains design configuration. 2. training_data.csv : Generated samples and objective function values.","title":"1. Basic LHS Sampling"},{"location":"user-guide/doe/usagedoe/#2-sobol-sequence-sampling","text":"Goal : Generate samples using the Sobol sequence for a low-discrepancy sampling approach. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return ( x [:, 0 ] - 1 ) ** 2 + ( x [:, 1 ] - 2 ) ** 2 design = InitialDesign ( output_dir = 'DATA_PREPARATION_SOBOL' , sampling_method = 'sobol' , show_progress = True ) # Define design variables design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.2 , description = 'Variable x1' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 6 , 6 ], cov = 0.2 , description = 'Variable x2' ) # Save configuration and run sampling design . save ( \"sobol_config\" ) results = design . run ( objective_function = objective_function , num_samples = 100 ) Output : 1. sobol_config.json 2. training_data.csv","title":"2. Sobol Sequence Sampling"},{"location":"user-guide/doe/usagedoe/#3-mixed-variable-types","text":"Goal : Generate samples with both design and environmental variables. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] ** 2 + 3 * x [:, 1 ] design = InitialDesign ( output_dir = 'DATA_PREPARATION_MIXED' , sampling_method = 'lhs' , show_progress = True ) # Define design variable design . add_design_variable ( name = 'x1' , range_bounds = [ - 5 , 5 ], cov = 0.1 , description = 'Design variable x1' ) # Define environmental variable design . add_env_variable ( name = 'env1' , distribution = 'normal' , mean = 10 , cov = 0.2 , description = 'Environmental variable env1' ) # Save configuration and run sampling design . save ( \"mixed_config\" ) results = design . run ( objective_function = objective_function , num_samples = 50 ) Output : 1. mixed_config.json 2. training_data.csv","title":"3. Mixed Variable Types"},{"location":"user-guide/doe/usagedoe/#4-random-sampling","text":"Goal : Generate samples using uniform random sampling. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] * x [:, 1 ] design = InitialDesign ( output_dir = 'DATA_PREPARATION_RANDOM' , sampling_method = 'random' , show_progress = True ) # Define design variables design . add_design_variable ( name = 'x1' , range_bounds = [ 0 , 10 ], cov = 0.05 , description = 'Random variable x1' ) design . add_design_variable ( name = 'x2' , range_bounds = [ - 10 , 0 ], cov = 0.05 , description = 'Random variable x2' ) # Save configuration and run sampling design . save ( \"random_config\" ) results = design . run ( objective_function = objective_function , num_samples = 20 ) Output : 1. random_config.json 2. training_data.csv","title":"4. Random Sampling"},{"location":"user-guide/doe/usagedoe/#5-loading-and-reusing-configurations","text":"Goal : Reuse a saved configuration to generate new samples. Code : from PyEGRO.initial_design import InitialDesign def objective_function ( x ): return x [:, 0 ] + x [:, 1 ] # Load configuration design = InitialDesign ( output_dir = 'DATA_PREPARATION_REUSE' , sampling_method = 'lhs' ) design . load ( \"lhs_config\" ) # Run with different sample count results = design . run ( objective_function = objective_function , num_samples = 100 ) Output : 1. Updated training_data.csv","title":"5. Loading and Reusing Configurations"},{"location":"user-guide/ego/overviewego/","text":"Efficient Global Optimization (EGO) / Overview \u00b6 Applications for enhancing Global Model Accuracy Efficient Global Optimization (EGO) is widely applied to improve the accuracy of global models for predictive tasks across the entire input domain. By leveraging surrogate models, such as Gaussian Processes (GP), EGO provides high-fidelity approximations of complex systems, ensuring accurate predictions for areas that are underexplored or uncertain. Key Features \u00b6 Surrogate Modeling : Uses Gaussian Processes Regression (GPR) to approximate expensive-to-evaluate objective functions. Acquisition Functions Expected Improvement (EI) \\(\\zeta\\) -Expected Improvement ( \\(\\zeta\\) -EI) Exploration Enhanced EI (E3I) Expected Improvement for Global Fit (EIGF) Distance-Enhanced Gradient (CRI3) Optimization Configuration Maximum iterations and stopping criteria RMSE thresholds and patience settings Relative improvement thresholds Flexible device selection (CPU/GPU) Visualization Tools 1D and 2D optimization problem visualizations Convergence plots Uncertainty analysis and parameter tracking Progress Tracking Detailed console outputs Metrics tracking for each iteration Integration with rich progress bars for an interactive experience Acquisition Functions \u00b6 Acquisition functions guide the optimization process. Below are the formulations: Expected Improvement (EI) \\[ EI(x) = \\begin{cases} \\sigma(x)\\phi(Z) + (y_{\\text{min}} - \\mu(x))\\Phi(Z) & \\text{if } \\sigma(x) > 0 \\\\ 0 & \\text{if } \\sigma(x) = 0 \\end{cases} \\] \\[ Z = \\frac{y_{\\text{min}} - \\mu(x)}{\\sigma(x)} \\] where \\(\\phi(Z)\\) is the Probability Density Function (PDF) and \\(\\Phi(Z)\\) is the Cumulative Density Function (CDF). \\(\\zeta\\) -Expected Improvement ( \\(\\zeta\\) -EI) \\[ \\zeta-EI(x) = \\begin{cases} \\sigma(x)\\phi(Z) + (y_{\\text{min}} - \\mu(x) - \\zeta)\\Phi(Z) & \\text{if } \\sigma(x) > 0 \\\\ 0 & \\text{if } \\sigma(x) = 0 \\end{cases} \\] \\[ Z = \\frac{y_{\\text{min}} - \\mu(x) - \\zeta}{\\sigma(x)} \\] Exploration Enhanced EI (E3I) [ 1 ] \\[ \\alpha^{E3I}(x) = \\frac{1}{M}\\mathbb{E}_x\\left[I(x, g_m)\\right] = \\begin{cases} \\frac{\\sigma(x)}{M}\\sum_{m=1}^{M}\\tau(Z) & \\text{if } \\sigma(x) > 0 \\\\ 0 & \\text{if } \\sigma(x) = 0 \\end{cases} \\] \\[ \\tau(Z) = Z\\Phi(Z) + \\phi(Z), \\quad Z = \\frac{y_{\\text{min}} - g_m(x)}{\\sigma(x)} \\] where \\(g_m\\) is the maximum value from \\(M\\) Thompson samples. Expected Improvement for Global Fit (EIGF) [ 2 ] \\[ EIGF(x) = (\\hat{y}(x) - y(x_{\\text{nearest}}))^2 + \\sigma^2(x) \\] where: - \\(\\hat{y}(x)\\) : Predicted mean at point \\(x\\) . - \\(y(x_{\\text{nearest}})\\) : Observed output at the nearest sampled point to \\(x\\) . - \\(\\sigma^2(x)\\) : Variance of the prediction at \\(x\\) . Distance-Enhanced Gradient (CRI3) [ 3 ] \\[ \\text{Crit}(\\xi) = (\\left|\\frac{\\partial \\hat{f}(\\xi)}{\\partial \\xi}\\right| \\Delta(\\xi) + D_f(\\xi)) \\hat{\\sigma}(\\xi) PDF(\\xi). \\] where: \\(\\Delta(\\xi) = \\min_{i=1,2,...,N}|\\xi - \\xi^{(i)}|\\) : Distance to the nearest sample. \\(D_f(\\xi) = |\\hat{f}(\\xi|\\theta) - \\hat{f}(\\xi|2\\theta)|\\) : Polynomial error estimate. \\(\\hat{\\sigma}(\\xi)PDF(\\xi)\\) : Predictive variance multiplied by the PDF of the input space. Output and Files \u00b6 Each EGO run produces: Optimization History: Tracks iteration details, objective values, and model parameters. Trained Models: Stores the final GP model and associated hyperparameters. Visualization Plots: For tracking optimization progress and uncertainty. References 1. Berk, J., Nguyen, V., Gupta, S., Rana, S., & Venkatesh, S. (2019). Exploration enhanced expected improvement for Bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part II 18 (pp. 621-637). Springer International Publishing. 2. Lam, C. Q. (2008). Sequential adaptive designs in computer experiments for response surface model fit (Doctoral dissertation, The Ohio State University). 3. Shimoyama, K., & Kawai, S. (2019). A kriging-based dynamic adaptive sampling method for uncertainty quantification. Transactions of the Japan Society for Aeronautical and Space Sciences, 62 (3), 137-150. Algorithm Comparison \u00b6","title":"Efficient Global Optimization (EGO) / Overview"},{"location":"user-guide/ego/overviewego/#efficient-global-optimization-ego-overview","text":"Applications for enhancing Global Model Accuracy Efficient Global Optimization (EGO) is widely applied to improve the accuracy of global models for predictive tasks across the entire input domain. By leveraging surrogate models, such as Gaussian Processes (GP), EGO provides high-fidelity approximations of complex systems, ensuring accurate predictions for areas that are underexplored or uncertain.","title":"Efficient Global Optimization (EGO) / Overview"},{"location":"user-guide/ego/overviewego/#key-features","text":"Surrogate Modeling : Uses Gaussian Processes Regression (GPR) to approximate expensive-to-evaluate objective functions. Acquisition Functions Expected Improvement (EI) \\(\\zeta\\) -Expected Improvement ( \\(\\zeta\\) -EI) Exploration Enhanced EI (E3I) Expected Improvement for Global Fit (EIGF) Distance-Enhanced Gradient (CRI3) Optimization Configuration Maximum iterations and stopping criteria RMSE thresholds and patience settings Relative improvement thresholds Flexible device selection (CPU/GPU) Visualization Tools 1D and 2D optimization problem visualizations Convergence plots Uncertainty analysis and parameter tracking Progress Tracking Detailed console outputs Metrics tracking for each iteration Integration with rich progress bars for an interactive experience","title":"Key Features"},{"location":"user-guide/ego/overviewego/#acquisition-functions","text":"Acquisition functions guide the optimization process. Below are the formulations: Expected Improvement (EI) \\[ EI(x) = \\begin{cases} \\sigma(x)\\phi(Z) + (y_{\\text{min}} - \\mu(x))\\Phi(Z) & \\text{if } \\sigma(x) > 0 \\\\ 0 & \\text{if } \\sigma(x) = 0 \\end{cases} \\] \\[ Z = \\frac{y_{\\text{min}} - \\mu(x)}{\\sigma(x)} \\] where \\(\\phi(Z)\\) is the Probability Density Function (PDF) and \\(\\Phi(Z)\\) is the Cumulative Density Function (CDF). \\(\\zeta\\) -Expected Improvement ( \\(\\zeta\\) -EI) \\[ \\zeta-EI(x) = \\begin{cases} \\sigma(x)\\phi(Z) + (y_{\\text{min}} - \\mu(x) - \\zeta)\\Phi(Z) & \\text{if } \\sigma(x) > 0 \\\\ 0 & \\text{if } \\sigma(x) = 0 \\end{cases} \\] \\[ Z = \\frac{y_{\\text{min}} - \\mu(x) - \\zeta}{\\sigma(x)} \\] Exploration Enhanced EI (E3I) [ 1 ] \\[ \\alpha^{E3I}(x) = \\frac{1}{M}\\mathbb{E}_x\\left[I(x, g_m)\\right] = \\begin{cases} \\frac{\\sigma(x)}{M}\\sum_{m=1}^{M}\\tau(Z) & \\text{if } \\sigma(x) > 0 \\\\ 0 & \\text{if } \\sigma(x) = 0 \\end{cases} \\] \\[ \\tau(Z) = Z\\Phi(Z) + \\phi(Z), \\quad Z = \\frac{y_{\\text{min}} - g_m(x)}{\\sigma(x)} \\] where \\(g_m\\) is the maximum value from \\(M\\) Thompson samples. Expected Improvement for Global Fit (EIGF) [ 2 ] \\[ EIGF(x) = (\\hat{y}(x) - y(x_{\\text{nearest}}))^2 + \\sigma^2(x) \\] where: - \\(\\hat{y}(x)\\) : Predicted mean at point \\(x\\) . - \\(y(x_{\\text{nearest}})\\) : Observed output at the nearest sampled point to \\(x\\) . - \\(\\sigma^2(x)\\) : Variance of the prediction at \\(x\\) . Distance-Enhanced Gradient (CRI3) [ 3 ] \\[ \\text{Crit}(\\xi) = (\\left|\\frac{\\partial \\hat{f}(\\xi)}{\\partial \\xi}\\right| \\Delta(\\xi) + D_f(\\xi)) \\hat{\\sigma}(\\xi) PDF(\\xi). \\] where: \\(\\Delta(\\xi) = \\min_{i=1,2,...,N}|\\xi - \\xi^{(i)}|\\) : Distance to the nearest sample. \\(D_f(\\xi) = |\\hat{f}(\\xi|\\theta) - \\hat{f}(\\xi|2\\theta)|\\) : Polynomial error estimate. \\(\\hat{\\sigma}(\\xi)PDF(\\xi)\\) : Predictive variance multiplied by the PDF of the input space.","title":"Acquisition Functions"},{"location":"user-guide/ego/overviewego/#output-and-files","text":"Each EGO run produces: Optimization History: Tracks iteration details, objective values, and model parameters. Trained Models: Stores the final GP model and associated hyperparameters. Visualization Plots: For tracking optimization progress and uncertainty. References 1. Berk, J., Nguyen, V., Gupta, S., Rana, S., & Venkatesh, S. (2019). Exploration enhanced expected improvement for Bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part II 18 (pp. 621-637). Springer International Publishing. 2. Lam, C. Q. (2008). Sequential adaptive designs in computer experiments for response surface model fit (Doctoral dissertation, The Ohio State University). 3. Shimoyama, K., & Kawai, S. (2019). A kriging-based dynamic adaptive sampling method for uncertainty quantification. Transactions of the Japan Society for Aeronautical and Space Sciences, 62 (3), 137-150.","title":"Output and Files"},{"location":"user-guide/ego/overviewego/#algorithm-comparison","text":"","title":"Algorithm Comparison"},{"location":"user-guide/ego/usageego/","text":"Efficient Global Optimization (EGO) / Usage \u00b6 Basic Usage Examples \u00b6 To access usage information import PyEGRO.ego PyEGRO . ego . print_usage () Example 1: Optimizing a Quadratic Function import numpy as np import pandas as pd from PyEGRO.ego import EfficientGlobalOptimization , TrainingConfig # Define the objective function def objective_function ( x ): x1 , x2 = x [:, 0 ], x [:, 1 ] y = x1 ** 2 + x2 ** 2 return y # Set bounds and variable names bounds = np . array ([[ - 5 , 5 ], [ - 5 , 5 ]]) variable_names = [ 'x1' , 'x2' ] # Generate initial data n_initial = 5 initial_x = np . random . uniform ( bounds [:, 0 ], bounds [:, 1 ], size = ( n_initial , len ( variable_names ))) initial_y = objective_function ( initial_x ) initial_data = pd . DataFrame ( initial_x , columns = variable_names ) initial_data [ 'y' ] = initial_y # Configure EGO config = TrainingConfig ( max_iterations = 20 , rmse_threshold = 0.001 , acquisition_name = \"eigf\" ) # Run EGO ego = EfficientGlobalOptimization ( objective_func = objective_function , bounds = bounds , variable_names = variable_names , config = config , initial_data = initial_data ) result = ego . run () # Output the optimization history print ( result . history ) Advanced Usage \u00b6 Example 2: Optimizing with Preloaded Data import json import pandas as pd from PyEGRO.ego import EfficientGlobalOptimization , TrainingConfig # Load pre-existing data with open ( 'data_info.json' , 'r' ) as f : data_info = json . load ( f ) initial_data = pd . read_csv ( 'training_data.csv' ) # Extract bounds and variable names from data info bounds = np . array ( data_info [ 'input_bound' ]) variable_names = [ var [ 'name' ] for var in data_info [ 'variables' ]] # Define the objective function # Define the objective function def objective_function ( x ): x1 , x2 = x [:, 0 ], x [:, 1 ] y = x1 ** 2 + x2 ** 2 return y # Configure EGO config = TrainingConfig ( max_iterations = 50 , rmse_threshold = 0.0001 , acquisition_name = \"eigf\" ) # Run EGO ego = EfficientGlobalOptimization ( objective_func = objective_function , bounds = bounds , variable_names = variable_names , config = config , initial_data = initial_data ) result = ego . run () # Save the results result . save ( \"output_directory\" ) Configuration Options TrainingConfig() Parameters : max_iterations : Maximum number of optimization iterations (default: 100). rmse_threshold : Stopping criteria based on RMSE improvement (default: 0.001). acquisition_name : Choice of acquisition function (e.g., \"ei\", \"e3i\", \"eigf\"). device : Specifies whether to use \"cpu\" or \"cuda\" (default: \"cpu\"). save_dir : Directory to save the results and trained model. verbose : Displays progress during the optimization (default: True). This document provides examples for using the EGO module in PyEGRO to solve complex optimization problems, preloading data, and loading trained models for predictions. For further assistance, refer to the PyEGRO documentation or contact support.","title":"Efficient Global Optimization (EGO) / Usage"},{"location":"user-guide/ego/usageego/#efficient-global-optimization-ego-usage","text":"","title":"Efficient Global Optimization (EGO) / Usage"},{"location":"user-guide/ego/usageego/#basic-usage-examples","text":"To access usage information import PyEGRO.ego PyEGRO . ego . print_usage () Example 1: Optimizing a Quadratic Function import numpy as np import pandas as pd from PyEGRO.ego import EfficientGlobalOptimization , TrainingConfig # Define the objective function def objective_function ( x ): x1 , x2 = x [:, 0 ], x [:, 1 ] y = x1 ** 2 + x2 ** 2 return y # Set bounds and variable names bounds = np . array ([[ - 5 , 5 ], [ - 5 , 5 ]]) variable_names = [ 'x1' , 'x2' ] # Generate initial data n_initial = 5 initial_x = np . random . uniform ( bounds [:, 0 ], bounds [:, 1 ], size = ( n_initial , len ( variable_names ))) initial_y = objective_function ( initial_x ) initial_data = pd . DataFrame ( initial_x , columns = variable_names ) initial_data [ 'y' ] = initial_y # Configure EGO config = TrainingConfig ( max_iterations = 20 , rmse_threshold = 0.001 , acquisition_name = \"eigf\" ) # Run EGO ego = EfficientGlobalOptimization ( objective_func = objective_function , bounds = bounds , variable_names = variable_names , config = config , initial_data = initial_data ) result = ego . run () # Output the optimization history print ( result . history )","title":"Basic Usage Examples"},{"location":"user-guide/ego/usageego/#advanced-usage","text":"Example 2: Optimizing with Preloaded Data import json import pandas as pd from PyEGRO.ego import EfficientGlobalOptimization , TrainingConfig # Load pre-existing data with open ( 'data_info.json' , 'r' ) as f : data_info = json . load ( f ) initial_data = pd . read_csv ( 'training_data.csv' ) # Extract bounds and variable names from data info bounds = np . array ( data_info [ 'input_bound' ]) variable_names = [ var [ 'name' ] for var in data_info [ 'variables' ]] # Define the objective function # Define the objective function def objective_function ( x ): x1 , x2 = x [:, 0 ], x [:, 1 ] y = x1 ** 2 + x2 ** 2 return y # Configure EGO config = TrainingConfig ( max_iterations = 50 , rmse_threshold = 0.0001 , acquisition_name = \"eigf\" ) # Run EGO ego = EfficientGlobalOptimization ( objective_func = objective_function , bounds = bounds , variable_names = variable_names , config = config , initial_data = initial_data ) result = ego . run () # Save the results result . save ( \"output_directory\" ) Configuration Options TrainingConfig() Parameters : max_iterations : Maximum number of optimization iterations (default: 100). rmse_threshold : Stopping criteria based on RMSE improvement (default: 0.001). acquisition_name : Choice of acquisition function (e.g., \"ei\", \"e3i\", \"eigf\"). device : Specifies whether to use \"cpu\" or \"cuda\" (default: \"cpu\"). save_dir : Directory to save the results and trained model. verbose : Displays progress during the optimization (default: True). This document provides examples for using the EGO module in PyEGRO to solve complex optimization problems, preloading data, and loading trained models for predictions. For further assistance, refer to the PyEGRO documentation or contact support.","title":"Advanced Usage"},{"location":"user-guide/meta/overviewmeta/","text":"Surrogate Modeling / Overview \u00b6 Surrogate modeling is a powerful tool used to approximate response of complex systems with represent mathematical models. Gaussian Process Regression (GPR) \u00b6 The PyEGRO library supports Gaussian Process Regression (GPR) , which is ideal for capturing nonlinear relationships while providing model uncertainty estimation. Built on top of GPyTorch which support using GPU computing, this library simplifies the construction of GPR models and is designed for seamless integration within the PyEGRO modular framework. Key Features \u00b6 Gaussian Process Regression (GPR): Supports Mat\u00e9rn Kernel with Automatic Relevance Determination (ARD). Provides posterior mean and variance for predictions. Device-agnostic implementation (CPU/GPU support). Device Management: Automatically selects CPU or GPU based on system availability. Optimized training and evaluation for hardware capabilities. Model Evaluation: Scalable evaluation of predictions with batch processing. Automatic scaling of inputs and outputs for stable training. Progress Tracking: Rich progress bars for training loops. Early stopping with customizable patience levels. Model Saving: Enable to save trained model allow importing for prediction. Basics Formulation \u00b6 Gaussian Process Regression: Given training data \\( (X, y) \\) , a GPR model assumes: \\[ y(X) \\sim \\mathcal{GP}(m(X), k(X, X')) \\] where: - \\( m(X) \\) : Mean function (constant by default). - \\( k(X, X') \\) : Covariance function (Mat\u00e9rn kernel). Mat\u00e9rn Kernel with ARD: The covariance function for inputs \\( X \\) and \\( X' \\) is: \\[ k(X, X') = \\sigma^2 \\left(1 + \\frac{\\sqrt{5} r}{l} + \\frac{5r^2}{3l^2}\\right) \\exp\\left(-\\frac{\\sqrt{5} r}{l}\\right) \\] where: - \\( r = \\|X - X'\\| \\) : Euclidean distance. - \\( l \\) : Lengthscale parameter (ARD for input dimensions). - \\( \\sigma^2 \\) : Signal variance. Training Objective: The model minimizes the negative marginal log likelihood (NMLL): \\[ \\mathcal{L} = -\\log p(y \\mid X) \\] Response and prediction variance estimation: Predictions provide both mean and variance: \\[ \\hat{y} = m(X) + k(X, X')k(X', X')^{-1}(y - m(X')) \\] \\[ \\sigma^2(X) = k(X, X) - k(X, X')k(X', X')^{-1}k(X', X) \\]","title":"Surrogate Modeling / Overview"},{"location":"user-guide/meta/overviewmeta/#surrogate-modeling-overview","text":"Surrogate modeling is a powerful tool used to approximate response of complex systems with represent mathematical models.","title":"Surrogate Modeling / Overview"},{"location":"user-guide/meta/overviewmeta/#gaussian-process-regression-gpr","text":"The PyEGRO library supports Gaussian Process Regression (GPR) , which is ideal for capturing nonlinear relationships while providing model uncertainty estimation. Built on top of GPyTorch which support using GPU computing, this library simplifies the construction of GPR models and is designed for seamless integration within the PyEGRO modular framework.","title":"Gaussian Process Regression (GPR)"},{"location":"user-guide/meta/overviewmeta/#key-features","text":"Gaussian Process Regression (GPR): Supports Mat\u00e9rn Kernel with Automatic Relevance Determination (ARD). Provides posterior mean and variance for predictions. Device-agnostic implementation (CPU/GPU support). Device Management: Automatically selects CPU or GPU based on system availability. Optimized training and evaluation for hardware capabilities. Model Evaluation: Scalable evaluation of predictions with batch processing. Automatic scaling of inputs and outputs for stable training. Progress Tracking: Rich progress bars for training loops. Early stopping with customizable patience levels. Model Saving: Enable to save trained model allow importing for prediction.","title":"Key Features"},{"location":"user-guide/meta/overviewmeta/#basics-formulation","text":"Gaussian Process Regression: Given training data \\( (X, y) \\) , a GPR model assumes: \\[ y(X) \\sim \\mathcal{GP}(m(X), k(X, X')) \\] where: - \\( m(X) \\) : Mean function (constant by default). - \\( k(X, X') \\) : Covariance function (Mat\u00e9rn kernel). Mat\u00e9rn Kernel with ARD: The covariance function for inputs \\( X \\) and \\( X' \\) is: \\[ k(X, X') = \\sigma^2 \\left(1 + \\frac{\\sqrt{5} r}{l} + \\frac{5r^2}{3l^2}\\right) \\exp\\left(-\\frac{\\sqrt{5} r}{l}\\right) \\] where: - \\( r = \\|X - X'\\| \\) : Euclidean distance. - \\( l \\) : Lengthscale parameter (ARD for input dimensions). - \\( \\sigma^2 \\) : Signal variance. Training Objective: The model minimizes the negative marginal log likelihood (NMLL): \\[ \\mathcal{L} = -\\log p(y \\mid X) \\] Response and prediction variance estimation: Predictions provide both mean and variance: \\[ \\hat{y} = m(X) + k(X, X')k(X', X')^{-1}(y - m(X')) \\] \\[ \\sigma^2(X) = k(X, X) - k(X, X')k(X', X')^{-1}k(X', X) \\]","title":"Basics Formulation"},{"location":"user-guide/meta/usagemeta/","text":"Surrogate Modeling / Usage \u00b6 Basic Usage \u00b6 Assume that the initial design data was created first, so training and information files are required. Step 1: Initialize the Trainer from PyEGRO.meta_trainer import MetaTraining # Initialize with default settings trainer = MetaTraining () # Alternatively, you can specify files explicitly trainer = MetaTraining ( data_info_file = 'data_info.json' , data_training_file = 'training_data.csv' ) Step 2: Train the Model # Train the model using default training data model , scaler_X , scaler_y = trainer . train () Step 3: Make Predictions # Input data for prediction X_new = [[ 1.2 , 3.4 , 5.6 ]] # Predict mean and standard deviation mean , std = trainer . predict ( X_new ) print ( f \"Predicted mean: { mean } , std: { std } \" ) Advanced Usage \u00b6 Using Custom Data # Custom training data import numpy as np def objective_function ( x ): return x * np . sin ( x ) # Generate custom data X = np . linspace ( 0 , 10 , 100 ) . reshape ( - 1 , 1 ) y = objective_function ( X ) . flatten () # Train with custom data model , scaler_X , scaler_y = trainer . train ( X , y , custom_data = True ) Load Model from File for Prediction # Load a saved model for making predictions from PyEGRO.meta_trainer import MetaTraining # Initialize the trainer trainer = MetaTraining () # Load the saved model and scalers trainer . load_model ( model_dir = 'RESULT_MODEL_GPR' ) # Make predictions using the loaded model X_new = [[ 2.5 ], [ 3.0 ], [ 4.5 ]] mean , std = trainer . predict ( X_new ) print ( f \"Predicted means: { mean . flatten () } \" ) print ( f \"Uncertainties: { std . flatten () } \" ) Configuration Options of MetaTraining MetaTraining Parameters: test_size : Train/test split ratio (default: 0.3 ) num_iterations : Maximum training iterations (default: 100 ) prefer_gpu : Use GPU when available (default: True ) show_progress : Display progress bar (default: True ) show_hardware_info : Show system details (default: True ) show_model_info : Display model architecture (default: True ) output_dir : Results directory (default: 'RESULT_MODEL_GPR' ) data_dir : Input data directory (default: 'DATA_PREPARATION' ) data_info_file : Path to data info JSON file (default: None ) data_training_file : Path to training data CSV file (default: None ) Full details can be displayed using the following command: # Import the library and print usage import PyEGRO.meta_trainer PyEGRO . meta_trainer . print_usage () Full Configuration Usage # Import the library and print usage import PyEGRO.meta_trainer import MetaTraining # Initialize the trainer with default configuration trainer = MetaTraining () # Alternatively, specify files explicitly trainer = MetaTraining ( data_info_file = 'data_info.json' , data_training_file = 'training_data.csv' ) # Train and evaluate with default data model , scaler_X , scaler_y = trainer . train () # Make predictions with a new input X_new = [[ 2.0 ]] mean , std = trainer . predict ( X_new ) print ( f \"Predicted mean: { mean } , std: { std } \" ) Output Files Saved Model: Model parameters and scalers are saved in the RESULT_MODEL_GPR directory. Files: gpr_model.pth , scaler_X.pkl , scaler_y.pkl Performance Plots: Training and testing performance plots are saved as model_performance.png .","title":"Surrogate Modeling / Usage"},{"location":"user-guide/meta/usagemeta/#surrogate-modeling-usage","text":"","title":"Surrogate Modeling / Usage"},{"location":"user-guide/meta/usagemeta/#basic-usage","text":"Assume that the initial design data was created first, so training and information files are required. Step 1: Initialize the Trainer from PyEGRO.meta_trainer import MetaTraining # Initialize with default settings trainer = MetaTraining () # Alternatively, you can specify files explicitly trainer = MetaTraining ( data_info_file = 'data_info.json' , data_training_file = 'training_data.csv' ) Step 2: Train the Model # Train the model using default training data model , scaler_X , scaler_y = trainer . train () Step 3: Make Predictions # Input data for prediction X_new = [[ 1.2 , 3.4 , 5.6 ]] # Predict mean and standard deviation mean , std = trainer . predict ( X_new ) print ( f \"Predicted mean: { mean } , std: { std } \" )","title":"Basic Usage"},{"location":"user-guide/meta/usagemeta/#advanced-usage","text":"Using Custom Data # Custom training data import numpy as np def objective_function ( x ): return x * np . sin ( x ) # Generate custom data X = np . linspace ( 0 , 10 , 100 ) . reshape ( - 1 , 1 ) y = objective_function ( X ) . flatten () # Train with custom data model , scaler_X , scaler_y = trainer . train ( X , y , custom_data = True ) Load Model from File for Prediction # Load a saved model for making predictions from PyEGRO.meta_trainer import MetaTraining # Initialize the trainer trainer = MetaTraining () # Load the saved model and scalers trainer . load_model ( model_dir = 'RESULT_MODEL_GPR' ) # Make predictions using the loaded model X_new = [[ 2.5 ], [ 3.0 ], [ 4.5 ]] mean , std = trainer . predict ( X_new ) print ( f \"Predicted means: { mean . flatten () } \" ) print ( f \"Uncertainties: { std . flatten () } \" ) Configuration Options of MetaTraining MetaTraining Parameters: test_size : Train/test split ratio (default: 0.3 ) num_iterations : Maximum training iterations (default: 100 ) prefer_gpu : Use GPU when available (default: True ) show_progress : Display progress bar (default: True ) show_hardware_info : Show system details (default: True ) show_model_info : Display model architecture (default: True ) output_dir : Results directory (default: 'RESULT_MODEL_GPR' ) data_dir : Input data directory (default: 'DATA_PREPARATION' ) data_info_file : Path to data info JSON file (default: None ) data_training_file : Path to training data CSV file (default: None ) Full details can be displayed using the following command: # Import the library and print usage import PyEGRO.meta_trainer PyEGRO . meta_trainer . print_usage () Full Configuration Usage # Import the library and print usage import PyEGRO.meta_trainer import MetaTraining # Initialize the trainer with default configuration trainer = MetaTraining () # Alternatively, specify files explicitly trainer = MetaTraining ( data_info_file = 'data_info.json' , data_training_file = 'training_data.csv' ) # Train and evaluate with default data model , scaler_X , scaler_y = trainer . train () # Make predictions with a new input X_new = [[ 2.0 ]] mean , std = trainer . predict ( X_new ) print ( f \"Predicted mean: { mean } , std: { std } \" ) Output Files Saved Model: Model parameters and scalers are saved in the RESULT_MODEL_GPR directory. Files: gpr_model.pth , scaler_X.pkl , scaler_y.pkl Performance Plots: Training and testing performance plots are saved as model_performance.png .","title":"Advanced Usage"},{"location":"user-guide/robustopt/overviewrobust/","text":"","title":"Overviewrobust"},{"location":"user-guide/robustopt/usagerobust/","text":"","title":"Usagerobust"},{"location":"user-guide/sensitivity/overviewsensitivity/","text":"","title":"Overviewsensitivity"},{"location":"user-guide/sensitivity/usagesensitivity/","text":"","title":"Usagesensitivity"},{"location":"user-guide/uq/overviewuq/","text":"","title":"Overviewuq"},{"location":"user-guide/uq/usageuq/","text":"","title":"Usageuq"}]}