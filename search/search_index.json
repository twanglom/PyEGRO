{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyegro-python-efficient-global-robust-optimization","title":"PyEGRO: Python Efficient Global Robust Optimization","text":""},{"location":"#overview","title":"Overview","text":"<p>PyEGRO is a Python library designed for solving complex engineering problems with efficient global robust optimization. It provides tools for initial design sampling, surrogate modeling, sensitivity analysis, and robust optimization. </p>"},{"location":"#why-choose-pyegro","title":"Why Choose PyEGRO?","text":"<ul> <li>Comprehensive Tools: Includes modules for every stage of the optimization process.</li> <li>Customizable: Adapts to diverse engineering and scientific needs.</li> <li>User-Friendly: Simple API with clear documentation.</li> <li>Performance-Oriented: Supports GPU acceleration for computationally expensive tasks.</li> </ul>"},{"location":"#key-modules","title":"\ud83d\udd11 Key Modules","text":""},{"location":"#1-design-of-experiments-doe","title":"1. Design of Experiments (DOE)","text":"<ul> <li>Advanced sampling methods:</li> <li>Latin Hypercube Sampling (LHS)</li> <li>Sobol Sequence</li> <li>Halton Sequence</li> <li>Support for:</li> <li>Design variables (deterministic) and environmental variables (stochastic).</li> <li>Multi-dimensional and complex domains.</li> <li>Customizable sampling criteria for enhanced precision.</li> </ul>"},{"location":"#2-efficient-global-optimization-ego","title":"2. Efficient Global Optimization (EGO)","text":"<ul> <li>Provides multiple acquisition functions:</li> <li>Expected Improvement (EI)</li> <li>Probability of Improvement (PI)</li> <li>Lower Confidence Bound (LCB)</li> <li>Exploration Enhanced EI (E3I) [1]</li> <li>Expected Improvement for Global Fit (EIGF) [2]</li> <li>Distance-Enhanced Gradient (CRI3) [3]</li> <li>Key Features:</li> <li>Comprehensive training configurations for surrogate models.</li> <li>Built-in visualization and performance tracking.</li> <li>Parallel processing support.</li> </ul>"},{"location":"#3-surrogate-modeling","title":"3. Surrogate Modeling","text":"<ul> <li>Supports Gaussian Process Regression (GPR) and Artificial Neural Network (ANN).</li> <li>Hyperparameter optimization using Optuna.</li> <li>Real-time progress visualization.</li> </ul>"},{"location":"#4-robust-optimization","title":"4. Robust Optimization","text":"<ul> <li>Support techniques: Monte Carlo Simulation (MCS) and Polynomial Chaos Expansion (PCE) </li> <li>Multi-objective Pareto solutions.</li> <li>Both direct function and surrogate-based evaluations supported.</li> </ul>"},{"location":"#5-sensitivity-analysis","title":"5. Sensitivity Analysis","text":"<ul> <li>Sobol indices for quantifying sensitivity.</li> <li>Seamless analysis with true or surrogate functions.</li> <li>Built-in visualization tools for insightful results.</li> </ul>"},{"location":"#6-uncertainty-quantification","title":"6. Uncertainty Quantification","text":"<ul> <li>Methods for uncertainty propagation:</li> <li>Moment estimation.</li> <li>Distribution analysis (PDF/CDF).</li> <li>Visualization of Mean and Confidence bounds on surrogate model</li> </ul>"},{"location":"#installation","title":"\ud83d\udee0 Installation","text":"<p>To install PyEGRO, use the following pip command: <pre><code>pip install PyEGRO\n</code></pre></p>"},{"location":"#documentation-structure","title":"\ud83d\udd17 Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>Learn how to install and configure PyEGRO, with quick-start examples.</p>"},{"location":"#user-guide","title":"User Guide","text":"<p>Detailed explanations of each module, including: - Design of Experiments - Efficient Global Optimization - Surrogate Modeling - Robust Optimization - Sensitivity Analysis - Uncertainty Quantification</p>"},{"location":"#application-examples","title":"Application Examples","text":"<p>Explore real-world use cases and examples for common problems solved using PyEGRO.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Comprehensive reference for all PyEGRO functions and classes.</p>"},{"location":"#community-and-contributions","title":"\ud83d\udc65 Community and Contributions","text":"<p>We welcome contributions to PyEGRO! Visit our GitHub repository to raise issues, submit pull requests, or explore the source code.</p>"},{"location":"#references","title":"References","text":"<p>1. Berk, J., Nguyen, V., Gupta, S., Rana, S., &amp; Venkatesh, S. (2019). Exploration enhanced expected improvement for Bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part II 18 (pp. 621-637). Springer International Publishing.</p> <p>2. Lam, C. Q. (2008). Sequential adaptive designs in computer experiments for response surface model fit (Doctoral dissertation, The Ohio State University).</p> <p>3. Shimoyama, K., &amp; Kawai, S. (2019). A kriging-based dynamic adaptive sampling method for uncertainty quantification. Transactions of the Japan Society for Aeronautical and Space Sciences, 62(3), 137-150.</p>"},{"location":"#license","title":"License","text":"<p>---</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>Install PyEGRO using pip:</p> <pre><code>pip install PyEGRO\n</code></pre>"},{"location":"user-guide/doe/overviewdoe/","title":"Mathematical Formulation","text":"<p>Sampling Methods</p> <ol> <li> <p>Latin Hypercube Sampling (LHS): Divides each variable's range into equal intervals and ensures one sample is drawn from each interval. The process ensures balanced coverage by sampling from:</p> \\[ x_i \\sim U\\left(\\frac{k-1}{N}, \\frac{k}{N}\\right), \\quad k = 1, \\ldots, N \\] <p>where \\(N\\) is the total number of samples.</p> </li> <li> <p>Sobol Sequence: Generates low-discrepancy sequences with uniformity, minimizing discrepancy \\(D\\):</p> \\[ D \\leq \\frac{\\log(N)^d}{N} \\] <p>where \\(d\\) is the dimensionality of the problem.</p> </li> <li> <p>Random Sampling: Random sampling draws samples \\(x\\) uniformly from the range \\([a_i, b_i]\\):</p> \\[ x_i \\sim U(a_i, b_i) \\] </li> </ol> <p>Objective Function Evaluation</p> <p>Samples \\(x\\) are evaluated using an objective function \\(f(x)\\):</p> \\[ \\text{Result} = f(x) \\] <p>This provides flexibility for custom functions during design optimization.</p>"},{"location":"user-guide/doe/usagedoe/","title":"Design of Experiment / Usage","text":"<p>This document provides examples for using the PyEGRO DOE module across different scenarios. The examples showcase variable definition, sampling methods, and saving/loading configurations.</p>"},{"location":"user-guide/doe/usagedoe/#default-settings-usage","title":"Default Settings Usage","text":"<p>Goal:</p> <p>Run the sampling process with default settings for rapid setup and testing.</p> <p>Code:</p> <pre><code>from pyegro.initial_design import InitialDesign\n\n# Create objective function \ndef objective_function(x):\n    x1, x2 = x[:, 0], x[:, 1] # format of input variable\n    y = x1**2 + x2**2 # Simple quadratic function\n    return y  \n\n# Create design with default settings\ndesign = InitialDesign(\n    sampling_method='lhs',  # Default: Latin Hypercube Sampling\n    show_progress=True\n)\n\n# Define a basic design variable\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='First variable'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.1,\n    description='Second variable'\n)\n\n# Save data to file\ndesign.save()\n\n# Run the sampling process\ndesign.run(\n    objective_function=objective_function,\n    num_samples=10  # Default number of samples\n)\n</code></pre> <p>Output:</p> <ol> <li><code>training_data.csv</code>: Contains generated samples with default settings.</li> <li>No additional configuration files are created unless explicitly saved.</li> </ol> <p></p>"},{"location":"user-guide/doe/usagedoe/#1-basic-lhs-sampling","title":"1. Basic LHS Sampling","text":"<p>Goal: Generate samples using Latin Hypercube Sampling (LHS) with two design variables.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0]**2 + x[:, 1]**2  # Simple quadratic function\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_LHS',\n    sampling_method='lhs',\n    sampling_criterion='maximin',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='First variable'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.1,\n    description='Second variable'\n)\n\n# Save configuration and run sampling\ndesign.save(\"lhs_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=50\n)\n</code></pre> Output: 1. <code>lhs_config.json</code>: Contains design configuration. 2. <code>training_data.csv</code>: Generated samples and objective function values.</p>"},{"location":"user-guide/doe/usagedoe/#2-sobol-sequence-sampling","title":"2. Sobol Sequence Sampling","text":"<p>Goal: Generate samples using the Sobol sequence for a low-discrepancy sampling approach.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return (x[:, 0] - 1)**2 + (x[:, 1] - 2)**2\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_SOBOL',\n    sampling_method='sobol',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.2,\n    description='Variable x1'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.2,\n    description='Variable x2'\n)\n\n# Save configuration and run sampling\ndesign.save(\"sobol_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=100\n)\n</code></pre> Output: 1. <code>sobol_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#3-mixed-variable-types","title":"3. Mixed Variable Types","text":"<p>Goal: Generate samples with both design and environmental variables.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0]**2 + 3 * x[:, 1]\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_MIXED',\n    sampling_method='lhs',\n    show_progress=True\n)\n\n# Define design variable\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='Design variable x1'\n)\n\n# Define environmental variable\ndesign.add_env_variable(\n    name='env1',\n    distribution='normal',\n    mean=10,\n    cov=0.2,\n    description='Environmental variable env1'\n)\n\n# Save configuration and run sampling\ndesign.save(\"mixed_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=50\n)\n</code></pre> Output: 1. <code>mixed_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#4-random-sampling","title":"4. Random Sampling","text":"<p>Goal: Generate samples using uniform random sampling.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0] * x[:, 1]\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_RANDOM',\n    sampling_method='random',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[0, 10],\n    cov=0.05,\n    description='Random variable x1'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-10, 0],\n    cov=0.05,\n    description='Random variable x2'\n)\n\n# Save configuration and run sampling\ndesign.save(\"random_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=20\n)\n</code></pre> Output: 1. <code>random_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#5-loading-and-reusing-configurations","title":"5. Loading and Reusing Configurations","text":"<p>Goal: Reuse a saved configuration to generate new samples.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0] + x[:, 1]\n\n# Load configuration\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_REUSE',\n    sampling_method='lhs'\n)\ndesign.load(\"lhs_config\")\n\n# Run with different sample count\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=100\n)\n</code></pre> Output: 1. Updated <code>training_data.csv</code></p>"}]}