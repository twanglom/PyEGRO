{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyegro-python-efficient-global-robust-optimization","title":"PyEGRO: Python Efficient Global Robust Optimization","text":""},{"location":"#overview","title":"Overview","text":"<p>PyEGRO is a Python library designed for solving complex engineering problems with efficient global robust optimization. It provides tools for initial design sampling, surrogate modeling, sensitivity analysis, and robust optimization. </p>"},{"location":"#key-modules","title":"\ud83d\udd11 Key Modules","text":""},{"location":"#1-design-of-experiments-doe","title":"1. Design of Experiments (DOE)","text":"<ul> <li>Advanced sampling methods:</li> <li>Latin Hypercube Sampling (LHS)</li> <li>Sobol Sequence</li> <li>Halton Sequence</li> <li>Design variables and environmental variables (deterministic and stochastic).</li> <li>Multi-dimensional and complex domains.</li> <li>Customizable sampling criteria for enhanced precision.</li> </ul>"},{"location":"#2-surrogate-modeling","title":"2. Surrogate Modeling","text":"<ul> <li>Supports Gaussian Process Regression (GPR) and Artificial Neural Network (ANN).</li> <li>Hyperparameter optimization using Optuna.</li> <li>Real-time progress visualization.</li> </ul>"},{"location":"#3-efficient-global-optimization-ego-for-surrogate-modeling","title":"3. Efficient Global Optimization (EGO) for Surrogate Modeling","text":"<ul> <li>Provides multiple acquisition functions:</li> <li>Expected Improvement (EI) and Boosting the Exploration term (\ud835\udf01-EI)</li> <li>Probability of Improvement (PI)</li> <li>Lower Confidence Bound (LCB)</li> <li>Exploration Enhanced EI (E3I) [1]</li> <li>Expected Improvement for Global Fit (EIGF) [2]</li> <li>Distance-Enhanced Gradient (CRI3) [3]</li> <li>Comprehensive training configurations for surrogate models.</li> <li>Built-in visualization and performance tracking.</li> </ul>"},{"location":"#4-robust-optimization","title":"4. Robust Optimization","text":"<ul> <li>Support techniques: Monte Carlo Simulation (MCS) and Polynomial Chaos Expansion (PCE) </li> <li>Multi-objective Pareto solutions.</li> <li>Both direct function and surrogate-based evaluations supported.</li> </ul>"},{"location":"#5-sensitivity-analysis","title":"5. Sensitivity Analysis","text":"<ul> <li>Sobol indices for quantifying sensitivity.</li> <li>Seamless analysis with true or surrogate functions.</li> <li>Built-in visualization tools for insightful results.</li> </ul>"},{"location":"#6-uncertainty-quantification","title":"6. Uncertainty Quantification","text":"<ul> <li>Methods for uncertainty propagation:</li> <li>Moment estimation.</li> <li>Distribution analysis (PDF/CDF).</li> <li>Visualization of Mean and Confidence bounds on surrogate model</li> </ul>"},{"location":"#installation","title":"\ud83d\udee0 Installation","text":"<p>To install PyEGRO, use the following pip command: <pre><code>pip install PyEGRO\n</code></pre></p>"},{"location":"#documentation-structure","title":"\ud83d\udd17 Documentation Structure","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>Learn how to install and configure PyEGRO, with quick-start examples.</p>"},{"location":"#user-guide","title":"User Guide","text":"<p>Detailed explanations of each module, including: - Design of Experiments - Efficient Global Optimization - Surrogate Modeling - Robust Optimization - Sensitivity Analysis - Uncertainty Quantification</p>"},{"location":"#application-examples","title":"Application Examples","text":"<p>Explore real-world use cases and examples for common problems solved using PyEGRO.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Comprehensive reference for all PyEGRO functions and classes.</p>"},{"location":"#community-and-contributions","title":"\ud83d\udc65 Community and Contributions","text":"<p>We welcome contributions to PyEGRO! Visit our GitHub repository to raise issues, submit pull requests, or explore the source code.</p>"},{"location":"#references","title":"References","text":"<p>1. Berk, J., Nguyen, V., Gupta, S., Rana, S., &amp; Venkatesh, S. (2019). Exploration enhanced expected improvement for Bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part II 18 (pp. 621-637). Springer International Publishing.</p> <p>2. Lam, C. Q. (2008). Sequential adaptive designs in computer experiments for response surface model fit (Doctoral dissertation, The Ohio State University).</p> <p>3. Shimoyama, K., &amp; Kawai, S. (2019). A kriging-based dynamic adaptive sampling method for uncertainty quantification. Transactions of the Japan Society for Aeronautical and Space Sciences, 62(3), 137-150.</p>"},{"location":"#about-the-author","title":"\ud83d\udc64 About the Author","text":"<p>Thanasak Wanglomklang PhD Student in Mechanical Engineering  </p> <ul> <li>Email: thanasak.wanglomklang@ec-lyon.fr </li> <li>Phone: +33 06 51 22 45 69  </li> <li>Location: Lyon, France  </li> <li>Personal Website: twanglom.github.io </li> </ul> <p>My current research is in optimization under uncertainty, uncertainty quantification in robust optimization schemes, and computational simulations in vibro-acoustics. My applications involve shape optimization of aircraft cabins for acoustic noise reduction. PyEGRO is a tool I am developing to solve the problems in my research. I hope this tool becomes a useful application in engineering design processes that account for uncertainty during the design phase.</p>"},{"location":"#license","title":"License","text":"<p>---</p>"},{"location":"examples/basics-surrogate-examples/basics-surrogate-examples/","title":"Basics Surrogate Model Training Examples","text":""},{"location":"examples/basics-surrogate-examples/basics-surrogate-examples/#objective-function","title":"Objective Function","text":"<p>The Gaussian-shaped test function is defined as:</p> \\[     f(X_1, X_2) = - \\left[ A_1 \\cdot \\exp \\left( - \\left( \\frac{(X_1 - \\mu_1)^2}{2 \\sigma_1^2} + \\frac{(X_2 - \\mu_2)^2}{2 \\sigma_2^2} \\right) \\right) +     A_2 \\cdot \\exp \\left( - \\left( \\frac{(X_1 - \\mu_3)^2}{2 \\sigma_3^2} + \\frac{(X_2 - \\mu_4)^2}{2 \\sigma_4^2} \\right) \\right) \\right] - 200 \\] <p>Where:</p> <ul> <li> <p>\\(A_1, A_2\\): Amplitudes of the Gaussian peaks.</p> </li> <li> <p>\\(\\mu_1, \\mu_2, \\mu_3, \\mu_4\\): Centers of the peaks.</p> </li> <li> <p>\\(\\sigma_1, \\sigma_2, \\sigma_3, \\sigma_4\\): Standard deviations controlling the spread of the peaks.</p> </li> </ul> <p>This function represents two Gaussian peaks with a global minimum and a local minimum.</p>"},{"location":"examples/basics-surrogate-examples/basics-surrogate-examples/#1-define-the-test-function","title":"1. Define the Test Function","text":"<p>To define the Gaussian-shaped test function, create a separate Python file named <code>objective.py</code>:</p> <p>File: objective.py <pre><code>import numpy as np\n\ndef true_function(x):\n    X1, X2 = x[:, 0], x[:, 1]\n    a1, x1, y1, sigma_x1, sigma_y1 = 100, 3, 2.1, 3, 3\n    a2, x2, y2, sigma_x2, sigma_y2 = 150, -1.5, -1.2, 1, 1\n    f = -(\n        a1 * np.exp(-((X1 - x1) ** 2 / (2 * sigma_x1 ** 2) + (X2 - y1) ** 2 / (2 * sigma_y1 ** 2))) +\n        a2 * np.exp(-((X1 - x2) ** 2 / (2 * sigma_x2 ** 2) + (X2 - y2) ** 2 / (2 * sigma_y2 ** 2))) - 200\n    )\n\n    return f\n</code></pre></p> <p>This function represents two Gaussian peaks, one for the global minimum and another for the local minimum.</p>"},{"location":"examples/basics-surrogate-examples/basics-surrogate-examples/#2-generate-design-samples","title":"2. Generate Design Samples","text":"<p>Steps Explained:</p> <ol> <li> <p>Define Variables: Add design variables <code>x1</code> and <code>x2</code> with their respective bounds and covariance values.</p> </li> <li> <p>Save Configuration: Save the configuration for reproducibility.</p> </li> <li> <p>Run Sampling: Generate 200 samples using the defined test function.</p> </li> </ol> <p>To generate initial design samples using LHS, create a file named <code>run_initial_design.py</code>:</p> <p>File: run_initial_design.py <pre><code>from PyEGRO.doe.initial_design import InitialDesign\nfrom objective import true_function\n\n# Create design with LHS sampling\ndesign_lhs = InitialDesign(\n    sampling_method=\"lhs\",\n    show_progress=True\n)\n\n# Add design variables\ndesign_lhs.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.2,\n    description='first design variable'\n)\n\ndesign_lhs.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.15,\n    description='second design variable'\n)\n\n# Save configuration\ndesign_lhs.save()\n\n# Run sampling\nresults_lhs = design_lhs.run(\n    objective_function=true_function,\n    num_samples=200\n)\n</code></pre></p> <p>Output</p> <ul> <li>Command line Displays</li> </ul> <p></p> <ul> <li>Save Output Directory <code>DATA_PREPARATION</code></li> </ul> <p></p>"},{"location":"examples/basics-surrogate-examples/basics-surrogate-examples/#3-train-the-surrogate-model","title":"3. Train the Surrogate Model","text":"<p>Steps Explained:</p> <ol> <li> <p>Initialize MetaTraining: Set up the training configuration and preprocessing options.</p> </li> <li> <p>Train Model: Train the Gaussian Process Regression (GPR) surrogate model using the sampled data.</p> </li> <li> <p>Save Results: Save the trained model and data scalers for future use.</p> </li> </ol> <p>Use the generated design samples to train the surrogate model. Below is an example script for surrogate model training:</p> <p>File: train_surrogate.py <pre><code>from PyEGRO.meta.meta_trainer import MetaTraining\n\n# Initialize MetaTraining\nmeta = MetaTraining()\n\n# Train surrogate model\nmodel, scaler_X, scaler_y = meta.train()\n\n# Save trained model and scalers\nmeta.save()\n</code></pre></p> <p>Output</p> <ul> <li>Command line Displays</li> </ul> <p></p> <ul> <li>Save Output Directory <code>RESULT_MODEL_GPR</code></li> </ul> <p></p> <ul> <li>Performance plot example</li> </ul> <p></p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>Install PyEGRO using pip:</p> <pre><code>pip install PyEGRO\n</code></pre>"},{"location":"user-guide/doe/overviewdoe/","title":"Design of Experiment / Overview","text":""},{"location":"user-guide/doe/overviewdoe/#sampling-methods","title":"Sampling Methods","text":"<ol> <li> <p>Latin Hypercube Sampling (LHS):    Latin Hypercube Sampling divides each variable's range into \\(N\\) equal intervals and ensures one sample is drawn from each interval without repetition. It maximizes uniformity in the sampling process:</p> \\[ x_i \\sim U\\left(\\frac{k-1}{N}, \\frac{k}{N}\\right), \\quad k = 1, \\ldots, N \\] <p>where: - \\( N \\): Total number of samples - \\( U(a, b) \\): Uniform distribution between \\( a \\) and \\( b \\)</p> </li> <li> <p>Sobol Sequence:    Sobol sequences are quasi-random low-discrepancy sequences designed to achieve uniformity in the sampling space. The discrepancy \\( D \\) measures the uniformity, which is minimized in Sobol sampling:</p> \\[ D \\leq \\frac{\\log(N)^d}{N} \\] <p>where: - \\( N \\): Total number of samples - \\( d \\): Dimensionality of the problem</p> </li> <li> <p>Random Sampling:    Random sampling draws samples \\( x \\) uniformly from the range \\( [a_i, b_i] \\):</p> \\[ x_i \\sim U(a_i, b_i) \\] </li> </ol> <p>Sampling Criteria</p> <p>Sampling criteria are additional constraints or goals applied during sampling, such as: - Maximin Criterion: Maximizes the minimum distance between samples. - Centering Criterion: Ensures that samples are centered within each interval.</p>"},{"location":"user-guide/doe/overviewdoe/#objective-function-evaluation","title":"Objective Function Evaluation","text":"<p>Once the samples \\( x \\) are generated, they are passed to an objective function \\( f(x) \\) to evaluate the desired output:</p> \\[ \\text{Result} = f(x) \\] <p>Where: - \\( x \\): Sampled input vector - \\( f(x) \\): User-defined objective function</p> <p>Performance Metrics for Sampling</p> <p>To evaluate the quality of the sampling, metrics such as space-filling, discrepancy, and variance are often used.</p>"},{"location":"user-guide/doe/usagedoe/","title":"Design of Experiment / Usage","text":"<p>This document provides examples for using the PyEGRO DOE module across different scenarios. The examples showcase variable definition, sampling methods, and saving/loading configurations.</p>"},{"location":"user-guide/doe/usagedoe/#default-settings-usage","title":"Default Settings Usage","text":"<p>Goal:</p> <p>Run the sampling process with default settings for rapid setup and testing.</p> <p>Code:</p> <pre><code>from PyEGRO.initial_design import InitialDesign\n\n# Create objective function \ndef objective_function(x):\n    x1, x2 = x[:, 0], x[:, 1] # format of input variable\n    y = x1**2 + x2**2 # Simple quadratic function\n    return y  \n\n# Create design with default settings\ndesign = InitialDesign(\n    sampling_method='lhs',  # Default: Latin Hypercube Sampling\n    show_progress=True\n)\n\n# Define a basic design variable\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='First variable'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.1,\n    description='Second variable'\n)\n\n# Save data to file\ndesign.save()\n\n# Run the sampling process\ndesign.run(\n    objective_function=objective_function,\n    num_samples=10  # Default number of samples\n)\n</code></pre> <p>Output:</p> <ol> <li><code>training_data.csv</code>: Contains generated samples with default settings.</li> <li>No additional configuration files are created unless explicitly saved.</li> </ol> <p></p>"},{"location":"user-guide/doe/usagedoe/#1-basic-lhs-sampling","title":"1. Basic LHS Sampling","text":"<p>Goal: Generate samples using Latin Hypercube Sampling (LHS) with two design variables.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0]**2 + x[:, 1]**2  # Simple quadratic function\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_LHS',\n    sampling_method='lhs',\n    sampling_criterion='maximin',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='First variable'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.1,\n    description='Second variable'\n)\n\n# Save configuration and run sampling\ndesign.save(\"lhs_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=50\n)\n</code></pre> Output: 1. <code>lhs_config.json</code>: Contains design configuration. 2. <code>training_data.csv</code>: Generated samples and objective function values.</p>"},{"location":"user-guide/doe/usagedoe/#2-sobol-sequence-sampling","title":"2. Sobol Sequence Sampling","text":"<p>Goal: Generate samples using the Sobol sequence for a low-discrepancy sampling approach.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return (x[:, 0] - 1)**2 + (x[:, 1] - 2)**2\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_SOBOL',\n    sampling_method='sobol',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.2,\n    description='Variable x1'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.2,\n    description='Variable x2'\n)\n\n# Save configuration and run sampling\ndesign.save(\"sobol_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=100\n)\n</code></pre> Output: 1. <code>sobol_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#3-mixed-variable-types","title":"3. Mixed Variable Types","text":"<p>Goal: Generate samples with both design and environmental variables.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0]**2 + 3 * x[:, 1]\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_MIXED',\n    sampling_method='lhs',\n    show_progress=True\n)\n\n# Define design variable\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='Design variable x1'\n)\n\n# Define environmental variable\ndesign.add_env_variable(\n    name='env1',\n    distribution='normal',\n    mean=10,\n    cov=0.2,\n    description='Environmental variable env1'\n)\n\n# Save configuration and run sampling\ndesign.save(\"mixed_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=50\n)\n</code></pre> Output: 1. <code>mixed_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#4-random-sampling","title":"4. Random Sampling","text":"<p>Goal: Generate samples using uniform random sampling.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0] * x[:, 1]\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_RANDOM',\n    sampling_method='random',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[0, 10],\n    cov=0.05,\n    description='Random variable x1'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-10, 0],\n    cov=0.05,\n    description='Random variable x2'\n)\n\n# Save configuration and run sampling\ndesign.save(\"random_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=20\n)\n</code></pre> Output: 1. <code>random_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#5-loading-and-reusing-configurations","title":"5. Loading and Reusing Configurations","text":"<p>Goal: Reuse a saved configuration to generate new samples.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0] + x[:, 1]\n\n# Load configuration\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_REUSE',\n    sampling_method='lhs'\n)\ndesign.load(\"lhs_config\")\n\n# Run with different sample count\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=100\n)\n</code></pre> Output: 1. Updated <code>training_data.csv</code></p>"},{"location":"user-guide/ego/overviewego/","title":"Efficient Global Optimization (EGO) / Overview","text":"<p>Applications for enhancing Global Model Accuracy</p> <p>Efficient Global Optimization (EGO) is widely applied to improve the accuracy of global models for predictive tasks across the entire input domain. By leveraging surrogate models, such as Gaussian Processes (GP), EGO provides high-fidelity approximations of complex systems, ensuring accurate predictions for areas that are underexplored or uncertain.</p>"},{"location":"user-guide/ego/overviewego/#key-features","title":"Key Features","text":"<p>Surrogate Modeling:</p> <ul> <li>Uses Gaussian Processes Regression (GPR) to approximate expensive-to-evaluate objective functions.</li> </ul> <p>Acquisition Functions</p> <ul> <li>Expected Improvement (EI)</li> <li>\\(\\zeta\\)-Expected Improvement (\\(\\zeta\\)-EI)</li> <li>Exploration Enhanced EI (E3I)</li> <li>Expected Improvement for Global Fit (EIGF)</li> <li>Distance-Enhanced Gradient (CRI3)</li> </ul> <p>Optimization Configuration</p> <ul> <li>Maximum iterations and stopping criteria</li> <li>RMSE thresholds and patience settings</li> <li>Relative improvement thresholds</li> <li>Flexible device selection (CPU/GPU)</li> </ul> <p>Visualization Tools</p> <ul> <li>1D and 2D optimization problem visualizations</li> <li>Convergence plots</li> <li>Uncertainty analysis and parameter tracking</li> </ul> <p>Progress Tracking</p> <ul> <li>Detailed console outputs</li> <li>Metrics tracking for each iteration</li> <li>Integration with rich progress bars for an interactive experience</li> </ul>"},{"location":"user-guide/ego/overviewego/#acquisition-functions","title":"Acquisition Functions","text":"<p>Acquisition functions guide the optimization process. Below are the formulations:</p> <p>Expected Improvement (EI)</p> \\[ EI(x) = \\begin{cases}  \\sigma(x)\\phi(Z) + (y_{\\text{min}} - \\mu(x))\\Phi(Z) &amp; \\text{if } \\sigma(x) &gt; 0 \\\\ 0 &amp; \\text{if } \\sigma(x) = 0  \\end{cases} \\] \\[ Z = \\frac{y_{\\text{min}} - \\mu(x)}{\\sigma(x)} \\] <p>where \\(\\phi(Z)\\) is the Probability Density Function (PDF) and \\(\\Phi(Z)\\) is the Cumulative Density Function (CDF).</p> <p>\\(\\zeta\\)-Expected Improvement (\\(\\zeta\\)-EI)</p> \\[ \\zeta-EI(x) = \\begin{cases}  \\sigma(x)\\phi(Z) + (y_{\\text{min}} - \\mu(x) - \\zeta)\\Phi(Z) &amp; \\text{if } \\sigma(x) &gt; 0 \\\\ 0 &amp; \\text{if } \\sigma(x) = 0  \\end{cases} \\] \\[ Z = \\frac{y_{\\text{min}} - \\mu(x) - \\zeta}{\\sigma(x)} \\] <p>Exploration Enhanced EI (E3I) [ 1 ]</p> \\[ \\alpha^{E3I}(x) = \\frac{1}{M}\\mathbb{E}_x\\left[I(x, g_m)\\right] = \\begin{cases}  \\frac{\\sigma(x)}{M}\\sum_{m=1}^{M}\\tau(Z) &amp; \\text{if } \\sigma(x) &gt; 0 \\\\ 0 &amp; \\text{if } \\sigma(x) = 0 \\end{cases} \\] \\[ \\tau(Z) = Z\\Phi(Z) + \\phi(Z), \\quad Z = \\frac{y_{\\text{min}} - g_m(x)}{\\sigma(x)} \\] <p>where \\(g_m\\) is the maximum value from \\(M\\) Thompson samples.</p> <p>Expected Improvement for Global Fit (EIGF) [ 2 ]</p> \\[ EIGF(x) = (\\hat{y}(x) - y(x_{\\text{nearest}}))^2 + \\sigma^2(x) \\] <p>where: - \\(\\hat{y}(x)\\): Predicted mean at point \\(x\\). - \\(y(x_{\\text{nearest}})\\): Observed output at the nearest sampled point to \\(x\\). - \\(\\sigma^2(x)\\): Variance of the prediction at \\(x\\).</p> <p>Distance-Enhanced Gradient (CRI3) [ 3 ]</p> \\[ \\text{Crit}(\\xi) = (\\left|\\frac{\\partial \\hat{f}(\\xi)}{\\partial \\xi}\\right| \\Delta(\\xi) + D_f(\\xi)) \\hat{\\sigma}(\\xi) PDF(\\xi). \\] <p>where:</p> <ul> <li> <p>\\(\\Delta(\\xi) = \\min_{i=1,2,...,N}|\\xi - \\xi^{(i)}|\\): Distance to the nearest sample.</p> </li> <li> <p>\\(D_f(\\xi) = |\\hat{f}(\\xi|\\theta) - \\hat{f}(\\xi|2\\theta)|\\): Polynomial error estimate.</p> </li> <li> <p>\\(\\hat{\\sigma}(\\xi)PDF(\\xi)\\): Predictive variance multiplied by the PDF of the input space.</p> </li> </ul>"},{"location":"user-guide/ego/overviewego/#output-and-files","title":"Output and Files","text":"<p>Each EGO run produces:</p> <ol> <li> <p>Optimization History: Tracks iteration details, objective values, and model parameters.</p> </li> <li> <p>Trained Models: Stores the final GP model and associated hyperparameters.</p> </li> <li> <p>Visualization Plots: For tracking optimization progress and uncertainty.</p> </li> </ol> <p>References</p> <p>1. Berk, J., Nguyen, V., Gupta, S., Rana, S., &amp; Venkatesh, S. (2019). Exploration enhanced expected improvement for Bayesian optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10\u201314, 2018, Proceedings, Part II 18 (pp. 621-637). Springer International Publishing.</p> <p>2. Lam, C. Q. (2008). Sequential adaptive designs in computer experiments for response surface model fit (Doctoral dissertation, The Ohio State University).</p> <p>3. Shimoyama, K., &amp; Kawai, S. (2019). A kriging-based dynamic adaptive sampling method for uncertainty quantification. Transactions of the Japan Society for Aeronautical and Space Sciences, 62(3), 137-150.</p>"},{"location":"user-guide/ego/overviewego/#algorithm-comparison","title":"Algorithm Comparison","text":""},{"location":"user-guide/ego/usageego/","title":"Efficient Global Optimization (EGO) / Usage","text":""},{"location":"user-guide/ego/usageego/#basic-usage-examples","title":"Basic Usage Examples","text":"<p>Example 1: Optimizing a Quadratic Function</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom PyEGRO.ego import EfficientGlobalOptimization, TrainingConfig\n\n# Define the objective function\ndef objective_function(x):\n    x1, x2 = x[:,0], x[:,1]\n    y = x1**2 + x2**2\n    return y\n\n# Set bounds and variable names\nbounds = np.array([[-5, 5], [-5, 5]])\nvariable_names = ['x1', 'x2']\n\n# Generate initial data\nn_initial = 5\ninitial_x = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_initial, len(variable_names)))\ninitial_y = objective_function(initial_x)\ninitial_data = pd.DataFrame(initial_x, columns=variable_names)\ninitial_data['y'] = initial_y\n\n# Configure EGO\nconfig = TrainingConfig(\n    max_iterations=20,\n    rmse_threshold=0.001,\n    acquisition_name=\"eigf\"\n)\n\n# Run EGO\nego = EfficientGlobalOptimization(\n    objective_func=objective_function,\n    bounds=bounds,\n    variable_names=variable_names,\n    config=config,\n    initial_data=initial_data\n)\nresult = ego.run()\n\n# Output the optimization history\nprint(result.history)\n</code></pre>"},{"location":"user-guide/ego/usageego/#advanced-usage","title":"Advanced Usage","text":"<p>Example 2: Optimizing with Preloaded Data</p> <pre><code>import json\nimport pandas as pd\nfrom PyEGRO.ego import EfficientGlobalOptimization, TrainingConfig\n\n# Load pre-existing data\nwith open('data_info.json', 'r') as f:\n    data_info = json.load(f)\ninitial_data = pd.read_csv('training_data.csv')\n\n# Extract bounds and variable names from data info\nbounds = np.array(data_info['input_bound'])\nvariable_names = [var['name'] for var in data_info['variables']]\n\n# Define the objective function\n# Define the objective function\ndef objective_function(x):\n    x1, x2 = x[:,0], x[:,1]\n    y = x1**2 + x2**2\n    return y\n\n# Configure EGO\nconfig = TrainingConfig(\n    max_iterations=50,\n    rmse_threshold=0.0001,\n    acquisition_name=\"eigf\"\n)\n\n# Run EGO\nego = EfficientGlobalOptimization(\n    objective_func=objective_function,\n    bounds=bounds,\n    variable_names=variable_names,\n    config=config,\n    initial_data=initial_data\n)\nresult = ego.run()\n\n# Save the results\nresult.save(\"output_directory\")\n</code></pre>"},{"location":"user-guide/ego/usageego/#loading-a-model-for-prediction","title":"Loading a Model for Prediction","text":"<p>EGO allows loading previously trained models for making predictions:</p> <pre><code>from pyegro.ego import EfficientGlobalOptimization\n\n# Load the trained model\nmodel = EfficientGlobalOptimization.load_model(\"output_directory/trained_model.pth\")\n\n# Define new input points for prediction\nnew_points = np.array([[1.5, -1.2], [0.0, 0.0], [-3.3, 2.7]])\n\n# Make predictions\npredictions = model.predict(new_points)\nprint(predictions)\n</code></pre> <p>Configuration Options</p> <p>TrainingConfig() Parameters:</p> <ul> <li> <p>max_iterations: Maximum number of optimization iterations (default: 100).</p> </li> <li> <p>rmse_threshold: Stopping criteria based on RMSE improvement (default: 0.001).</p> </li> <li> <p>acquisition_name: Choice of acquisition function (e.g., \"ei\", \"e3i\", \"eigf\").</p> </li> <li> <p>device: Specifies whether to use \"cpu\" or \"cuda\" (default: \"cpu\").</p> </li> <li> <p>save_dir: Directory to save the results and trained model.</p> </li> <li> <p>verbose: Displays progress during the optimization (default: True).</p> </li> </ul> <p>This document provides examples for using the EGO module in PyEGRO to solve complex optimization problems, preloading data, and loading trained models for predictions. For further assistance, refer to the PyEGRO documentation or contact support.</p>"},{"location":"user-guide/meta/overviewmeta/","title":"Surrogate Modeling / Overview","text":"<p>Surrogate modeling is a powerful tool used to approximate response of complex systems with represent mathematical models.</p>"},{"location":"user-guide/meta/overviewmeta/#gaussian-process-regression-gpr","title":"Gaussian Process Regression (GPR)","text":"<p>The PyEGRO library supports Gaussian Process Regression (GPR), which is ideal for capturing nonlinear relationships while providing model uncertainty estimation. Built on top of GPyTorch which support using GPU computing, this library simplifies the construction of GPR models and is designed for seamless integration within the PyEGRO modular framework.</p>"},{"location":"user-guide/meta/overviewmeta/#key-features","title":"Key Features","text":"<p>Gaussian Process Regression (GPR):</p> <ul> <li>Supports Mat\u00e9rn Kernel with Automatic Relevance Determination (ARD).</li> <li>Provides posterior mean and variance for predictions.</li> <li>Device-agnostic implementation (CPU/GPU support).</li> </ul> <p>Device Management:</p> <ul> <li>Automatically selects CPU or GPU based on system availability.</li> <li>Optimized training and evaluation for hardware capabilities.</li> </ul> <p>Model Evaluation:</p> <ul> <li>Scalable evaluation of predictions with batch processing.</li> <li>Automatic scaling of inputs and outputs for stable training.</li> </ul> <p>Progress Tracking:</p> <ul> <li>Rich progress bars for training loops.</li> <li>Early stopping with customizable patience levels.</li> </ul> <p>Model Saving:</p> <ul> <li>Enable to save trained model allow importing for prediction.</li> </ul>"},{"location":"user-guide/meta/overviewmeta/#basics-formulation","title":"Basics Formulation","text":"<p>Gaussian Process Regression:</p> <p>Given training data \\( (X, y) \\), a GPR model assumes:</p> \\[ y(X) \\sim \\mathcal{GP}(m(X), k(X, X')) \\] <p>where: - \\( m(X) \\): Mean function (constant by default). - \\( k(X, X') \\): Covariance function (Mat\u00e9rn kernel).</p> <p>Mat\u00e9rn Kernel with ARD:</p> <p>The covariance function for inputs \\( X \\) and \\( X' \\) is:</p> \\[ k(X, X') = \\sigma^2 \\left(1 + \\frac{\\sqrt{5} r}{l} + \\frac{5r^2}{3l^2}\\right) \\exp\\left(-\\frac{\\sqrt{5} r}{l}\\right) \\] <p>where: - \\( r = \\|X - X'\\| \\): Euclidean distance. - \\( l \\): Lengthscale parameter (ARD for input dimensions). - \\( \\sigma^2 \\): Signal variance.</p> <p>Training Objective:</p> <p>The model minimizes the negative marginal log likelihood (NMLL):</p> \\[ \\mathcal{L} = -\\log p(y \\mid X) \\] <p>Response and prediction variance estimation:</p> <p>Predictions provide both mean and variance:</p> \\[ \\hat{y} = m(X) + k(X, X')k(X', X')^{-1}(y - m(X')) \\] \\[ \\sigma^2(X) = k(X, X) - k(X, X')k(X', X')^{-1}k(X', X) \\]"},{"location":"user-guide/meta/usagemeta/","title":"Surrogate Modeling / Usage","text":""},{"location":"user-guide/meta/usagemeta/#basic-usage","title":"Basic Usage","text":"<p>Assume that the initial design data was created first, so training and information files are required.</p> <p></p> <p>Step 1: Initialize the Trainer</p> <pre><code>from PyEGRO.meta_trainer import MetaTraining\n\n# Initialize with default settings\ntrainer = MetaTraining()\n\n# Alternatively, you can specify files explicitly\ntrainer = MetaTraining(\n    data_info_file='data_info.json',\n    data_training_file='training_data.csv'\n)\n</code></pre> <p>Step 2: Train the Model</p> <pre><code># Train the model using default training data\nmodel, scaler_X, scaler_y = trainer.train()\n</code></pre> <p>Step 3: Make Predictions</p> <pre><code># Input data for prediction\nX_new = [[1.2, 3.4, 5.6]]\n\n# Predict mean and standard deviation\nmean, std = trainer.predict(X_new)\nprint(f\"Predicted mean: {mean}, std: {std}\")\n</code></pre>"},{"location":"user-guide/meta/usagemeta/#advanced-usage","title":"Advanced Usage","text":"<p>Using Custom Data</p> <pre><code># Custom training data\nimport numpy as np\n\ndef objective_function(x):\n    return x * np.sin(x)\n\n# Generate custom data\nX = np.linspace(0, 10, 100).reshape(-1, 1)\ny = objective_function(X).flatten()\n\n# Train with custom data\nmodel, scaler_X, scaler_y = trainer.train(X, y, custom_data=True)\n</code></pre> <p>Load Model from File for Prediction</p> <pre><code># Load a saved model for making predictions\nfrom PyEGRO.meta_trainer import MetaTraining\n\n# Initialize the trainer\ntrainer = MetaTraining()\n\n# Load the saved model and scalers\ntrainer.load_model(model_dir='RESULT_MODEL_GPR')\n\n# Make predictions using the loaded model\nX_new = [[2.5], [3.0], [4.5]]\nmean, std = trainer.predict(X_new)\nprint(f\"Predicted means: {mean.flatten()}\")\nprint(f\"Uncertainties: {std.flatten()}\")\n</code></pre> <p>Configuration Options of MetaTraining</p> <p>MetaTraining Parameters:</p> <ul> <li>test_size: Train/test split ratio (default: <code>0.3</code>)</li> <li>num_iterations: Maximum training iterations (default: <code>100</code>)</li> <li>prefer_gpu: Use GPU when available (default: <code>True</code>)</li> <li>show_progress: Display progress bar (default: <code>True</code>)</li> <li>show_hardware_info: Show system details (default: <code>True</code>)</li> <li>show_model_info: Display model architecture (default: <code>True</code>)</li> <li>output_dir: Results directory (default: <code>'RESULT_MODEL_GPR'</code>)</li> <li>data_dir: Input data directory (default: <code>'DATA_PREPARATION'</code>)</li> <li>data_info_file: Path to data info JSON file (default: <code>None</code>)</li> <li>data_training_file: Path to training data CSV file (default: <code>None</code>)</li> </ul> <p>Full details can be displayed using the following command:</p> <pre><code># Import the library and print usage\nimport PyEGRO.meta_trainer\n\nPyEGRO.meta_trainer.print_usage()\n</code></pre> <p>Full Configuration Usage</p> <pre><code># Import the library and print usage\nimport PyEGRO.meta_trainer import MetaTraining\n\n# Initialize the trainer with default configuration\ntrainer = MetaTraining()\n\n# Alternatively, specify files explicitly\ntrainer = MetaTraining(\n    data_info_file='data_info.json',\n    data_training_file='training_data.csv'\n)\n\n# Train and evaluate with default data\nmodel, scaler_X, scaler_y = trainer.train()\n\n# Make predictions with a new input\nX_new = [[2.0]]\nmean, std = trainer.predict(X_new)\nprint(f\"Predicted mean: {mean}, std: {std}\")\n</code></pre> <p>Output Files</p> <p>Saved Model:</p> <ul> <li>Model parameters and scalers are saved in the <code>RESULT_MODEL_GPR</code> directory.</li> <li>Files: <code>gpr_model.pth</code>, <code>scaler_X.pkl</code>, <code>scaler_y.pkl</code></li> </ul> <p>Performance Plots:</p> <ul> <li>Training and testing performance plots are saved as <code>model_performance.png</code>.</li> </ul>"}]}