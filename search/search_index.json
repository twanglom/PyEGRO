{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pyegro-python-efficient-global-robust-optimization","title":"PyEGRO: Python Efficient Global Robust Optimization","text":""},{"location":"#overview","title":"Overview","text":"<p>PyEGRO is a Python library designed for solving complex engineering problems with efficient global robust optimization. It provides tools for initial design sampling, surrogate modeling, sensitivity analysis, and robust optimization.</p>"},{"location":"#features","title":"\ud83d\udcda Features","text":""},{"location":"#design-of-experiments","title":"Design of Experiments","text":"<ul> <li>Advanced sampling methods like Latin Hypercube, Sobol, and Halton</li> <li>Support for design and environmental variables</li> <li>Customizable sampling criteria</li> <li>Multi-dimensional support</li> </ul>"},{"location":"#efficient-global-optimization","title":"Efficient Global Optimization","text":"<ul> <li> <p>Multiple acquisition functions available:    -- Expected Improvement (EI) and Boosting the Exploration term (\ud835\udf01-EI)    -- Probability of Improvement (PI)    -- Lower Confidence Bound (LCB)    -- E3I (Exploration Enhanced Expected Improvement )    -- EIGF (Expected Improvement for Global Fit)    -- CRI3 (Distance-Enhanced Gradient)</p> </li> <li> <p>Comprehensive training configuration</p> </li> <li>Built-in visualization tools</li> <li>Parallel evaluation support</li> </ul>"},{"location":"#surrogate-model-training","title":"Surrogate Model Training","text":"<ul> <li>Gaussian Process Regression (GPR) training</li> <li>Hardware optimization (CPU/GPU)</li> <li>Progress tracking and visualization</li> <li>Hyperparameter optimization</li> </ul>"},{"location":"#robust-optimization","title":"Robust Optimization","text":"<ul> <li>Monte Carlo Simulation (MCS)</li> <li>Polynomial Chaos Expansion (PCE)</li> <li>Support for both direct and surrogate evaluation</li> <li>Multi-objective Pareto solution</li> </ul>"},{"location":"#sensitivity-analysis","title":"Sensitivity Analysis","text":"<ul> <li>Sobol indices calculation</li> <li>Support for true functions and surrogates</li> <li>Analysis and visualization</li> </ul>"},{"location":"#uncertainty-quantification","title":"Uncertainty Quantification","text":"<ul> <li>Uncertainty Propagation</li> <li>Distribution Analysis</li> <li>Moment estimation</li> <li>PDF/CDF estimation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>Install PyEGRO using pip:</p> <pre><code>pip install PyEGRO\n</code></pre>"},{"location":"user-guide/doe/overviewdoe/","title":"Mathematical Formulation","text":"<p>Sampling Methods</p> <ol> <li> <p>Latin Hypercube Sampling (LHS): Divides each variable's range into equal intervals and ensures one sample is drawn from each interval. The process ensures balanced coverage by sampling from:</p> \\[ x_i \\sim U\\left(\\frac{k-1}{N}, \\frac{k}{N}\\right), \\quad k = 1, \\ldots, N \\] <p>where \\(N\\) is the total number of samples.</p> </li> <li> <p>Sobol Sequence: Generates low-discrepancy sequences with uniformity, minimizing discrepancy \\(D\\):</p> \\[ D \\leq \\frac{\\log(N)^d}{N} \\] <p>where \\(d\\) is the dimensionality of the problem.</p> </li> <li> <p>Random Sampling: Random sampling draws samples \\(x\\) uniformly from the range \\([a_i, b_i]\\):</p> \\[ x_i \\sim U(a_i, b_i) \\] </li> </ol> <p>Objective Function Evaluation</p> <p>Samples \\(x\\) are evaluated using an objective function \\(f(x)\\):</p> \\[ \\text{Result} = f(x) \\] <p>This provides flexibility for custom functions during design optimization.</p>"},{"location":"user-guide/doe/usagedoe/","title":"Design of Experiment / Usage","text":"<p>This document provides examples for using the PyEGRO DOE module across different scenarios. The examples showcase variable definition, sampling methods, and saving/loading configurations.</p>"},{"location":"user-guide/doe/usagedoe/#default-settings-usage","title":"Default Settings Usage","text":"<p>Goal:</p> <p>Run the sampling process with default settings for rapid setup and testing.</p> <p>Code:</p> <pre><code>from pyegro.initial_design import InitialDesign\n\n# Create design with default settings\ndesign = InitialDesign(\n    sampling_method='lhs',  # Default: Latin Hypercube Sampling\n    show_progress=True\n)\n\n# Define a basic design variable\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[0, 1],  # Default range\n    description='Default variable x1'\n)\n\n# Run the sampling process\ndesign.run(\n    num_samples=10  # Default number of samples\n)\n</code></pre> <p>Output:</p> <ol> <li><code>training_data.csv</code>: Contains generated samples with default settings.</li> <li>No additional configuration files are created unless explicitly saved.</li> </ol> <p></p>"},{"location":"user-guide/doe/usagedoe/#1-basic-lhs-sampling","title":"1. Basic LHS Sampling","text":"<p>Goal: Generate samples using Latin Hypercube Sampling (LHS) with two design variables.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0]**2 + x[:, 1]**2  # Simple quadratic function\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_LHS',\n    sampling_method='lhs',\n    sampling_criterion='maximin',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='First variable'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.1,\n    description='Second variable'\n)\n\n# Save configuration and run sampling\ndesign.save(\"lhs_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=50\n)\n</code></pre> Output: 1. <code>lhs_config.json</code>: Contains design configuration. 2. <code>training_data.csv</code>: Generated samples and objective function values.</p>"},{"location":"user-guide/doe/usagedoe/#2-sobol-sequence-sampling","title":"2. Sobol Sequence Sampling","text":"<p>Goal: Generate samples using the Sobol sequence for a low-discrepancy sampling approach.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return (x[:, 0] - 1)**2 + (x[:, 1] - 2)**2\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_SOBOL',\n    sampling_method='sobol',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.2,\n    description='Variable x1'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-6, 6],\n    cov=0.2,\n    description='Variable x2'\n)\n\n# Save configuration and run sampling\ndesign.save(\"sobol_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=100\n)\n</code></pre> Output: 1. <code>sobol_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#3-mixed-variable-types","title":"3. Mixed Variable Types","text":"<p>Goal: Generate samples with both design and environmental variables.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0]**2 + 3 * x[:, 1]\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_MIXED',\n    sampling_method='lhs',\n    show_progress=True\n)\n\n# Define design variable\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[-5, 5],\n    cov=0.1,\n    description='Design variable x1'\n)\n\n# Define environmental variable\ndesign.add_env_variable(\n    name='env1',\n    distribution='normal',\n    mean=10,\n    cov=0.2,\n    description='Environmental variable env1'\n)\n\n# Save configuration and run sampling\ndesign.save(\"mixed_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=50\n)\n</code></pre> Output: 1. <code>mixed_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#4-random-sampling","title":"4. Random Sampling","text":"<p>Goal: Generate samples using uniform random sampling.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0] * x[:, 1]\n\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_RANDOM',\n    sampling_method='random',\n    show_progress=True\n)\n\n# Define design variables\ndesign.add_design_variable(\n    name='x1',\n    range_bounds=[0, 10],\n    cov=0.05,\n    description='Random variable x1'\n)\ndesign.add_design_variable(\n    name='x2',\n    range_bounds=[-10, 0],\n    cov=0.05,\n    description='Random variable x2'\n)\n\n# Save configuration and run sampling\ndesign.save(\"random_config\")\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=20\n)\n</code></pre> Output: 1. <code>random_config.json</code> 2. <code>training_data.csv</code></p>"},{"location":"user-guide/doe/usagedoe/#5-loading-and-reusing-configurations","title":"5. Loading and Reusing Configurations","text":"<p>Goal: Reuse a saved configuration to generate new samples.</p> <p>Code: <pre><code>from PyEGRO.initial_design import InitialDesign\n\ndef objective_function(x):\n    return x[:, 0] + x[:, 1]\n\n# Load configuration\ndesign = InitialDesign(\n    output_dir='DATA_PREPARATION_REUSE',\n    sampling_method='lhs'\n)\ndesign.load(\"lhs_config\")\n\n# Run with different sample count\nresults = design.run(\n    objective_function=objective_function,\n    num_samples=100\n)\n</code></pre> Output: 1. Updated <code>training_data.csv</code></p>"}]}